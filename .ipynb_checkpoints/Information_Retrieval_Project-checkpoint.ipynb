{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Information Retrieval\n",
    "\n",
    "This is the document retrieval and sentence retrieval part of the project."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  1. Unpack the zip file.\n",
    "    \n",
    "Unpack the 'wiki-pages-text.zip' in the current directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import zipfile\n",
    "def unpack():\n",
    "    with zipfile.ZipFile('wiki-pages-text.zip') as file:\n",
    "        file.extractall()\n",
    "unpack()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Load file. \n",
    "\n",
    "Load the training dataset and the wiki txt file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-06T05:47:34.886653Z",
     "start_time": "2019-05-06T05:46:58.941199Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of the train data is: 145449\n",
      "\n",
      "\n",
      "Length of the document is: 25248397\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "with open('train.json', 'r') as f:  # load training dataset\n",
    "        train_data = json.load(f)   \n",
    "print(\"Length of the train data is: \" + str(len(train_data)))\n",
    "\n",
    "with open('devset.json', 'r') as f1:  # load dev dataset\n",
    "        dev_data = json.load(f1) \n",
    "        \n",
    "# appeand all the wiki txt sentences to one document\n",
    "def loadfile(folder): \n",
    "    document = []\n",
    "    list_of_files = os.listdir(folder)\n",
    "    \n",
    "    for file in list_of_files:\n",
    "        try:\n",
    "            filename = os.path.join(folder, file)\n",
    "            with open(filename, 'r') as doc:\n",
    "                for line in doc:\n",
    "                    document.append(line)     \n",
    "        except Exception as e:\n",
    "            print(\"No files found here!\")\n",
    "            raise e\n",
    "    return document\n",
    "document = loadfile(\"wiki-pages-text\")\n",
    "\n",
    "print(\"Length of the document is: \" + str(len(document)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Preprocess \n",
    "Preprocess includes: strip punctuations, tokenize,lemma, lower case, remove stop words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-06T05:36:07.109919Z",
     "start_time": "2019-05-06T05:36:07.042428Z"
    }
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "# nltk.download('punkt')\n",
    "# nltk.download('stopwords')\n",
    "# nltk.download('wordnet')\n",
    "\n",
    "lemmatizer = nltk.stem.wordnet.WordNetLemmatizer()\n",
    "\n",
    "def lemmatize(word):\n",
    "    lemma = lemmatizer.lemmatize(word,'v')\n",
    "    if lemma == word:\n",
    "        lemma = lemmatizer.lemmatize(word,'n')\n",
    "    return lemma\n",
    "\n",
    "stop_words = set(stopwords.words('english')) \n",
    "\n",
    "def preprocess(sentence):\n",
    "    norm_sentence = []\n",
    "    sentence = re.sub(r'[^\\w\\s]', '', sentence) # strip punctuations: remove ',' '.',etc.\n",
    "    tokens = nltk.tokenize.word_tokenize(sentence)  \n",
    "                \n",
    "    for token in tokens:\n",
    "        token = lemmatize(token)\n",
    "        token = token.lower() \n",
    "        if (token == \"no\" or token == \"not\"):  # keep no from stop words as it is useful for analysis\n",
    "            norm_sentence.append(token)  \n",
    "        if token not in stop_words:         # remove stop words\n",
    "            norm_sentence.append(token)                  \n",
    "    return norm_sentence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. Build inverted index.\n",
    "Compute document term frequency, build <b> inverted index </b> and then uses BM25 to rank.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-06T05:36:35.608703Z",
     "start_time": "2019-05-06T05:36:35.579306Z"
    }
   },
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "def doc_term_freq(doc):\n",
    "    doc_term_freqs = []\n",
    "    for sentence in doc:\n",
    "        doc_term_freqs.append(Counter(sentence)) \n",
    "    return doc_term_freqs\n",
    "\n",
    "class InvertedIndex:\n",
    "    def __init__(self, vocab, doc_term_freqs):\n",
    "        self.vocab = vocab\n",
    "        self.doc_len = [0] * len(doc_term_freqs)\n",
    "        self.doc_term_freqs = [[] for i in range(len(vocab))]\n",
    "        self.doc_ids = [[] for i in range(len(vocab))]\n",
    "        self.doc_freqs = [0] * len(vocab)\n",
    "        self.total_num_docs = 0\n",
    "        self.total_doc_len = 0\n",
    "        for docid, term_freqs in enumerate(doc_term_freqs):\n",
    "            doc_len = sum(term_freqs.values())\n",
    "            self.total_doc_len += doc_len\n",
    "            self.doc_len[docid] = doc_len\n",
    "            self.total_num_docs += 1\n",
    "            for term, freq in term_freqs.items():\n",
    "                term_id = vocab[term]\n",
    "                self.doc_ids[term_id].append(docid)\n",
    "                self.doc_term_freqs[term_id].append(freq)\n",
    "                self.doc_freqs[term_id] += 1\n",
    "\n",
    "    def num_terms(self):\n",
    "        return len(self.doc_ids)\n",
    "\n",
    "    def num_docs(self):\n",
    "        return self.total_num_docs\n",
    "\n",
    "    def docids(self, term):\n",
    "        term_id = self.vocab[term]\n",
    "        return self.doc_ids[term_id]\n",
    "\n",
    "    def freqs(self, term):\n",
    "        term_id = self.vocab[term]\n",
    "        return self.doc_term_freqs[term_id]\n",
    "\n",
    "    def f_t(self, term):\n",
    "        term_id = self.vocab[term]\n",
    "        return self.doc_freqs[term_id]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-06T05:34:26.554193Z",
     "start_time": "2019-05-06T05:34:26.541446Z"
    }
   },
   "source": [
    "#### 5. BM25 \n",
    "Okapi BM25 function is used to rank the sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-06T05:36:59.699374Z",
     "start_time": "2019-05-06T05:36:59.680002Z"
    }
   },
   "outputs": [],
   "source": [
    "from math import log, sqrt\n",
    "\n",
    "def BM25(query, index, k=5):\n",
    "    k1 = 1.2\n",
    "    k3 = 1.5\n",
    "    b = 0.75\n",
    "    # scores stores doc ids and their scores\n",
    "    scores = Counter()\n",
    "    length_average = index.total_doc_len/index.total_num_docs  # average length of document \n",
    "    \n",
    "    for term in query:\n",
    "        \n",
    "        if term not in list(vocab.keys()):     # skip if the query word is not in the vocab\n",
    "            continue\n",
    "        \n",
    "        N = index.num_docs()      # N: total number of documents\n",
    "        ft = index.f_t(term)       # ft: document frequency of term\n",
    "        docs = index.docids(term)    # docs: all doc ids that contain the term \n",
    "        dft = index.freqs(term)      # dft: all document freqs of the term \n",
    "        fqt = Counter(query)[term]   # fqt: frequency of tern t in query\n",
    "        \n",
    "        for num, docid in enumerate(docs):                                                                       \n",
    "            fdt = dft[num]                                #fdt: frequency of term t in document d   \n",
    "            length = sqrt(abs(index.doc_len[docid]))      # length: length of the doc\n",
    "            \n",
    "            #BM25 consists of three parts\n",
    "            idf = log((N - ft + 0.5)/(ft + 0.5))\n",
    "            tf = ((k1 + 1) * fdt)/(k1 * ((1-b) + b * length/length_average) + fdt)\n",
    "            query_tf = (k3 + 1) * fqt / (k3 + fqt)\n",
    "            \n",
    "            scores[docid] +=  idf * tf * query_tf           \n",
    "    return scores.most_common(k)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6. Use page identifier to reduce the search space.\n",
    "\n",
    "Build a dictionary that consists the <b>page identifier</b> as the key and the document index(ranges form 0 to 25248397 ） as value.\n",
    "\n",
    "Examples looks like: \"Alexander McNair\" : [1,2,3,4...18] \n",
    "\n",
    "<b>To do:</b> the keywords list may also consider synonyms for each keyword.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-06T05:50:31.187650Z",
     "start_time": "2019-05-06T05:49:17.304338Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of keys = 5396106\n"
     ]
    }
   ],
   "source": [
    "final_dic = {}\n",
    "\n",
    "for id, doc in enumerate(document):\n",
    "    final_dic.setdefault((doc.split()[0].replace(\"_\", \" \")), []).append(id)\n",
    "\n",
    "keys = list(final_dic.keys())\n",
    "print(\"Length of keys = {}\".format(len(keys)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7. Retrieval Evidence\n",
    "<b>7.1</b> Given a query, tokenize it first, then for each token in the query, try to find it in the keys list. \n",
    "\n",
    "<b>7.2</b> If there is a match or substring match, then retrieve that sentence by using index value to find the raw sentence in txt file. \n",
    "\n",
    "<b>7.3</b> Find all the related sentences by this way and then builds a inverted index and \n",
    "uses BM25 to rank and to retrieval top K sentences as the evidence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-06T05:59:14.124605Z",
     "start_time": "2019-05-06T05:59:13.951956Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alexander McNair\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17]\n",
      "Alatskivi\n",
      "[18, 19, 20, 21]\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21]\n",
      "\n",
      "\n",
      "Number of sentences = 22\n",
      "Number of unique terms = 198\n",
      "['alexander_mcnair', '0', 'alexander', 'mcnair', 'lrb', 'may', '5', '1775', 'march', '18', '1826', 'rrb', 'american', 'frontiersman', 'politician', '1', 'first', 'governor', 'missouri', 'entry', 'state', '1820', '1824', '4', 'bear', 'lancaster', 'province', 'pennsylvania', 'grow', 'mifflin']\n",
      "\n",
      "\n",
      "['alexander_mcnair', '0', 'alexander', 'mcnair', 'lrb', 'may', '5', '1775', 'march', '18', '1826', 'rrb', 'american', 'frontiersman', 'politician']\n",
      "['alexander_mcnair', '1', 'first', 'governor', 'missouri', 'entry', 'state', '1820', '1824']\n",
      "['alexander_mcnair', '4', 'mcnair', 'bear', 'lancaster', 'province', 'pennsylvania', 'grow', 'mifflin', 'county']\n",
      "documents = 22\n",
      "number of terms = 198\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'sqrt' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-121-6f8cfecb7658>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[0mprocessed_query\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpreprocess\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquery\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m \u001b[0mbm25_results\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBM25\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprocessed_query\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minvindex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mrank\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mres\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbm25_results\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"RANK {:2d} DOCID {:8d} SCORE {:.3f} CONTENT {:}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrank\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mres\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mres\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdocument\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mres\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-115-d8472676293f>\u001b[0m in \u001b[0;36mBM25\u001b[0;34m(query, index, k)\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mnum\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdocid\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdocs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m             \u001b[0mfdt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdft\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnum\u001b[0m\u001b[0;34m]\u001b[0m                                \u001b[0;31m#fdt: frequency of term t in document d\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m             \u001b[0mlength\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mabs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdoc_len\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdocid\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m      \u001b[0;31m# length: length of the doc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m             \u001b[0;31m#BM25 consists of three parts\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'sqrt' is not defined"
     ]
    }
   ],
   "source": [
    "test_query = \"Alexander Alatskivi\"\n",
    "retrievaled_sentences = []\n",
    "\n",
    "for word in  nltk.tokenize.word_tokenize(test_query):\n",
    "    for key in keys[:30]:\n",
    "        if word in key:\n",
    "            print(key)\n",
    "            print(final_dic[key])\n",
    "            retrievaled_sentences.append(final_dic[key])\n",
    "\n",
    "retrievaled_sentences = [item for sublist in retrievaled_sentences for item in sublist]  \n",
    "print(retrievaled_sentences)  \n",
    "\n",
    "#########testing \n",
    "processed_doc = [] # processed_docs stores the list of processed docs\n",
    "vocab = {}\n",
    "unique_id = 0\n",
    "\n",
    " # testing 30 sentences\n",
    "for retrievaled_sentence in retrievaled_sentences:\n",
    "    norm_sentence = preprocess(document[retrievaled_sentence])\n",
    "    for token in norm_sentence:\n",
    "        if token not in vocab:\n",
    "            vocab.update({token: unique_id})     \n",
    "            unique_id = unique_id + 1\n",
    "    processed_doc.append(norm_sentence) \n",
    "\n",
    "print(\"\\n\")\n",
    "print(\"Number of sentences = {}\".format(len(processed_doc)))\n",
    "print(\"Number of unique terms = {}\".format(len(vocab)))\n",
    "\n",
    "print(list(vocab.keys())[:30])\n",
    "print(\"\\n\")\n",
    "print(processed_doc[0])\n",
    "print(processed_doc[1])\n",
    "print(processed_doc[2])\n",
    "\n",
    "doc_term_freqs = doc_term_freq(processed_doc)\n",
    "\n",
    "invindex = InvertedIndex(vocab, doc_term_freqs)\n",
    "\n",
    "# print inverted index stats\n",
    "print(\"documents = {}\".format(invindex.num_docs()))\n",
    "print(\"number of terms = {}\".format(invindex.num_terms()))\n",
    "\n",
    "query = \"Alexander Alatskivi\"\n",
    "processed_query = preprocess(query)\n",
    "\n",
    "bm25_results = BM25(processed_query, invindex)\n",
    "for rank, res in enumerate(bm25_results):\n",
    "    print(\"RANK {:2d} DOCID {:8d} SCORE {:.3f} CONTENT {:}\".format(rank+1,res[0],res[1],document[res[0]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Predict evidence for 'train.json' dataset  </b>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-04T11:52:48.325706Z",
     "start_time": "2019-05-04T11:52:48.230589Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'claim': 'Nikolaj Coster-Waldau worked with the Fox Broadcasting Company.', 'label': 'SUPPORTS', 'evidence': [['Fox_Broadcasting_Company', 0], ['Nikolaj_Coster-Waldau', 7]], 'predicted_evidence': [['Alcenya_Crowley', '5'], ['Alan_E._Freedman', '1'], ['American_Dad!_-LRB-season_10-RRB-', '1'], ['Alan_Chorlton', '18'], ['Alejandra_Ávalos', '1']]}\n",
      "\n",
      "\n",
      "{'claim': 'Roman Atwood is a content creator.', 'label': 'SUPPORTS', 'evidence': [['Roman_Atwood', 1], ['Roman_Atwood', 3]], 'predicted_evidence': [['Alice_Mak_-LRB-cartoonist-RRB-', '1'], ['Amatus_of_Nusco', '2'], ['Alveum', '1'], ['Amrit_Desai', '1'], ['Alakh_Niranjan', '0']]}\n",
      "\n",
      "\n",
      "{'claim': 'History of art includes architecture, dance, sculpture, music, painting, poetry literature, theatre, narrative, film, photography and graphic arts.', 'label': 'SUPPORTS', 'evidence': [['History_of_art', 2]], 'predicted_evidence': [['Aleksandra_Dulic', '7'], ['Amshuverma', '6'], ['Aldro_Hibbard', '12'], ['Alley_Theatre_-LRB-Strabane-RRB-', '5'], ['All-Union_art_exhibition_-LRB-Moscow,_1957-RRB-', '0']]}\n",
      "\n",
      "\n",
      "{'claim': 'Adrienne Bailon is an accountant.', 'label': 'REFUTES', 'evidence': [['Adrienne_Bailon', 0]], 'predicted_evidence': [['Alcenya_Crowley', '5'], ['Alberto_Couriel', '0'], ['Amos_Bad_Heart_Bull', '1']]}\n",
      "\n",
      "\n",
      "{'claim': 'System of a Down briefly disbanded in limbo.', 'label': 'NOT ENOUGH INFO', 'evidence': [], 'predicted_evidence': [['Aming', '1'], ['Airbus_-LRB-band-RRB-', '27'], ['Air_Defence_Regiment_-LRB-Sweden-RRB-', '1'], ['Albert_and_Temmy_Latner_Jewish_Public_Library_-LRB-Toronto-RRB-', '6'], ['Alice_in_Wonderland_-LRB-1966_TV_play-RRB-', '16']]}\n",
      "\n",
      "\n",
      "{'claim': 'Homeland is an American television spy thriller based on the Israeli television series Prisoners of War.', 'label': 'SUPPORTS', 'evidence': [['Prisoners_of_War_-LRB-TV_series-RRB-', 0], ['Homeland_-LRB-TV_series-RRB-', 0]], 'predicted_evidence': [['American_Horror_Story-COLON-_Asylum', '0'], ['Album_-LRB-Land_of_the_Lost-RRB-', '0'], ['Alfred_Hitchcock_Presents', '0'], ['Akumu-chan', '5'], ['Allison_Argo', '7']]}\n",
      "\n",
      "\n",
      "{'claim': 'Beautiful reached number two on the Billboard Hot 100 in 2003.', 'label': 'NOT ENOUGH INFO', 'evidence': [], 'predicted_evidence': [['All_Those_Years_Ago', '3'], ['Almost_Goodbye', '2'], ['All_the_Good_Ones_Are_Gone', '2'], ['Already_Gone_-LRB-Eagles_song-RRB-', '4'], ['Amnesia_-LRB-Cherish_song-RRB-', '4']]}\n",
      "\n",
      "\n",
      "{'claim': 'Neal Schon was named in 1954.', 'label': 'NOT ENOUGH INFO', 'evidence': [], 'predicted_evidence': [['Alfons_Van_den_Brande', '1'], ['Amata_schoutedeni', '1'], ['Alarko_Holding', '2'], ['Alf_Ramsøy', '7'], ['Alexey_Sorokin_-LRB-military_commander-RRB-', '11']]}\n",
      "\n",
      "\n",
      "{'claim': 'The Boston Celtics play their home games at TD Garden.', 'label': 'SUPPORTS', 'evidence': [['Boston_Celtics', 3]], 'predicted_evidence': [['Air_Force_Falcons_baseball', '5'], ['Al-Taawoun_FC', '10'], ['Al_Federoff', '12'], ['Alfred_Edwin_Brain_Sr.', '13'], ['Alta_Little', '25']]}\n",
      "\n",
      "\n",
      "{'claim': 'The Ten Commandments is an epic film.', 'label': 'SUPPORTS', 'evidence': [['The_Ten_Commandments_-LRB-1956_film-RRB-', 20], ['The_Ten_Commandments_-LRB-1956_film-RRB-', 0]], 'predicted_evidence': [['An_American_Romance', '0'], ['Alexandra_Burke', '2'], ['All_the_Wrong_Places_-LRB-song-RRB-', '1'], ['Ambalika', '0'], ['Aleksandr_Gorban', '15']]}\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def get_evidence(train_data):\n",
    "    for key in list(train_data)[:10]:\n",
    "        train_data[key][\"predicted_evidence\"] = []\n",
    "        \n",
    "        bm25_result = BM25(preprocess(train_data[key][\"claim\"]), invindex)\n",
    "        for rank, res in enumerate(bm25_result):\n",
    "            train_data[key][\"predicted_evidence\"].append([document[res[0]].split()[0], document[res[0]].split()[1]])\n",
    "    return train_data\n",
    "\n",
    "predicted_train = get_evidence(train_data)\n",
    "\n",
    "for key in list(predicted_train)[:10]:\n",
    "    print(predicted_train[key])\n",
    "    print(\"\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
