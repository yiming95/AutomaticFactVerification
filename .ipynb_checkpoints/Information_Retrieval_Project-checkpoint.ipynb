{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Information Retrieval\n",
    "\n",
    "This is the document retrieval and sentence retrieval part of the project.\n",
    "Method "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Unpack the zip file.\n",
    "    \n",
    "Unpack the 'wiki-pages-text.zip' in the current directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import zipfile\n",
    "\n",
    "def unpack():\n",
    "    with zipfile.ZipFile('wiki-pages-text.zip') as file:\n",
    "        file.extractall()\n",
    "unpack()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Load file. \n",
    "\n",
    "Load the training dataset and the wiki txt file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-04T06:57:40.534288Z",
     "start_time": "2019-05-04T06:57:23.897895Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of the train data is: 145449\n",
      "Top 3 instances in train data\n",
      "75397 {'claim': 'Nikolaj Coster-Waldau worked with the Fox Broadcasting Company.', 'label': 'SUPPORTS', 'evidence': [['Fox_Broadcasting_Company', 0], ['Nikolaj_Coster-Waldau', 7]]}\n",
      "150448 {'claim': 'Roman Atwood is a content creator.', 'label': 'SUPPORTS', 'evidence': [['Roman_Atwood', 1], ['Roman_Atwood', 3]]}\n",
      "214861 {'claim': 'History of art includes architecture, dance, sculpture, music, painting, poetry literature, theatre, narrative, film, photography and graphic arts.', 'label': 'SUPPORTS', 'evidence': [['History_of_art', 2]]}\n",
      "\n",
      "\n",
      "['wiki-009.txt', 'wiki-021.txt', 'wiki-035.txt', 'wiki-034.txt', 'wiki-020.txt', 'wiki-008.txt', 'wiki-036.txt', 'wiki-022.txt', 'wiki-023.txt', 'wiki-037.txt', 'wiki-033.txt', 'wiki-027.txt', 'wiki-026.txt', 'wiki-032.txt', 'wiki-024.txt', 'wiki-030.txt', 'wiki-018.txt', 'wiki-019.txt', 'wiki-031.txt', 'wiki-025.txt', 'wiki-042.txt', 'wiki-056.txt', 'wiki-081.txt', 'wiki-095.txt', 'wiki-094.txt', 'wiki-080.txt', 'wiki-057.txt', 'wiki-043.txt', 'wiki-069.txt', 'wiki-055.txt', 'wiki-041.txt', 'wiki-096.txt', 'wiki-082.txt', 'wiki-109.txt', 'wiki-108.txt', 'wiki-083.txt', 'wiki-097.txt', 'wiki-040.txt', 'wiki-054.txt', 'wiki-068.txt', 'wiki-050.txt', 'wiki-044.txt', 'wiki-078.txt', 'wiki-093.txt', 'wiki-087.txt', 'wiki-086.txt', 'wiki-092.txt', 'wiki-079.txt', 'wiki-045.txt', 'wiki-051.txt', 'wiki-047.txt', 'wiki-053.txt', 'wiki-084.txt', 'wiki-090.txt', 'wiki-091.txt', 'wiki-085.txt', 'wiki-052.txt', 'wiki-046.txt', 'wiki-063.txt', 'wiki-077.txt', 'wiki-088.txt', 'wiki-103.txt', 'wiki-102.txt', 'wiki-089.txt', 'wiki-076.txt', 'wiki-062.txt', 'wiki-048.txt', 'wiki-074.txt', 'wiki-060.txt', 'wiki-100.txt', 'wiki-101.txt', 'wiki-061.txt', 'wiki-075.txt', 'wiki-049.txt', 'wiki-071.txt', 'wiki-065.txt', 'wiki-059.txt', 'wiki-105.txt', 'wiki-104.txt', 'wiki-058.txt', 'wiki-064.txt', 'wiki-070.txt', 'wiki-066.txt', 'wiki-072.txt', 'wiki-099.txt', 'wiki-106.txt', 'wiki-107.txt', 'wiki-098.txt', 'wiki-073.txt', 'wiki-067.txt', 'wiki-028.txt', 'wiki-014.txt', 'wiki-015.txt', 'wiki-001.txt', 'wiki-029.txt', 'wiki-017.txt', 'wiki-003.txt', 'wiki-002.txt', 'wiki-016.txt', 'wiki-012.txt', 'wiki-006.txt', 'wiki-007.txt', 'wiki-013.txt', 'wiki-005.txt', 'wiki-011.txt', 'wiki-039.txt', 'wiki-038.txt', 'wiki-010.txt', 'wiki-004.txt']\n",
      "\n",
      "\n",
      "Length of the document is: 25248397\n",
      "\n",
      "\n",
      "Alexander_McNair 0 Alexander McNair -LRB- May 5 , 1775 -- March 18 , 1826 -RRB- was an American frontiersman and politician .\n",
      "\n",
      "Alexander_McNair 1 He was the first Governor of Missouri from its entry as a state in 1820 , until 1824 .\n",
      "\n",
      "Alexander_McNair 4 McNair was born in Lancaster in the Province of Pennsylvania and grew up in Mifflin County .\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "with open('train.json', 'r') as f:  # load training dataset\n",
    "        train_data = json.load(f)   \n",
    "\n",
    "print(\"Length of the train data is: \" + str(len(train_data)))\n",
    "\n",
    "# Top 3 instances in train data\n",
    "print(\"Top 3 instances in train data\")\n",
    "for key in list(train_data)[:3]:\n",
    "    print(key, train_data[key])\n",
    "print(\"\\n\")\n",
    "\n",
    "# appeand all the wiki txt sentences to one document\n",
    "def loadfile(folder): \n",
    "    document = []\n",
    "    list_of_files = os.listdir(folder)\n",
    "    print(list_of_files)\n",
    "    \n",
    "    for file in list_of_files:\n",
    "        try:\n",
    "            filename = os.path.join(folder, file)\n",
    "            with open(filename, 'r') as doc:\n",
    "                for line in doc:\n",
    "                    document.append(line)     \n",
    "        except Exception as e:\n",
    "            print(\"No files found here!\")\n",
    "            raise e\n",
    "            \n",
    "    return document\n",
    "\n",
    "document = loadfile(\"wiki-pages-text\")\n",
    "\n",
    "print(\"\\n\")\n",
    "print(\"Length of the document is: \" + str(len(document)))\n",
    "print(\"\\n\")\n",
    "\n",
    "# index0 wiki file, index1 sentence\n",
    "print(document[0])\n",
    "print(document[1])\n",
    "print(document[2])\n",
    "print(\"\\n\")\n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>3. Preprocess </b>\n",
    "\n",
    "3.1 Preprocess the sentence: strip punctuations, tokenize,lemma, lower case, remove stop words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-04T08:58:12.958670Z",
     "start_time": "2019-05-04T08:58:09.957211Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Number of sentences = 10000\n",
      "Number of unique terms = 25147\n",
      "\n",
      "\n",
      "['alexander_mcnair', '0', 'alexander', 'mcnair', 'lrb', 'may', '5', '1775', 'march', '18', '1826', 'rrb', 'american', 'frontiersman', 'politician']\n",
      "['alexander_mcnair', '1', 'first', 'governor', 'missouri', 'entry', 'state', '1820', '1824']\n",
      "['alexander_mcnair', '4', 'mcnair', 'bear', 'lancaster', 'province', 'pennsylvania', 'grow', 'mifflin', 'county']\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "lemmatizer = nltk.stem.wordnet.WordNetLemmatizer()\n",
    "\n",
    "def lemmatize(word):\n",
    "    lemma = lemmatizer.lemmatize(word,'v')\n",
    "    if lemma == word:\n",
    "        lemma = lemmatizer.lemmatize(word,'n')\n",
    "    return lemma\n",
    "\n",
    "stop_words = set(stopwords.words('english')) \n",
    "\n",
    "processed_doc = [] # processed_docs stores the list of processed docs\n",
    "vocab = {}\n",
    "unique_id = 0\n",
    "\n",
    "def preprocess(sentence):\n",
    "    norm_sentence = []\n",
    "    sentence = re.sub(r'[^\\w\\s]', '', sentence) # strip punctuations: remove ',' '.',etc.\n",
    "    tokens = nltk.tokenize.word_tokenize(sentence)  \n",
    "                \n",
    "    for token in tokens:\n",
    "        token = lemmatize(token)\n",
    "        token = token.lower() \n",
    "        if (token == \"no\" or token == \"not\"):  # keep no from stop words as it is useful for analysis\n",
    "            norm_sentence.append(token)  \n",
    "        if token not in stop_words:         # remove stop words\n",
    "            norm_sentence.append(token)                  \n",
    "    return norm_sentence\n",
    "\n",
    " # testing 10000 sentences\n",
    "for sentence in document[:10000]:\n",
    "    norm_sentence = preprocess(sentence)\n",
    "    \n",
    "    for token in norm_sentence:\n",
    "        if token not in vocab:\n",
    "            vocab.update({token: unique_id})     \n",
    "            unique_id = unique_id + 1\n",
    "                \n",
    "    processed_doc.append(norm_sentence) \n",
    "\n",
    "print(\"\\n\")\n",
    "print(\"Number of sentences = {}\".format(len(processed_doc)))\n",
    "print(\"Number of unique terms = {}\".format(len(vocab)))\n",
    "\n",
    "print(\"\\n\")\n",
    "print(processed_doc[0])\n",
    "print(processed_doc[1])\n",
    "print(processed_doc[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3.2 Calculate document term frequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-04T09:54:29.584722Z",
     "start_time": "2019-05-04T09:54:29.521575Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of doc term freqs = 10000\n",
      "\n",
      "\n",
      "Counter({'alexander_mcnair': 1, '0': 1, 'alexander': 1, 'mcnair': 1, 'lrb': 1, 'may': 1, '5': 1, '1775': 1, 'march': 1, '18': 1, '1826': 1, 'rrb': 1, 'american': 1, 'frontiersman': 1, 'politician': 1})\n",
      "1\n",
      "Counter({'alexander_mcnair': 1, '1': 1, 'first': 1, 'governor': 1, 'missouri': 1, 'entry': 1, 'state': 1, '1820': 1, '1824': 1})\n",
      "Counter({'alexander_mcnair': 1, '4': 1, 'mcnair': 1, 'bear': 1, 'lancaster': 1, 'province': 1, 'pennsylvania': 1, 'grow': 1, 'mifflin': 1, 'county': 1})\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "def doc_term_freq(doc):\n",
    "    doc_term_freqs = []\n",
    "    \n",
    "    for sentence in doc:\n",
    "        doc_term_freqs.append(Counter(sentence)) \n",
    "    \n",
    "    return doc_term_freqs\n",
    "\n",
    "doc_term_freqs = doc_term_freq(processed_doc)\n",
    "\n",
    "print(\"Number of doc term freqs = {}\".format(len(doc_term_freqs)))\n",
    "print(\"\\n\")\n",
    "print(doc_term_freqs[0])\n",
    "print(doc_term_freqs[0]['lrb'])\n",
    "print(doc_term_freqs[1])\n",
    "print(doc_term_freqs[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3.3 Build Inverted Index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-04T09:47:51.046884Z",
     "start_time": "2019-05-04T09:47:50.898690Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "documents = 10000\n",
      "number of terms = 25147\n"
     ]
    }
   ],
   "source": [
    "class InvertedIndex:\n",
    "    def __init__(self, vocab, doc_term_freqs):\n",
    "        self.vocab = vocab\n",
    "        self.doc_len = [0] * len(doc_term_freqs)\n",
    "        self.doc_term_freqs = [[] for i in range(len(vocab))]\n",
    "        self.doc_ids = [[] for i in range(len(vocab))]\n",
    "        self.doc_freqs = [0] * len(vocab)\n",
    "        self.total_num_docs = 0\n",
    "        self.total_doc_len = 0\n",
    "        for docid, term_freqs in enumerate(doc_term_freqs):\n",
    "            doc_len = sum(term_freqs.values())\n",
    "            self.total_doc_len += doc_len\n",
    "            self.doc_len[docid] = doc_len\n",
    "            self.total_num_docs += 1\n",
    "            for term, freq in term_freqs.items():\n",
    "                term_id = vocab[term]\n",
    "                self.doc_ids[term_id].append(docid)\n",
    "                self.doc_term_freqs[term_id].append(freq)\n",
    "                self.doc_freqs[term_id] += 1\n",
    "\n",
    "    def num_terms(self):\n",
    "        return len(self.doc_ids)\n",
    "\n",
    "    def num_docs(self):\n",
    "        return self.total_num_docs\n",
    "\n",
    "    def docids(self, term):\n",
    "        term_id = self.vocab[term]\n",
    "        return self.doc_ids[term_id]\n",
    "\n",
    "    def freqs(self, term):\n",
    "        term_id = self.vocab[term]\n",
    "        return self.doc_term_freqs[term_id]\n",
    "\n",
    "    def f_t(self, term):\n",
    "        term_id = self.vocab[term]\n",
    "        return self.doc_freqs[term_id]\n",
    "\n",
    "invindex = InvertedIndex(vocab, doc_term_freqs)\n",
    "\n",
    "# print inverted index stats\n",
    "print(\"documents = {}\".format(invindex.num_docs()))\n",
    "print(\"number of terms = {}\".format(invindex.num_terms()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3.4 Uses score function to rank the sentences.\n",
    "\n",
    "\\begin{equation*}\n",
    "Score(Q,d) = \\frac{1}{\\sqrt{|d|}} \\times \\sum_{i=1}^q \\log(1 + f_{d,t}) * \\log( \\frac{N}{f_t} ) \n",
    "\\end{equation*}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-04T09:47:59.139566Z",
     "start_time": "2019-05-04T09:47:59.126885Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed query is = ['alexander_mcnair']\n",
      "RANK  1 DOCID        7 SCORE 2.190 CONTENT Alexander_McNair 9 Alexander was defeated .\n",
      "\n",
      "RANK  2 DOCID       16 SCORE 1.549 CONTENT Alexander_McNair 24 After his time as governor , he worked in the Indian Department until his death .\n",
      "\n",
      "RANK  3 DOCID        1 SCORE 1.460 CONTENT Alexander_McNair 1 He was the first Governor of Missouri from its entry as a state in 1820 , until 1824 .\n",
      "\n",
      "RANK  4 DOCID       17 SCORE 1.460 CONTENT Alexander_McNair 25 He died of influenza , and is buried in Calvary Cemetery in St. Louis .\n",
      "\n",
      "RANK  5 DOCID        2 SCORE 1.385 CONTENT Alexander_McNair 4 McNair was born in Lancaster in the Province of Pennsylvania and grew up in Mifflin County .\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from math import log, sqrt\n",
    "\n",
    "# given a query and an index returns a list of the k highest scoring documents as tuples containing <docid,score>\n",
    "def query_tfidf(query, index, k=5):\n",
    "    \n",
    "    # scores stores doc ids and their scores\n",
    "    scores = Counter()\n",
    "    for term in query:   \n",
    "        N = index.num_docs()      # N: total number of documents\n",
    "        ft = index.f_t(term)       # ft: document frequency of term\n",
    "        docs = index.docids(term)    # docs: all doc ids that contain the term \n",
    "        dft = index.freqs(term)      # dft: all document freqs of the term         \n",
    "        \n",
    "        for num, docid in enumerate(docs):                # num: index used for iterate the docs                                                        \n",
    "            fdt = dft[num]                                #fdt: frequency of term t in document d   \n",
    "            length = sqrt(abs(index.doc_len[docid]))      # length: length of the doc\n",
    "            tfidf = log(1 + fdt)*log(N/ft)                 # tfidf: construct the score formula \n",
    "            scores[docid] += tfidf/length                  # score: the final score\n",
    "\n",
    "    return scores.most_common(k)\n",
    "\n",
    "query = \"Alexander_McNair\"\n",
    "\n",
    "processed_query = preprocess(query)\n",
    "print(\"Processed query is = {}\".format(processed_query))\n",
    "\n",
    "results = query_tfidf(processed_query, invindex)\n",
    "for rank, res in enumerate(results):\n",
    "    print(\"RANK {:2d} DOCID {:8d} SCORE {:.3f} CONTENT {:}\".format(rank+1,res[0],res[1],document[res[0]]))\n",
    "    # print(\"RANK \"+ str(rank+1) + \" Evidence: \" + str(document[res[0]][:100]))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>BM25  </b>\n",
    "\n",
    "Use BM25 to rank score. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-04T07:23:26.050115Z",
     "start_time": "2019-05-04T07:23:25.291872Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RANK  1 DOCID        0 SCORE 25147.000 CONTENT Alexander_McNair 0 Alexander McNair -LRB- May 5 , 1775 -- March 18 , 1826 -\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def BM25(query, index, k=5):\n",
    "    k1 = 1.2\n",
    "    k3 = 1.5\n",
    "    b = 0.75\n",
    "    # scores stores doc ids and their scores\n",
    "    scores = Counter()\n",
    "    length_average = index.total_doc_len/index.total_num_docs  # average length of document \n",
    "    \n",
    "    for term in query:\n",
    "        N = index.num_docs()      # N: total number of documents\n",
    "        ft = index.f_t(term)       # ft: document frequency of term\n",
    "        docs = index.docids(term)    # docs: all doc ids that contain the term \n",
    "        dft = index.freqs(term)      # dft: all document freqs of the term \n",
    "        fqt = Counter(query)[term]   # fqt: frequency of tern t in query\n",
    "        \n",
    "        for num, docid in enumerate(docs):                                                                       \n",
    "            fdt = dft[num]                                #fdt: frequency of term t in document d   \n",
    "            length = sqrt(abs(index.doc_len[docid]))      # length: length of the doc\n",
    "            \n",
    "            #BM25 consists of three parts\n",
    "            idf = log((N - ft + 0.5)/(ft + 0.5))\n",
    "            tf = ((k1 + 1) * fdt)/(k1 * ((1-b) + b * length/length_average) + fdt)\n",
    "            query_tf = (k3 + 1) * fqt / (k3 + fqt)\n",
    "            \n",
    "            scores[docid] +=  idf * tf * query_tf           \n",
    "    return scores.most_common(k)\n",
    "    \n",
    "bm25_results = BM25(processed_query, invindex)\n",
    "for rank, res in enumerate(bm25_results):\n",
    "    print(\"RANK {:2d} DOCID {:8d} SCORE {:.3f} CONTENT {:}\".format(rank+1,res[0],res[1],document[res[0]]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
