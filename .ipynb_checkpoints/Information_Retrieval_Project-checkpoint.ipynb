{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Information Retrieval\n",
    "\n",
    "This is the document retrieval and sentence retrieval part of the project."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  1. Unpack the zip file.\n",
    "    \n",
    "Unpack the 'wiki-pages-text.zip' in the current directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import zipfile\n",
    "def unpack():\n",
    "    with zipfile.ZipFile('wiki-pages-text.zip') as file:\n",
    "        file.extractall()\n",
    "unpack()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Load file. \n",
    "\n",
    "Load the training dataset and the wiki txt file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-09T06:09:07.871248Z",
     "start_time": "2019-05-09T06:05:14.358872Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of the train data is: 145449\n",
      "Length of the dev data is: 5001\n",
      "Length of the dev result data is: 5001\n",
      "Length of the test data is: 14997\n",
      "Length of the corpus is: 25248397\n",
      "(25248397, 1)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "with open('train.json', 'r') as f:  # load training dataset\n",
    "        train_data = json.load(f)   \n",
    "print(\"Length of the train data is: \" + str(len(train_data)))\n",
    "\n",
    "with open('devset.json', 'r') as f1:  # load dev dataset\n",
    "        dev_data = json.load(f1) \n",
    "print(\"Length of the dev data is: \" + str(len(dev_data)))\n",
    "\n",
    "with open('devset_result.json', 'r') as f2:  # store result \n",
    "        res_data = json.load(f2) \n",
    "print(\"Length of the dev result data is: \" + str(len(res_data)))\n",
    "\n",
    "with open('test-unlabelled.json', 'r') as f3:  # store result \n",
    "     test_data = json.load(f3) \n",
    "print(\"Length of the test data is: \" + str(len(test_data)))\n",
    "        \n",
    "# appeand all the wiki txt sentences to one document\n",
    "def loadfile(folder): \n",
    "    corpus = []\n",
    "    list_of_files = os.listdir(folder)\n",
    "    \n",
    "    for file in list_of_files:\n",
    "        try:\n",
    "            filename = os.path.join(folder, file)\n",
    "            with open(filename, 'r') as doc:\n",
    "                for line in doc:\n",
    "                    corpus.append(line)     \n",
    "        except Exception as e:\n",
    "            print(\"No files found here!\")\n",
    "            raise e\n",
    "    return corpus\n",
    "corpus = loadfile(\"wiki-pages-text\")\n",
    "print(\"Length of the corpus is: \" + str(len(corpus)))\n",
    "\n",
    "corpus_df = pd.DataFrame(corpus)\n",
    "print(corpus_df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Preprocess \n",
    "\n",
    "Preprocess includes: strip punctuations, tokenize,lemma, lower case, remove stop words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-09T07:21:16.585757Z",
     "start_time": "2019-05-09T07:21:16.263427Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                            wiki_text\n",
      "0   Alexander_McNair 0 Alexander McNair -LRB- May ...\n",
      "1   Alexander_McNair 1 He was the first Governor o...\n",
      "2   Alexander_McNair 4 McNair was born in Lancaste...\n",
      "3   Alexander_McNair 5 His grandfather , David McN...\n",
      "4   Alexander_McNair 6 David McNair , Jr. , Alexan...\n",
      "5   Alexander_McNair 7 Alexander went to school as...\n",
      "6   Alexander_McNair 8 He reached an agreement wit...\n",
      "7       Alexander_McNair 9 Alexander was defeated .\\n\n",
      "8   Alexander_McNair 10 He became a member of the ...\n",
      "9   Alexander_McNair 13 In 1804 , McNair traveled ...\n",
      "10  Alexander_McNair 14 In that year he married Ma...\n",
      "0     alexander_mcnair 0 alexander mcnair lrb may 5 ...\n",
      "1     alexander_mcnair 1 he be the first governor of...\n",
      "2     alexander_mcnair 4 mcnair be bear in lancaster...\n",
      "3     alexander_mcnair 5 his grandfather david mcnai...\n",
      "4     alexander_mcnair 6 david mcnair jr alexander s...\n",
      "5     alexander_mcnair 7 alexander go to school a a ...\n",
      "6     alexander_mcnair 8 he reach an agreement with ...\n",
      "7                alexander_mcnair 9 alexander be defeat\n",
      "8     alexander_mcnair 10 he become a member of the ...\n",
      "9     alexander_mcnair 13 in 1804 mcnair travel to w...\n",
      "10    alexander_mcnair 14 in that year he marry marg...\n",
      "Name: wiki_text, dtype: object\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "tokenizer = nltk.tokenize.RegexpTokenizer(r'\\w+')\n",
    "lemmatizer = nltk.stem.WordNetLemmatizer()\n",
    "\n",
    "nltk.download('wordnet')\n",
    "def lemmatize(word):\n",
    "    lemma = lemmatizer.lemmatize(word,'v')\n",
    "    if lemma == word:\n",
    "        lemma = lemmatizer.lemmatize(word,'n')\n",
    "    return lemma\n",
    "\n",
    "def pre_process(comment) -> str:\n",
    "    # lower cased\n",
    "    comment = comment.lower()\n",
    "    # tokenize\n",
    "    words = tokenizer.tokenize(comment)\n",
    "    # lemmatize \n",
    "    words = [lemmatize(w) for w in words]\n",
    "    processed_comment = \" \".join(words)\n",
    "    return processed_comment\n",
    "\n",
    "def process_dataset(dataset: pd.DataFrame) -> pd.DataFrame:\n",
    "    processed_corpus = dataset['wiki_text'].apply(lambda text: pre_process(text))\n",
    "    return processed_corpus\n",
    "\n",
    "corpus_df.columns = ['wiki_text']\n",
    "print(corpus_df.loc[:10])\n",
    "\n",
    "processed_corpus_df = process_dataset(corpus_df.loc[:2000])\n",
    "processed_corpus_df.to_pickle(\"./processed_corpus.pkl\")\n",
    "print(processed_corpus_df.loc[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load processed dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-09T07:21:20.326259Z",
     "start_time": "2019-05-09T07:21:20.315106Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2001,)\n",
      "0     alexander_mcnair 0 alexander mcnair lrb may 5 ...\n",
      "1     alexander_mcnair 1 he be the first governor of...\n",
      "2     alexander_mcnair 4 mcnair be bear in lancaster...\n",
      "3     alexander_mcnair 5 his grandfather david mcnai...\n",
      "4     alexander_mcnair 6 david mcnair jr alexander s...\n",
      "5     alexander_mcnair 7 alexander go to school a a ...\n",
      "6     alexander_mcnair 8 he reach an agreement with ...\n",
      "7                alexander_mcnair 9 alexander be defeat\n",
      "8     alexander_mcnair 10 he become a member of the ...\n",
      "9     alexander_mcnair 13 in 1804 mcnair travel to w...\n",
      "10    alexander_mcnair 14 in that year he marry marg...\n",
      "Name: wiki_text, dtype: object\n"
     ]
    }
   ],
   "source": [
    "load_processed_corpus_df = pd.read_pickle(\"./processed_corpus.pkl\")\n",
    "print(load_processed_corpus_df.shape)\n",
    "print(load_processed_corpus_df.loc[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Use Sklearn to build tf-idf.\n",
    "\n",
    "tfidf_vectorizer is the tf-idf model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-09T09:29:00.839300Z",
     "start_time": "2019-05-09T09:28:59.479897Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import pickle\n",
    "\n",
    "max_features = 20000\n",
    "tfidf_vectorizer = TfidfVectorizer(max_features=max_features, norm='l2')\n",
    "tfidf_vectorizer.fit(load_processed_corpus_df)\n",
    "\n",
    "pickle.dump(tfidf_vectorizer, open(\"tfidf_vectorizer.pickle\",\"wb\"))\n",
    "\n",
    "# load_tfidf_vectorizer = pickle.load(open(\"tfidf_vectorizer.pickle\",\"rb\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  Test tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-09T10:00:28.712594Z",
     "start_time": "2019-05-09T10:00:28.692290Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "alexander alatskivi\n",
      "0    alexander alatskivi\n",
      "Name: 0, dtype: object\n",
      "  (0, 911)\t0.5919900134396463\n",
      "  (0, 791)\t0.8059452983842806\n",
      "alexander  -  0.5919900134396463\n",
      "alatskivi  -  0.8059452983842806\n"
     ]
    }
   ],
   "source": [
    "query = \"Alexander  Alatskivi\"\n",
    "pre_query= pre_process(query)\n",
    "print(pre_query)\n",
    "query_df = pd.DataFrame([pre_query])\n",
    "print(query_df[0])\n",
    "res = tfidf_vectorizer.transform(query_df[0])\n",
    "print(res)\n",
    "\n",
    "feature_names = tfidf_vectorizer.get_feature_names()\n",
    "for col in res.nonzero()[1]:\n",
    "    print(feature_names[col], ' - ', res[0, col])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use page identifier to reduce the search space.\n",
    "\n",
    "Build a dictionary that consists the <b>page identifier</b> as the key and the document index(ranges form 0 to 25248397 ） as value.\n",
    "\n",
    "Examples looks like: \"Alexander McNair\" : [1,2,3,4...18] \n",
    "\n",
    "<b>To do:</b> the keywords list may also consider synonyms for each keyword.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-09T08:38:19.362035Z",
     "start_time": "2019-05-09T08:28:46.009296Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of keys = 5396106\n"
     ]
    }
   ],
   "source": [
    "final_dic = {}\n",
    "\n",
    "for id, doc in enumerate(corpus):\n",
    "    final_dic.setdefault((doc.split()[0].replace(\"_\", \" \")), []).append(id)\n",
    "\n",
    "keys = list(final_dic.keys())\n",
    "print(\"Length of keys = {}\".format(len(keys)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Build a dictionary for all the unique words in the keys.\n",
    "\n",
    "Keywords is a dictionary that stores all the unique page identifier word and its corresponing document index list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-09T11:06:55.203668Z",
     "start_time": "2019-05-09T11:06:55.197695Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Alexander McNair', 'Alatskivi', 'An American Girl Story - Maryellen 1955-COLON- Extraordinary Christmas', 'Alta Outcome Document', 'Al ‘Urban', 'Albano buoy system', 'Amuka', 'Aleksandr Abdulkhalikov', 'Amalia Ciardi Duprè', 'Akbil', 'All These Years', 'Ambulance services of Victoria', 'Alabama elections, 2018', 'Akarsu, Ardanuç', 'Albertinovac', 'All-India Yadav Mahasabha', 'Alejandro Rodríguez López', 'Aleksandr Luzin', 'American Thighs', 'Alan Brown -LRB-Australian politician-RRB-']\n",
      "Alexander\n",
      "McNair\n",
      "Alatskivi\n",
      "An\n",
      "American\n",
      "Girl\n",
      "Story\n",
      "-\n",
      "Maryellen\n",
      "1955-COLON-\n",
      "Extraordinary\n",
      "Christmas\n",
      "Alta\n",
      "Outcome\n",
      "Document\n",
      "Al\n",
      "‘Urban\n",
      "Albano\n",
      "buoy\n",
      "system\n",
      "Amuka\n",
      "Aleksandr\n",
      "Abdulkhalikov\n",
      "Amalia\n",
      "Ciardi\n",
      "Duprè\n",
      "Akbil\n",
      "All\n",
      "These\n",
      "Years\n",
      "Ambulance\n",
      "services\n",
      "of\n",
      "Victoria\n",
      "Alabama\n",
      "elections,\n",
      "2018\n",
      "Akarsu,\n",
      "Ardanuç\n",
      "Albertinovac\n",
      "All-India\n",
      "Yadav\n",
      "Mahasabha\n",
      "Alejandro\n",
      "Rodríguez\n",
      "López\n",
      "Aleksandr\n",
      "Luzin\n",
      "American\n",
      "Thighs\n",
      "Alan\n",
      "Brown\n",
      "-LRB-Australian\n",
      "politician-RRB-\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def create_keywords(keys, final_dic):\n",
    "    keywords = {}\n",
    "    for key in keys:\n",
    "        words = key.split()\n",
    "        for word in words:\n",
    "            print(word)\n",
    "            keywords[word] = final_dic[key]\n",
    "    return keywords\n",
    "\n",
    "# keywords should then remove stop words\n",
    "print(keys[:20])\n",
    "keywords = create_keywords(keys[:20], final_dic)\n",
    "#print(\"Length of keywords = {}\".format(len(keywords)))\n",
    "#print(keywords)\n",
    "#print('\\n')\n",
    "#print(keywords.get('Alexander'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-07T13:09:03.477935Z",
     "start_time": "2019-05-07T13:09:01.436343Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub data rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_data_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "def calculate_cosines(claim_tfidf, evi_tfidf) -> np.ndarray:\n",
    "    cosines = np.zeros((claim_tfidf.shape[0], 1))\n",
    "    for i in range(len(cosines)):\n",
    "        claim_vector = claim_tfidf[i]\n",
    "        evi_vector = evi_tfidf[i]\n",
    "        cosine_matrix = cosine_similarity([claim_vector.toarray()[0], evi_vector.toarray()[0]])\n",
    "        cosines[i][0] = cosine_matrix[0][1]\n",
    "    return cosines\n",
    "\n",
    "print(keywords.get('Alexander'))\n",
    "# testQuery = \"Alexander Alatskivi\"\n",
    "# query_tfidf = tfidf_vectorizer.transform(testQuery)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Retrieval Evidence\n",
    "Given a query, tokenize it first, then for each token in the query, find it in the keywords dictionary. \n",
    "\n",
    "\n",
    "To do: smater select the in the range of its alphabet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-07T13:06:29.017162Z",
     "start_time": "2019-05-07T13:06:13.469277Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub data rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_data_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Given a query, returns a list that contains all the document index values.\n",
    "def extract_sentences(query, keys, final_dic):\n",
    "    retrievaled_sentences = []\n",
    "    for word in  nltk.tokenize.word_tokenize(query):\n",
    "        retrievaled_sentences.append([keywords.get(word)])\n",
    "            \n",
    "    retrievaled_sentences = [item for sublist in retrievaled_sentences for item in sublist]\n",
    "    return retrievaled_sentences\n",
    "\n",
    "testQuery = \"Alexander Alatskivi\"\n",
    "print(extract_sentences(testQuery,keys[:100],final_dic))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If there is a match or substring match, then retrieve that sentence by using index value to find the raw sentence in txt file. \n",
    "\n",
    "Find all the related sentences by this way and then builds a inverted index and \n",
    "uses BM25 to rank and to retrieval top K sentences as the evidence.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-06T09:16:35.130407Z",
     "start_time": "2019-05-06T09:16:35.123213Z"
    }
   },
   "outputs": [],
   "source": [
    "# Use the result from above step, find all the raw sentences in txt file.\n",
    "# return a list of the doc id orderd by rnaking tfidf or bm25 score.\n",
    "\n",
    "def retrieval_evidence(query, keys, final_dic):\n",
    "    processed_doc = [] # processed_docs stores the list of processed docs\n",
    "    vocab = {}\n",
    "    unique_id = 0\n",
    "    rank_result = []\n",
    "    \n",
    "    retrievaled_sentences = extract_sentences(query,keys,final_dic)\n",
    "    \n",
    "    # find the row sentences and save them in processed_doc\n",
    "    for retrievaled_sentence in retrievaled_sentences:\n",
    "        norm_sentence = preprocess(document[retrievaled_sentence])\n",
    "        for token in norm_sentence:\n",
    "            if token not in vocab:\n",
    "                vocab.update({token: unique_id})     \n",
    "                unique_id = unique_id + 1\n",
    "        processed_doc.append(norm_sentence) \n",
    "    \n",
    "    # calculate doc term freqs and build an inverted index\n",
    "    doc_term_freqs = doc_term_freq(processed_doc)\n",
    "    invindex = InvertedIndex(vocab, doc_term_freqs)\n",
    "    \n",
    "    processed_query = preprocess(query)\n",
    "    bm25_results = BM25(processed_query, invindex, vocab)\n",
    "    tfidf_results = tfidf(processed_query, invindex, vocab)\n",
    "    \n",
    "    for rank, res in enumerate(tfidf_results):\n",
    "        # print(\"RANK {:2d} DOCID {:8d} SCORE {:.3f} CONTENT {:}\".format(rank+1,res[0],res[1],document[res[0]]))\n",
    "        rank_result.append((res[0]))\n",
    "    return rank_result\n",
    "\n",
    "# test_query = \"Nikolaj Coster-Waldau worked with the Fox Broadcasting Company.\"\n",
    "# test_query = \"Alexander Alatskivi\"\n",
    "# print(retrieval_evidence(test_query,keys[:100],final_dic))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Write the result to json file.\n",
    "\n",
    "The dataset used is 'devset.json', the predicted result is 'devset_result.json'.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-06T09:20:54.413365Z",
     "start_time": "2019-05-06T09:20:10.863991Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_evidence(res_data):\n",
    "    for key in list(res_data)[:10]:\n",
    "        res_data[key][\"evidence\"] = []\n",
    "        tfidf_result = retrieval_evidence(res_data[key][\"claim\"],keys[:5000],final_dic)\n",
    "        for res in tfidf_result:\n",
    "            res_data[key][\"evidence\"].append([document[res].split()[0], document[res].split()[1]])\n",
    "    return res_data\n",
    "\n",
    "#testing, for top 10 instances in the dev, and only consider top 100 sentences in the documents.\n",
    "predicted_train = get_evidence(res_data)\n",
    "\n",
    "#for key in list(predicted_train)[:10]:\n",
    " #   print(predicted_train[key])\n",
    "  #  print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gensim library.\n",
    "\n",
    "Use gensim library to calculate the coscine similarity in IR."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-07T07:53:52.493929Z",
     "start_time": "2019-05-07T07:53:52.468714Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['i', \"'m\", 'taking', 'the', 'show', 'on', 'the', 'road', '.'], ['my', 'socks', 'are', 'a', 'force', 'multiplier', '.'], ['i', 'am', 'the', 'barber', 'who', 'cuts', 'everyone', \"'s\", 'hair', 'who', 'does', \"n't\", 'cut', 'their', 'own', '.'], ['legend', 'has', 'it', 'that', 'the', 'mind', 'is', 'a', 'mad', 'monkey', '.'], ['i', 'make', 'my', 'own', 'fun', '.']]\n",
      "Dictionary(36 unique tokens: [\"'m\", '.', 'i', 'on', 'road']...)\n",
      "13\n",
      "Number of words in dictionary: 36\n",
      "0 'm\n",
      "1 .\n",
      "2 i\n",
      "3 on\n",
      "4 road\n",
      "5 show\n",
      "6 taking\n",
      "7 the\n",
      "8 a\n",
      "9 are\n",
      "10 force\n",
      "11 multiplier\n",
      "12 my\n",
      "13 socks\n",
      "14 's\n",
      "15 am\n",
      "16 barber\n",
      "17 cut\n",
      "18 cuts\n",
      "19 does\n",
      "20 everyone\n",
      "21 hair\n",
      "22 n't\n",
      "23 own\n",
      "24 their\n",
      "25 who\n",
      "26 has\n",
      "27 is\n",
      "28 it\n",
      "29 legend\n",
      "30 mad\n",
      "31 mind\n",
      "32 monkey\n",
      "33 that\n",
      "34 fun\n",
      "35 make\n",
      "[[(0, 1), (1, 1), (2, 1), (3, 1), (4, 1), (5, 1), (6, 1), (7, 2)], [(1, 1), (8, 1), (9, 1), (10, 1), (11, 1), (12, 1), (13, 1)], [(1, 1), (2, 1), (7, 1), (14, 1), (15, 1), (16, 1), (17, 1), (18, 1), (19, 1), (20, 1), (21, 1), (22, 1), (23, 1), (24, 1), (25, 2)], [(1, 1), (7, 1), (8, 1), (26, 1), (27, 1), (28, 1), (29, 1), (30, 1), (31, 1), (32, 1), (33, 1)], [(1, 1), (2, 1), (12, 1), (23, 1), (34, 1), (35, 1)]]\n",
      "TfidfModel(num_docs=5, num_nnz=47)\n"
     ]
    }
   ],
   "source": [
    "import gensim\n",
    "\n",
    "raw_documents = [\"I'm taking the show on the road.\",\n",
    "                 \"My socks are a force multiplier.\",\n",
    "                 \"I am the barber who cuts everyone's hair who doesn't cut their own.\",\n",
    "                 \"Legend has it that the mind is a mad monkey.\",\n",
    "                 \"I make my own fun.\"]\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "gen_docs = [[w.lower() for w in word_tokenize(text)] for text in raw_documents]\n",
    "\n",
    "dictionary = gensim.corpora.Dictionary(gen_docs)\n",
    "print(gen_docs)\n",
    "print(dictionary)\n",
    "print(dictionary.token2id['socks'])\n",
    "print(\"Number of words in dictionary:\",len(dictionary))\n",
    "for i in range(len(dictionary)):\n",
    "    print(i, dictionary[i])\n",
    "\n",
    "corpus = [dictionary.doc2bow(gen_doc) for gen_doc in gen_docs]\n",
    "print(corpus)\n",
    "\n",
    "tf_idf = gensim.models.TfidfModel(corpus)\n",
    "print(tf_idf)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
