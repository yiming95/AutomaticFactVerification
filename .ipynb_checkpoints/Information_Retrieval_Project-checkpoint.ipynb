{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Information Retrieval\n",
    "\n",
    "This is the document retrieval and sentence retrieval part of the project."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  1. Unpack the zip file.\n",
    "    \n",
    "Unpack the 'wiki-pages-text.zip' in the current directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-09T16:18:13.994080Z",
     "start_time": "2019-05-09T16:18:13.991649Z"
    }
   },
   "outputs": [],
   "source": [
    "import zipfile\n",
    "def unpack():\n",
    "    with zipfile.ZipFile('wiki-pages-text.zip') as file:\n",
    "        file.extractall()\n",
    "# unpack()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Load file. \n",
    "\n",
    "Load the training dataset and the wiki txt file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-10T07:07:16.457713Z",
     "start_time": "2019-05-10T07:06:56.830845Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of the train data is: 145449\n",
      "Length of the dev data is: 5001\n",
      "Length of the dev result data is: 5001\n",
      "Length of the test data is: 14997\n",
      "Length of the corpus is: 25248397\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "with open('train.json', 'r') as f:  # load training dataset\n",
    "        train_data = json.load(f)   \n",
    "print(\"Length of the train data is: \" + str(len(train_data)))\n",
    "\n",
    "with open('devset.json', 'r') as f1:  # load dev dataset\n",
    "        dev_data = json.load(f1) \n",
    "print(\"Length of the dev data is: \" + str(len(dev_data)))\n",
    "\n",
    "with open('devset_result.json', 'r') as f2:  # store result \n",
    "        res_data = json.load(f2) \n",
    "print(\"Length of the dev result data is: \" + str(len(res_data)))\n",
    "\n",
    "with open('test-unlabelled.json', 'r') as f3:  # store result \n",
    "     test_data = json.load(f3) \n",
    "print(\"Length of the test data is: \" + str(len(test_data)))\n",
    "        \n",
    "# appeand all the wiki txt sentences to one document\n",
    "def loadfile(folder): \n",
    "    corpus = []\n",
    "    list_of_files = os.listdir(folder)\n",
    "    \n",
    "    for file in list_of_files:\n",
    "        try:\n",
    "            filename = os.path.join(folder, file)\n",
    "            with open(filename, 'r') as doc:\n",
    "                for line in doc:\n",
    "                    corpus.append(line)     \n",
    "        except Exception as e:\n",
    "            print(\"No files found here!\")\n",
    "            raise e\n",
    "    return corpus\n",
    "corpus = loadfile(\"wiki-pages-text\")\n",
    "print(\"Length of the corpus is: \" + str(len(corpus)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pandas process the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-10T07:07:26.093127Z",
     "start_time": "2019-05-10T07:07:20.293006Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1000, 3)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>page_identifier</th>\n",
       "      <th>sentence_number</th>\n",
       "      <th>sentence_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Alexander_McNair</td>\n",
       "      <td>0</td>\n",
       "      <td>Alexander McNair -LRB- May 5   1775 -- March 1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Alexander_McNair</td>\n",
       "      <td>1</td>\n",
       "      <td>He was the first Governor of Missouri from its...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Alexander_McNair</td>\n",
       "      <td>4</td>\n",
       "      <td>McNair was born in Lancaster in the Province o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Alexander_McNair</td>\n",
       "      <td>5</td>\n",
       "      <td>His grandfather   David McNair   Sr.   immigra...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Alexander_McNair</td>\n",
       "      <td>6</td>\n",
       "      <td>David McNair   Jr.   Alexander 's father -LRB-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Alexander_McNair</td>\n",
       "      <td>7</td>\n",
       "      <td>Alexander went to school as a child   and atte...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Alexander_McNair</td>\n",
       "      <td>8</td>\n",
       "      <td>He reached an agreement with his mother and br...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Alexander_McNair</td>\n",
       "      <td>9</td>\n",
       "      <td>Alexander was defeated .\\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Alexander_McNair</td>\n",
       "      <td>10</td>\n",
       "      <td>He became a member of the Pennsylvania militia...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Alexander_McNair</td>\n",
       "      <td>13</td>\n",
       "      <td>In 1804   McNair traveled to what is now Misso...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    page_identifier sentence_number  \\\n",
       "0  Alexander_McNair               0   \n",
       "1  Alexander_McNair               1   \n",
       "2  Alexander_McNair               4   \n",
       "3  Alexander_McNair               5   \n",
       "4  Alexander_McNair               6   \n",
       "5  Alexander_McNair               7   \n",
       "6  Alexander_McNair               8   \n",
       "7  Alexander_McNair               9   \n",
       "8  Alexander_McNair              10   \n",
       "9  Alexander_McNair              13   \n",
       "\n",
       "                                       sentence_text  \n",
       "0  Alexander McNair -LRB- May 5   1775 -- March 1...  \n",
       "1  He was the first Governor of Missouri from its...  \n",
       "2  McNair was born in Lancaster in the Province o...  \n",
       "3  His grandfather   David McNair   Sr.   immigra...  \n",
       "4  David McNair   Jr.   Alexander 's father -LRB-...  \n",
       "5  Alexander went to school as a child   and atte...  \n",
       "6  He reached an agreement with his mother and br...  \n",
       "7                         Alexander was defeated .\\n  \n",
       "8  He became a member of the Pennsylvania militia...  \n",
       "9  In 1804   McNair traveled to what is now Misso...  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus_df = pd.DataFrame(corpus[:1000])\n",
    "corpus_df.columns = ['text']\n",
    "\n",
    "corpus_df['page_identifier'] = corpus_df.text.apply(lambda x: x.split(' ')[0])  \n",
    "corpus_df['sentence_number'] = corpus_df.text.apply(lambda x: x.split(' ')[1]) \n",
    "corpus_df['sentence_text'] = corpus_df.text.apply(lambda x: x.split(' ')[2:])  \n",
    "corpus_df['sentence_text'] = [','.join(map(str, l)) for l in corpus_df['sentence_text']]\n",
    "corpus_df[\"sentence_text\"] = corpus_df['sentence_text'].str.replace(',',' ')\n",
    "corpus_df = corpus_df.drop('text', 1)\n",
    "\n",
    "print(corpus_df.shape)\n",
    "corpus_df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Preprocess \n",
    "\n",
    "Preprocess includes: strip punctuations, tokenize,lemma, lower case, remove stop words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-10T07:07:31.407364Z",
     "start_time": "2019-05-10T07:07:29.404018Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/zhangyiming/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "tokenizer = nltk.tokenize.RegexpTokenizer(r'\\w+')\n",
    "lemmatizer = nltk.stem.WordNetLemmatizer()\n",
    "\n",
    "nltk.download('wordnet')\n",
    "def lemmatize(word):\n",
    "    lemma = lemmatizer.lemmatize(word,'v')\n",
    "    if lemma == word:\n",
    "        lemma = lemmatizer.lemmatize(word,'n')\n",
    "    return lemma\n",
    "\n",
    "def pre_process(comment) -> str:\n",
    "    # lower cased\n",
    "    comment = comment.lower()\n",
    "    # tokenize\n",
    "    words = tokenizer.tokenize(comment)\n",
    "    # lemmatize \n",
    "    words = [lemmatize(w) for w in words]\n",
    "    processed_comment = \" \".join(words)\n",
    "    return processed_comment\n",
    "\n",
    "def process_dataset(dataset: pd.DataFrame) -> pd.DataFrame:\n",
    "    corpus = dataset['sentence_text']\n",
    "    processed_corpus = corpus.apply(lambda text: pre_process(text))\n",
    "    dataset['sentence_text'] = processed_corpus.iloc[0: len(dataset)]\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Save the pre-processed corpus dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-10T07:07:39.727255Z",
     "start_time": "2019-05-10T07:07:36.612317Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>page_identifier</th>\n",
       "      <th>sentence_number</th>\n",
       "      <th>sentence_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Alexander_McNair</td>\n",
       "      <td>0</td>\n",
       "      <td>alexander mcnair lrb may 5 1775 march 18 1826 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Alexander_McNair</td>\n",
       "      <td>1</td>\n",
       "      <td>he be the first governor of missouri from it e...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Alexander_McNair</td>\n",
       "      <td>4</td>\n",
       "      <td>mcnair be bear in lancaster in the province of...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Alexander_McNair</td>\n",
       "      <td>5</td>\n",
       "      <td>his grandfather david mcnair sr immigrate to p...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Alexander_McNair</td>\n",
       "      <td>6</td>\n",
       "      <td>david mcnair jr alexander s father lrb b 1736 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Alexander_McNair</td>\n",
       "      <td>7</td>\n",
       "      <td>alexander go to school a a child and attend on...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Alexander_McNair</td>\n",
       "      <td>8</td>\n",
       "      <td>he reach an agreement with his mother and brot...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Alexander_McNair</td>\n",
       "      <td>9</td>\n",
       "      <td>alexander be defeat</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Alexander_McNair</td>\n",
       "      <td>10</td>\n",
       "      <td>he become a member of the pennsylvania militia...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Alexander_McNair</td>\n",
       "      <td>13</td>\n",
       "      <td>in 1804 mcnair travel to what be now missouri ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    page_identifier sentence_number  \\\n",
       "0  Alexander_McNair               0   \n",
       "1  Alexander_McNair               1   \n",
       "2  Alexander_McNair               4   \n",
       "3  Alexander_McNair               5   \n",
       "4  Alexander_McNair               6   \n",
       "5  Alexander_McNair               7   \n",
       "6  Alexander_McNair               8   \n",
       "7  Alexander_McNair               9   \n",
       "8  Alexander_McNair              10   \n",
       "9  Alexander_McNair              13   \n",
       "\n",
       "                                       sentence_text  \n",
       "0  alexander mcnair lrb may 5 1775 march 18 1826 ...  \n",
       "1  he be the first governor of missouri from it e...  \n",
       "2  mcnair be bear in lancaster in the province of...  \n",
       "3  his grandfather david mcnair sr immigrate to p...  \n",
       "4  david mcnair jr alexander s father lrb b 1736 ...  \n",
       "5  alexander go to school a a child and attend on...  \n",
       "6  he reach an agreement with his mother and brot...  \n",
       "7                                alexander be defeat  \n",
       "8  he become a member of the pennsylvania militia...  \n",
       "9  in 1804 mcnair travel to what be now missouri ...  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import datetime\n",
    "# start = datetime.datetime.now()\n",
    "processed_corpus_df = pd.DataFrame(process_dataset(corpus_df))\n",
    "processed_corpus_df.to_pickle(\"./processed_corpus.pkl\")\n",
    "processed_corpus_df.head(10)\n",
    "# end = datetime.datetime.now()\n",
    "# print(end-start)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load pre-processed corpus dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-10T07:07:43.293504Z",
     "start_time": "2019-05-10T07:07:43.282748Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1000, 3)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>page_identifier</th>\n",
       "      <th>sentence_number</th>\n",
       "      <th>sentence_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Alexander_McNair</td>\n",
       "      <td>0</td>\n",
       "      <td>alexander mcnair lrb may 5 1775 march 18 1826 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Alexander_McNair</td>\n",
       "      <td>1</td>\n",
       "      <td>he be the first governor of missouri from it e...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Alexander_McNair</td>\n",
       "      <td>4</td>\n",
       "      <td>mcnair be bear in lancaster in the province of...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Alexander_McNair</td>\n",
       "      <td>5</td>\n",
       "      <td>his grandfather david mcnair sr immigrate to p...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Alexander_McNair</td>\n",
       "      <td>6</td>\n",
       "      <td>david mcnair jr alexander s father lrb b 1736 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Alexander_McNair</td>\n",
       "      <td>7</td>\n",
       "      <td>alexander go to school a a child and attend on...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Alexander_McNair</td>\n",
       "      <td>8</td>\n",
       "      <td>he reach an agreement with his mother and brot...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Alexander_McNair</td>\n",
       "      <td>9</td>\n",
       "      <td>alexander be defeat</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Alexander_McNair</td>\n",
       "      <td>10</td>\n",
       "      <td>he become a member of the pennsylvania militia...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Alexander_McNair</td>\n",
       "      <td>13</td>\n",
       "      <td>in 1804 mcnair travel to what be now missouri ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    page_identifier sentence_number  \\\n",
       "0  Alexander_McNair               0   \n",
       "1  Alexander_McNair               1   \n",
       "2  Alexander_McNair               4   \n",
       "3  Alexander_McNair               5   \n",
       "4  Alexander_McNair               6   \n",
       "5  Alexander_McNair               7   \n",
       "6  Alexander_McNair               8   \n",
       "7  Alexander_McNair               9   \n",
       "8  Alexander_McNair              10   \n",
       "9  Alexander_McNair              13   \n",
       "\n",
       "                                       sentence_text  \n",
       "0  alexander mcnair lrb may 5 1775 march 18 1826 ...  \n",
       "1  he be the first governor of missouri from it e...  \n",
       "2  mcnair be bear in lancaster in the province of...  \n",
       "3  his grandfather david mcnair sr immigrate to p...  \n",
       "4  david mcnair jr alexander s father lrb b 1736 ...  \n",
       "5  alexander go to school a a child and attend on...  \n",
       "6  he reach an agreement with his mother and brot...  \n",
       "7                                alexander be defeat  \n",
       "8  he become a member of the pennsylvania militia...  \n",
       "9  in 1804 mcnair travel to what be now missouri ...  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_processed_corpus_df = pd.read_pickle(\"./processed_corpus.pkl\")\n",
    "print(load_processed_corpus_df.shape)\n",
    "load_processed_corpus_df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Use Sklearn to build tf-idf.\n",
    "\n",
    "tfidf_vectorizer is the tf-idf model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-10T07:07:48.026650Z",
     "start_time": "2019-05-10T07:07:47.986084Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary Size :  4650\n",
      "Shape of Matrix :  (4650, 1000)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import pickle\n",
    "\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "tfidf = tfidf_vectorizer.fit_transform(load_processed_corpus_df['sentence_text'])\n",
    "tfidf = tfidf.T\n",
    "\n",
    "print('Vocabulary Size : ', len(tfidf_vectorizer.get_feature_names()))\n",
    "print('Shape of Matrix : ', tfidf.shape)\n",
    "\n",
    "pickle.dump(tfidf, open(\"tfidf.pickle\",\"wb\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Apply SVD "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-10T07:08:03.069922Z",
     "start_time": "2019-05-10T07:08:01.339309Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4650, 20)\n",
      "(1000, 20)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Applying SVD\n",
    "K= 20 # number of desirable features \n",
    "U, s, VT = np.linalg.svd(tfidf.toarray())\n",
    "tfidf_reduced = np.dot(U[:,:K], np.dot(np.diag(s[:K]), VT[:K, :]))\n",
    "\n",
    "# Getting document and term representation\n",
    "terms_rep = np.dot(U[:,:K], np.diag(s[:K])) # M X K matrix where M = Vocabulary Size and N = Number of documents\n",
    "docs_rep = np.dot(np.diag(s[:K]), VT[:K, :]).T # N x K matrix \n",
    "\n",
    "print(terms_rep.shape)\n",
    "print(docs_rep.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  6. Test tfidf with query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-10T07:08:31.160220Z",
     "start_time": "2019-05-10T07:08:31.136129Z"
    }
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "vocabulary_ not found",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-42461689b1c5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0mquery_rep\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"Alexander  Alatskivi\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m \u001b[0mpre_query\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mlsa_query\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquery_rep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpre_query\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdocs_rep\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-9-42461689b1c5>\u001b[0m in \u001b[0;36mlsa_query\u001b[0;34m(query)\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mquery_rep\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mq\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpre_process\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquery\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0mq\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtfidf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocabulary_\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m             \u001b[0mquery_rep\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtfidf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvocabulary_\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mq\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.7/site-packages/scipy/sparse/base.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, attr)\u001b[0m\n\u001b[1;32m    684\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetnnz\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    685\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 686\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mattr\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\" not found\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    687\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    688\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: vocabulary_ not found"
     ]
    }
   ],
   "source": [
    "import math\n",
    "from scipy.spatial.distance import cosine\n",
    "\n",
    "load_tfidf = pickle.load(open(\"tfidf.pickle\",\"rb\"))\n",
    "\n",
    "def lsa_query(query):\n",
    "    query_rep = []\n",
    "    for q in pre_process(query).split():\n",
    "        if q in tfidf_vectorizer.vocabulary_:\n",
    "            query_rep.append(tfidf_vectorizer.vocabulary_[q])\n",
    "        else:\n",
    "            continue\n",
    "    query_rep = np.mean(terms_rep[query_rep],axis=0)\n",
    "    return query_rep\n",
    "\n",
    "query_rep = \"Alexander  Alatskivi\"\n",
    "pre_query= lsa_query(query_rep)\n",
    "print(pre_query)\n",
    "print(docs_rep[0])\n",
    "print(1- cosine(pre_query,docs_rep[0]))\n",
    "\n",
    "query_doc_cos_dist = []\n",
    "\n",
    "for doc_rep in docs_rep:\n",
    "    query_doc_cos_dist.append(cosine(pre_query, doc_rep))\n",
    "\n",
    "print(len(query_doc_cos_dist))\n",
    "print(query_doc_cos_dist[0])\n",
    "print(query_doc_cos_dist[1])\n",
    "\n",
    "query_doc_sort_index = np.argsort(np.array(query_doc_cos_dist))\n",
    "\n",
    "count = 0\n",
    "for rank, sort_index in enumerate(query_doc_sort_index):\n",
    "    print ('Rank : ', rank, ' Consine : ', 1 - query_doc_cos_dist[sort_index], 'Page Identifier: ',load_processed_corpus_df['page_identifier'][sort_index], 'Sentence number: ', load_processed_corpus_df['sentence_number'][sort_index], ' Sentence : ', load_processed_corpus_df['sentence_text'][sort_index])\n",
    "    if count == 4 :\n",
    "        break\n",
    "    else:\n",
    "        count += 1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  7. Retrieval Evidence and write the result to json file.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-10T06:02:12.356586Z",
     "start_time": "2019-05-10T06:02:12.044828Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'claim': 'Colin Kaepernick became a starting quarterback during the 49ers 63rd season in the National Football League.', 'label': 'NOT ENOUGH INFO', 'evidence': [['Alexander_McNair', 9], ['Alexander_Carson_-LRB-filmmaker-RRB-', 0], ['Alexander_Frederick,_Landgrave_of_Hesse', 0], ['Alexander_Martin_-LRB-sport_shooter-RRB-', 0], ['Aleksandr_Krotov', 0]]}\n",
      "\n",
      "\n",
      "{'claim': 'Tilda Swinton is a vegan.', 'label': 'NOT ENOUGH INFO', 'evidence': [['Alexander_McNair', 9], ['Alexander_Carson_-LRB-filmmaker-RRB-', 0], ['Alexander_Frederick,_Landgrave_of_Hesse', 0], ['Alexander_Martin_-LRB-sport_shooter-RRB-', 0], ['Aleksandr_Krotov', 0]]}\n",
      "\n",
      "\n",
      "{'claim': 'Fox 2000 Pictures released the film Soul Food.', 'label': 'SUPPORTS', 'evidence': [['Alexander_McNair', 9], ['Alexander_Carson_-LRB-filmmaker-RRB-', 0], ['Alexander_Frederick,_Landgrave_of_Hesse', 0], ['Alexander_Martin_-LRB-sport_shooter-RRB-', 0], ['Aleksandr_Krotov', 0]]}\n",
      "\n",
      "\n",
      "{'claim': 'Anne Rice was born in New Jersey.', 'label': 'NOT ENOUGH INFO', 'evidence': [['Alexander_McNair', 9], ['Alexander_Carson_-LRB-filmmaker-RRB-', 0], ['Alexander_Frederick,_Landgrave_of_Hesse', 0], ['Alexander_Martin_-LRB-sport_shooter-RRB-', 0], ['Aleksandr_Krotov', 0]]}\n",
      "\n",
      "\n",
      "{'claim': 'Telemundo is a English-language television network.', 'label': 'REFUTES', 'evidence': [['Alexander_McNair', 9], ['Alexander_Carson_-LRB-filmmaker-RRB-', 0], ['Alexander_Frederick,_Landgrave_of_Hesse', 0], ['Alexander_Martin_-LRB-sport_shooter-RRB-', 0], ['Aleksandr_Krotov', 0]]}\n",
      "\n",
      "\n",
      "{'claim': \"Damon Albarn's debut album was released in 2011.\", 'label': 'REFUTES', 'evidence': [['Alexander_McNair', 9], ['Alexander_Carson_-LRB-filmmaker-RRB-', 0], ['Alexander_Frederick,_Landgrave_of_Hesse', 0], ['Alexander_Martin_-LRB-sport_shooter-RRB-', 0], ['Aleksandr_Krotov', 0]]}\n",
      "\n",
      "\n",
      "{'claim': 'There is a capital called Mogadishu.', 'label': 'SUPPORTS', 'evidence': [['Alexander_McNair', 9], ['Alexander_Carson_-LRB-filmmaker-RRB-', 0], ['Alexander_Frederick,_Landgrave_of_Hesse', 0], ['Alexander_Martin_-LRB-sport_shooter-RRB-', 0], ['Aleksandr_Krotov', 0]]}\n",
      "\n",
      "\n",
      "{'claim': 'Savages was exclusively a German film.', 'label': 'REFUTES', 'evidence': [['Alexander_McNair', 9], ['Alexander_Carson_-LRB-filmmaker-RRB-', 0], ['Alexander_Frederick,_Landgrave_of_Hesse', 0], ['Alexander_Martin_-LRB-sport_shooter-RRB-', 0], ['Aleksandr_Krotov', 0]]}\n",
      "\n",
      "\n",
      "{'claim': 'Happiness in Slavery is a gospel song by Nine Inch Nails.', 'label': 'NOT ENOUGH INFO', 'evidence': [['Alexander_McNair', 9], ['Alexander_Carson_-LRB-filmmaker-RRB-', 0], ['Alexander_Frederick,_Landgrave_of_Hesse', 0], ['Alexander_Martin_-LRB-sport_shooter-RRB-', 0], ['Aleksandr_Krotov', 0]]}\n",
      "\n",
      "\n",
      "{'claim': 'Andrew Kevin Walker is only Chinese.', 'label': 'REFUTES', 'evidence': [['Alexander_McNair', 9], ['Alexander_Carson_-LRB-filmmaker-RRB-', 0], ['Alexander_Frederick,_Landgrave_of_Hesse', 0], ['Alexander_Martin_-LRB-sport_shooter-RRB-', 0], ['Aleksandr_Krotov', 0]]}\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from scipy.spatial.distance import cosine\n",
    "\n",
    "load_tfidf = pickle.load(open(\"tfidf.pickle\",\"rb\"))\n",
    "\n",
    "def lsa_query(query):\n",
    "    query_rep = []\n",
    "    for q in pre_process(query).split():\n",
    "        if q in vectorizer.vocabulary_:\n",
    "            query_rep.append(vectorizer.vocabulary_[q])\n",
    "        else:\n",
    "            continue\n",
    "    query_rep = np.mean(terms_rep[query_rep],axis=0)\n",
    "    return query_rep\n",
    "\n",
    "def retrieval_evidence(res_data):\n",
    "    for key in list(res_data)[:10]:\n",
    "        res_data[key][\"evidence\"] = []\n",
    "        lsa_query(res_data[key][\"claim\"])\n",
    "        \n",
    "        query_doc_cos_dist = []\n",
    "        for doc_rep in docs_rep:\n",
    "            query_doc_cos_dist.append(cosine(pre_query, doc_rep))\n",
    "        query_doc_sort_index = np.argsort(np.array(query_doc_cos_dist))\n",
    "        \n",
    "        # retrieval top 5 evidence\n",
    "        count = 0\n",
    "        for rank, sort_index in enumerate(query_doc_sort_index):\n",
    "            res_data[key][\"evidence\"].append([load_processed_corpus_df['page_identifier'][sort_index],int(load_processed_corpus_df['sentence_number'][sort_index])])\n",
    "            if count == 4 :\n",
    "                break\n",
    "            else:\n",
    "                count += 1\n",
    "                \n",
    "    return res_data\n",
    "\n",
    "#testing, for top 10 instances in the dev, and only consider top 100 sentences in the documents.\n",
    "predicted_train = retrieval_evidence(res_data)\n",
    "\n",
    "for key in list(predicted_train)[:10]:\n",
    "    print(predicted_train[key])\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8. Another method: Gensim library tfidf model and LSA .\n",
    "\n",
    "Use gensim library to calculate the coscine similarity in IR."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-07T07:53:52.493929Z",
     "start_time": "2019-05-07T07:53:52.468714Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['i', \"'m\", 'taking', 'the', 'show', 'on', 'the', 'road', '.'], ['my', 'socks', 'are', 'a', 'force', 'multiplier', '.'], ['i', 'am', 'the', 'barber', 'who', 'cuts', 'everyone', \"'s\", 'hair', 'who', 'does', \"n't\", 'cut', 'their', 'own', '.'], ['legend', 'has', 'it', 'that', 'the', 'mind', 'is', 'a', 'mad', 'monkey', '.'], ['i', 'make', 'my', 'own', 'fun', '.']]\n",
      "Dictionary(36 unique tokens: [\"'m\", '.', 'i', 'on', 'road']...)\n",
      "13\n",
      "Number of words in dictionary: 36\n",
      "0 'm\n",
      "1 .\n",
      "2 i\n",
      "3 on\n",
      "4 road\n",
      "5 show\n",
      "6 taking\n",
      "7 the\n",
      "8 a\n",
      "9 are\n",
      "10 force\n",
      "11 multiplier\n",
      "12 my\n",
      "13 socks\n",
      "14 's\n",
      "15 am\n",
      "16 barber\n",
      "17 cut\n",
      "18 cuts\n",
      "19 does\n",
      "20 everyone\n",
      "21 hair\n",
      "22 n't\n",
      "23 own\n",
      "24 their\n",
      "25 who\n",
      "26 has\n",
      "27 is\n",
      "28 it\n",
      "29 legend\n",
      "30 mad\n",
      "31 mind\n",
      "32 monkey\n",
      "33 that\n",
      "34 fun\n",
      "35 make\n",
      "[[(0, 1), (1, 1), (2, 1), (3, 1), (4, 1), (5, 1), (6, 1), (7, 2)], [(1, 1), (8, 1), (9, 1), (10, 1), (11, 1), (12, 1), (13, 1)], [(1, 1), (2, 1), (7, 1), (14, 1), (15, 1), (16, 1), (17, 1), (18, 1), (19, 1), (20, 1), (21, 1), (22, 1), (23, 1), (24, 1), (25, 2)], [(1, 1), (7, 1), (8, 1), (26, 1), (27, 1), (28, 1), (29, 1), (30, 1), (31, 1), (32, 1), (33, 1)], [(1, 1), (2, 1), (12, 1), (23, 1), (34, 1), (35, 1)]]\n",
      "TfidfModel(num_docs=5, num_nnz=47)\n"
     ]
    }
   ],
   "source": [
    "import gensim\n",
    "\n",
    "raw_documents = [\"I'm taking the show on the road.\",\n",
    "                 \"My socks are a force multiplier.\",\n",
    "                 \"I am the barber who cuts everyone's hair who doesn't cut their own.\",\n",
    "                 \"Legend has it that the mind is a mad monkey.\",\n",
    "                 \"I make my own fun.\"]\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "gen_docs = [[w.lower() for w in word_tokenize(text)] for text in raw_documents]\n",
    "\n",
    "dictionary = gensim.corpora.Dictionary(gen_docs)\n",
    "print(gen_docs)\n",
    "print(dictionary)\n",
    "print(dictionary.token2id['socks'])\n",
    "print(\"Number of words in dictionary:\",len(dictionary))\n",
    "for i in range(len(dictionary)):\n",
    "    print(i, dictionary[i])\n",
    "\n",
    "corpus = [dictionary.doc2bow(gen_doc) for gen_doc in gen_docs]\n",
    "print(corpus)\n",
    "\n",
    "tf_idf = gensim.models.TfidfModel(corpus)\n",
    "print(tf_idf)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
