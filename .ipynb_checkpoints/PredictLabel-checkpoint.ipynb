{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict Label with extracted evidence texts\n",
    "This notebook builds the MLP model for RTM step according to the FNC competition paper.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data preprocess"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load data as pandas DF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-05T07:46:17.695125Z",
     "start_time": "2019-05-05T07:46:14.818705Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>NOINFO</th>\n",
       "      <th>REF</th>\n",
       "      <th>SUP</th>\n",
       "      <th>claim</th>\n",
       "      <th>evi_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>Ireland does not have relatively low-lying mou...</td>\n",
       "      <td>Ireland 10 The island 's geography comprises r...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>The drama Dark Matter stars Taylor Schilling.</td>\n",
       "      <td>Taylor_Schilling 2 She made her film debut in ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>In 1932, Prussia was taken over.</td>\n",
       "      <td>Prussia 30 In the Weimar Republic , the state ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>IZombie premiered in 2015.</td>\n",
       "      <td>IZombie_-LRB-TV_series-RRB- 2 The series premi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>Ronald Reagan had a nationality.</td>\n",
       "      <td>Ronald_Reagan 0 Ronald Wilson Reagan -LRB- -LS...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>Samoa Joe wrestles professionally.</td>\n",
       "      <td>Samoa_Joe 0 Nuufolau Joel `` Joe '' Seanoa -LR...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>University of Oxford is in the universe.</td>\n",
       "      <td>University_of_Oxford 0 The University of Oxfor...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>The Renaissance began online.</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>Portia de Rossi appeared on Scandal.</td>\n",
       "      <td>Portia_de_Rossi 1 She appeared as a regular ca...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>The Berlin Wall was only standing for 10 years.</td>\n",
       "      <td>Berlin_Wall 0 The Berlin Wall -LRB- Berliner M...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   NOINFO  REF  SUP                                              claim  \\\n",
       "0       0    1    0  Ireland does not have relatively low-lying mou...   \n",
       "1       0    0    1      The drama Dark Matter stars Taylor Schilling.   \n",
       "2       0    0    1                   In 1932, Prussia was taken over.   \n",
       "3       0    0    1                         IZombie premiered in 2015.   \n",
       "4       0    0    1                   Ronald Reagan had a nationality.   \n",
       "5       0    0    1                 Samoa Joe wrestles professionally.   \n",
       "6       0    0    1           University of Oxford is in the universe.   \n",
       "7       1    0    0                      The Renaissance began online.   \n",
       "8       0    0    1               Portia de Rossi appeared on Scandal.   \n",
       "9       0    1    0    The Berlin Wall was only standing for 10 years.   \n",
       "\n",
       "                                            evi_text  \n",
       "0  Ireland 10 The island 's geography comprises r...  \n",
       "1  Taylor_Schilling 2 She made her film debut in ...  \n",
       "2  Prussia 30 In the Weimar Republic , the state ...  \n",
       "3  IZombie_-LRB-TV_series-RRB- 2 The series premi...  \n",
       "4  Ronald_Reagan 0 Ronald Wilson Reagan -LRB- -LS...  \n",
       "5  Samoa_Joe 0 Nuufolau Joel `` Joe '' Seanoa -LR...  \n",
       "6  University_of_Oxford 0 The University of Oxfor...  \n",
       "7                                                     \n",
       "8  Portia_de_Rossi 1 She appeared as a regular ca...  \n",
       "9  Berlin_Wall 0 The Berlin Wall -LRB- Berliner M...  "
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "train_file_path = \"./JSONFiles/\" + \"train_with_text.json\"\n",
    "use_test_file = False\n",
    "if use_test_file:\n",
    "    test_file_path = './JSONFiles/' + 'test_with_text.json'\n",
    "else:\n",
    "    test_file_path = './JSONFiles/' + 'dev_with_text.json'\n",
    "\n",
    "with open(train_file_path, mode='r') as f:\n",
    "    train = json.load(f)\n",
    "with open(test_file_path, mode='r') as f:\n",
    "    test = json.load(f)\n",
    "\n",
    "def load_training_data(dataset: dict) -> list:\n",
    "    dataset_list = []\n",
    "    for key in dataset.keys():\n",
    "        record = dataset.get(key)\n",
    "        claim = record.get(\"claim\")\n",
    "        evi_texts = record.get(\"evidence_texts\")\n",
    "        text = \"\"\n",
    "        for evi in evi_texts:\n",
    "            text += evi\n",
    "        SUP = NOINFO = REF = 0\n",
    "        if record.get(\"label\") == \"SUPPORTS\":\n",
    "            SUP = 1\n",
    "        elif record.get(\"label\") == \"REFUTES\":\n",
    "            REF = 1\n",
    "        else:\n",
    "            NOINFO = 1\n",
    "        dataset_record = {\n",
    "            \"claim\": claim,\n",
    "            \"evi_text\": text,\n",
    "            \"SUP\": SUP,\n",
    "            \"NOINFO\": NOINFO,\n",
    "            \"REF\": REF\n",
    "        }\n",
    "        dataset_list.append(dataset_record)\n",
    "    return dataset_list\n",
    "\n",
    "def load_test_data(dataset: dict) -> list:\n",
    "    dataset_list = []\n",
    "    for key in dataset.keys():\n",
    "        record = dataset.get(key)\n",
    "        claim = record.get(\"claim\")\n",
    "        evi_index = record.get(\"evidence\")\n",
    "        evi_texts = record.get(\"evidence_texts\")\n",
    "        text = \"\"\n",
    "        for evi in evi_texts:\n",
    "            text += evi\n",
    "\n",
    "        dataset_record = {\n",
    "            \"key\": key,\n",
    "            \"claim\": claim,\n",
    "            \"evidence\": evi_index,\n",
    "            \"evi_text\": text\n",
    "        }\n",
    "        dataset_list.append(dataset_record)\n",
    "    return dataset_list\n",
    "\n",
    "train_df = pd.DataFrame(load_training_data(train))\n",
    "test_df = pd.DataFrame(load_test_data(test))\n",
    "\n",
    "train_df[0: 10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-05T07:46:27.116037Z",
     "start_time": "2019-05-05T07:46:27.097480Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>claim</th>\n",
       "      <th>evi_text</th>\n",
       "      <th>evidence</th>\n",
       "      <th>key</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Ripon College's student number totaled in at a...</td>\n",
       "      <td>Ripon_College_-LRB-Wisconsin-RRB- 1 As of 2015...</td>\n",
       "      <td>[[Ripon_College_-LRB-Wisconsin-RRB-, 1]]</td>\n",
       "      <td>100038</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Kesha was baptized on March 1st, 1987.</td>\n",
       "      <td>Kesha 0 Kesha Rose Sebert -LRB- -LSB- ˈkɛʃə_ro...</td>\n",
       "      <td>[[Kesha, 0]]</td>\n",
       "      <td>100083</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Birthday Song (2 Chainz song) was banned by So...</td>\n",
       "      <td>Birthday_Song_-LRB-2_Chainz_song-RRB- 1 The so...</td>\n",
       "      <td>[[Birthday_Song_-LRB-2_Chainz_song-RRB-, 1]]</td>\n",
       "      <td>100169</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>The University of Illinois at Chicago is a col...</td>\n",
       "      <td>University_of_Illinois_at_Chicago 0 The Univer...</td>\n",
       "      <td>[[University_of_Illinois_at_Chicago, 0]]</td>\n",
       "      <td>100234</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>French Indochina was officially known as the I...</td>\n",
       "      <td></td>\n",
       "      <td>[]</td>\n",
       "      <td>100359</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Damon Albarn has refused to ever work with Bri...</td>\n",
       "      <td>Damon_Albarn 17 His debut solo studio album Ev...</td>\n",
       "      <td>[[Damon_Albarn, 17]]</td>\n",
       "      <td>100366</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Lost (TV series) is a series of plays.</td>\n",
       "      <td>Lost_-LRB-TV_series-RRB- 0 Lost is an American...</td>\n",
       "      <td>[[Lost_-LRB-TV_series-RRB-, 0]]</td>\n",
       "      <td>100429</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Edison Machine Works was barely set up to prod...</td>\n",
       "      <td></td>\n",
       "      <td>[]</td>\n",
       "      <td>100457</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>The human brain is set apart from mammalian br...</td>\n",
       "      <td></td>\n",
       "      <td>[]</td>\n",
       "      <td>100461</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>There are rumors that Augustus' wife, Livia, p...</td>\n",
       "      <td></td>\n",
       "      <td>[]</td>\n",
       "      <td>100481</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               claim  \\\n",
       "0  Ripon College's student number totaled in at a...   \n",
       "1             Kesha was baptized on March 1st, 1987.   \n",
       "2  Birthday Song (2 Chainz song) was banned by So...   \n",
       "3  The University of Illinois at Chicago is a col...   \n",
       "4  French Indochina was officially known as the I...   \n",
       "5  Damon Albarn has refused to ever work with Bri...   \n",
       "6             Lost (TV series) is a series of plays.   \n",
       "7  Edison Machine Works was barely set up to prod...   \n",
       "8  The human brain is set apart from mammalian br...   \n",
       "9  There are rumors that Augustus' wife, Livia, p...   \n",
       "\n",
       "                                            evi_text  \\\n",
       "0  Ripon_College_-LRB-Wisconsin-RRB- 1 As of 2015...   \n",
       "1  Kesha 0 Kesha Rose Sebert -LRB- -LSB- ˈkɛʃə_ro...   \n",
       "2  Birthday_Song_-LRB-2_Chainz_song-RRB- 1 The so...   \n",
       "3  University_of_Illinois_at_Chicago 0 The Univer...   \n",
       "4                                                      \n",
       "5  Damon_Albarn 17 His debut solo studio album Ev...   \n",
       "6  Lost_-LRB-TV_series-RRB- 0 Lost is an American...   \n",
       "7                                                      \n",
       "8                                                      \n",
       "9                                                      \n",
       "\n",
       "                                       evidence     key  \n",
       "0      [[Ripon_College_-LRB-Wisconsin-RRB-, 1]]  100038  \n",
       "1                                  [[Kesha, 0]]  100083  \n",
       "2  [[Birthday_Song_-LRB-2_Chainz_song-RRB-, 1]]  100169  \n",
       "3      [[University_of_Illinois_at_Chicago, 0]]  100234  \n",
       "4                                            []  100359  \n",
       "5                          [[Damon_Albarn, 17]]  100366  \n",
       "6               [[Lost_-LRB-TV_series-RRB-, 0]]  100429  \n",
       "7                                            []  100457  \n",
       "8                                            []  100461  \n",
       "9                                            []  100481  "
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df[0: 10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tokenization and Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-05T07:03:45.329028Z",
     "start_time": "2019-05-05T07:02:46.427009Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/wenbin/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>NOINFO</th>\n",
       "      <th>REF</th>\n",
       "      <th>SUP</th>\n",
       "      <th>claim</th>\n",
       "      <th>evi_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>ireland do not have relatively low lie mountain</td>\n",
       "      <td>ireland 10 the island s geography comprise rel...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>the drama dark matter star taylor schilling</td>\n",
       "      <td>taylor_schilling 2 she make her film debut in ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>in 1932 prussia be take over</td>\n",
       "      <td>prussia 30 in the weimar republic the state of...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>izombie premier in 2015</td>\n",
       "      <td>izombie_ lrb tv_series rrb 2 the series premie...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>ronald reagan have a nationality</td>\n",
       "      <td>ronald_reagan 0 ronald wilson reagan lrb lsb ˈ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>samoa joe wrestle professionally</td>\n",
       "      <td>samoa_joe 0 nuufolau joel joe seanoa lrb bear ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>university of oxford be in the universe</td>\n",
       "      <td>university_of_oxford 0 the university of oxfor...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>the renaissance begin online</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>portia de rossi appear on scandal</td>\n",
       "      <td>portia_de_rossi 1 she appear a a regular cast ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>the berlin wall be only stand for 10 year</td>\n",
       "      <td>berlin_wall 0 the berlin wall lrb berliner mau...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   NOINFO  REF  SUP                                            claim  \\\n",
       "0       0    1    0  ireland do not have relatively low lie mountain   \n",
       "1       0    0    1      the drama dark matter star taylor schilling   \n",
       "2       0    0    1                     in 1932 prussia be take over   \n",
       "3       0    0    1                          izombie premier in 2015   \n",
       "4       0    0    1                 ronald reagan have a nationality   \n",
       "5       0    0    1                 samoa joe wrestle professionally   \n",
       "6       0    0    1          university of oxford be in the universe   \n",
       "7       1    0    0                     the renaissance begin online   \n",
       "8       0    0    1                portia de rossi appear on scandal   \n",
       "9       0    1    0        the berlin wall be only stand for 10 year   \n",
       "\n",
       "                                            evi_text  \n",
       "0  ireland 10 the island s geography comprise rel...  \n",
       "1  taylor_schilling 2 she make her film debut in ...  \n",
       "2  prussia 30 in the weimar republic the state of...  \n",
       "3  izombie_ lrb tv_series rrb 2 the series premie...  \n",
       "4  ronald_reagan 0 ronald wilson reagan lrb lsb ˈ...  \n",
       "5  samoa_joe 0 nuufolau joel joe seanoa lrb bear ...  \n",
       "6  university_of_oxford 0 the university of oxfor...  \n",
       "7                                                     \n",
       "8  portia_de_rossi 1 she appear a a regular cast ...  \n",
       "9  berlin_wall 0 the berlin wall lrb berliner mau...  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "\n",
    "tokenizer = nltk.tokenize.RegexpTokenizer(r'\\w+')\n",
    "lemmatizer = nltk.stem.WordNetLemmatizer()\n",
    "\n",
    "def lemmatize(word):\n",
    "    lemma = lemmatizer.lemmatize(word,'v')\n",
    "    if lemma == word:\n",
    "        lemma = lemmatizer.lemmatize(word,'n')\n",
    "    return lemma\n",
    "\n",
    "def pre_process(comment) -> str:\n",
    "    # lower cased\n",
    "    comment = comment.lower()\n",
    "    # tokenize\n",
    "    words = tokenizer.tokenize(comment)\n",
    "    # lemmatize \n",
    "    words = [lemmatize(w) for w in words]\n",
    "    # remove stop words\n",
    "#     stop_words = nltk.corpus.stopwords.words('english')\n",
    "#     words = [w for w in words if not w in stop_words]\n",
    "    # return result\n",
    "    processed_comment = \" \".join(words)\n",
    "    return processed_comment\n",
    "\n",
    "def process_dataset(dataset: pd.DataFrame) -> pd.DataFrame:\n",
    "    corpus = pd.concat([dataset['claim'], dataset['evi_text']])\n",
    "    processed_corpus = corpus.apply(lambda text: pre_process(text))\n",
    "    dataset['claim'] = processed_corpus.iloc[0: len(dataset)]\n",
    "    dataset['evi_text'] = processed_corpus.iloc[len(dataset):,]\n",
    "    return dataset\n",
    "\n",
    "train_df = process_dataset(train_df)\n",
    "test_df = process_dataset(test_df)\n",
    "train_df[0: 10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TF Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-05T07:15:07.255868Z",
     "start_time": "2019-05-05T07:14:53.388282Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from scipy.sparse import coo_matrix, hstack\n",
    "\n",
    "max_features = 5000 \n",
    "\n",
    "train_corpus = pd.concat([train_df['claim'], train_df['evi_text']])\n",
    "test_corpus = pd.concat([test_df['claim'], test_df['evi_text']])\n",
    "\n",
    "tf_vectorizer = CountVectorizer(max_features=max_features)\n",
    "tf_vectorizer.fit(train_corpus)\n",
    "train_claim_tf_features = tf_vectorizer.transform(train_df['claim'])\n",
    "train_evi_tf_features = tf_vectorizer.transform(train_df['evi_text'])\n",
    "test_claim_tf_features = tf_vectorizer.transform(test_df['claim'])\n",
    "test_evi_tf_features = tf_vectorizer.transform(test_df['claim'])\n",
    "\n",
    "train_tf_features = hstack([train_claim_tf_features, train_evi_tf_features])\n",
    "test_tf_features = hstack([test_claim_tf_features, test_evi_tf_features])\n",
    "# claim_tf_vectorizer = CountVectorizer(max_features=max_features)\n",
    "# claim_tf = claim_tf_vectorizer.fit_transform(train_df['claim'])\n",
    "# evi_text_tf_vectorizer = CountVectorizer(max_features=max_features)\n",
    "# evi_text_tf = evi_text_tf_vectorizer.fit_transform(train_df['evi_text'])\n",
    "# tf_features = hstack([claim_tf, evi_text_tf])\n",
    "\n",
    "# tf_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TF_IDF Cosine similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-05T07:28:58.049971Z",
     "start_time": "2019-05-05T07:27:55.869318Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "max_features = 5000\n",
    "\n",
    "all_corpus = pd.concat([train_corpus, test_corpus])\n",
    "\n",
    "def calculate_cosines(claim_tfidf, evi_tfidf) -> np.ndarray:\n",
    "    cosines = np.zeros((claim_tfidf.shape[0], 1))\n",
    "    for i in range(len(cosines)):\n",
    "        claim_vector = claim_tfidf[i]\n",
    "        evi_vector = evi_tfidf[i]\n",
    "        cosine_matrix = cosine_similarity([claim_vector.toarray()[0], evi_vector.toarray()[0]])\n",
    "        cosines[i][0] = cosine_matrix[0][1]\n",
    "    return cosines\n",
    "\n",
    "tfidf_vectorizer = TfidfVectorizer(max_features=max_features, norm='l2')\n",
    "tfidf_vectorizer.fit(all_corpus)\n",
    "\n",
    "train_claim_tfidf = tfidf_vectorizer.transform(train_df['claim'])\n",
    "train_evi_tfidf = tfidf_vectorizer.transform(train_df['evi_text'])\n",
    "train_cosines = calculate_cosines(train_claim_tfidf, train_evi_tfidf)\n",
    "\n",
    "test_claim_tfidf = tfidf_vectorizer.transform(test_df['claim'])\n",
    "test_evi_tfidf = tfidf_vectorizer.transform(test_df['evi_text'])\n",
    "test_cosines = calculate_cosines(test_claim_tfidf, test_evi_tfidf)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-04T09:25:00.223600Z",
     "start_time": "2019-05-04T09:25:00.220788Z"
    }
   },
   "source": [
    "#### Concat features together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-05T07:30:46.422319Z",
     "start_time": "2019-05-05T07:30:42.704304Z"
    }
   },
   "outputs": [],
   "source": [
    "x_train = hstack([train_tf_features, train_cosines]).toarray()\n",
    "y_train = train_df[train_df.columns[0:3]].values\n",
    "x_test = hstack([test_tf_features, test_cosines]).toarray()\n",
    "\n",
    "print(x_train.shape)\n",
    "print(y_train.shape)\n",
    "print(x_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build and Train model\n",
    "Build an MLP with tensor (10001, 1) as input, 1 hidden layer with 100 neurons, and softmax layer for output. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-05T07:34:27.322937Z",
     "start_time": "2019-05-05T07:34:27.226013Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_3 (Dense)              (None, 64)                640128    \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 3)                 195       \n",
      "=================================================================\n",
      "Total params: 640,323\n",
      "Trainable params: 640,323\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# from keras.layers import Dense, Input, LSTM, Embedding, Dropout, Activation\n",
    "# from keras.layers import Bidirectional, GlobalMaxPool1D\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from IPython.display import SVG\n",
    "from keras.utils.vis_utils import model_to_dot\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(units=64, activation='relu', input_dim=x_train.shape[1]))\n",
    "model.add(Dense(units=3, activation='softmax'))\n",
    "model.compile(loss=keras.losses.categorical_crossentropy,\n",
    "              optimizer=keras.optimizers.SGD(lr=0.01, momentum=0.9, nesterov=True))\n",
    "\n",
    "model.summary()\n",
    "# SVG(model_to_dot(model).create(prog='dot', format='svg'))\n",
    "model.fit(x=x_train, y=y_train, batch_size=128, epochs=1, validation_split=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-29T10:39:45.256150Z",
     "start_time": "2019-04-29T10:39:45.252494Z"
    }
   },
   "source": [
    "## Apply model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Apply model on test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-05T07:37:34.667159Z",
     "start_time": "2019-05-05T07:37:34.359960Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5001/5001 [==============================] - 0s 60us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([0.1872538 , 0.26825348, 0.5444928 ], dtype=float32)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test = model.predict(x_test, batch_size=128, verbose=1)\n",
    "y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-05T07:38:04.076572Z",
     "start_time": "2019-05-05T07:38:04.073683Z"
    }
   },
   "source": [
    "### Output result to file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-05T07:59:52.879795Z",
     "start_time": "2019-05-05T07:59:52.486447Z"
    }
   },
   "outputs": [],
   "source": [
    "result_dict = {}\n",
    "\n",
    "for i in range(len(test_df)):\n",
    "    if np.argmax(y_test[i]) == 0:\n",
    "        label = \"NOT ENOUGH INFO\"\n",
    "    elif np.argmax(y_test[i]) == 1:\n",
    "        label = \"REFUTES\"\n",
    "    else:\n",
    "        label = \"SUPPORTS\"\n",
    "    key = test_df['key'][i]\n",
    "    result_dict.update({\n",
    "        key:{\n",
    "            \"claim\": test_df['claim'][i],\n",
    "            \"label\": label,\n",
    "            \"evidence\": test_df['evidence'][i]\n",
    "        }\n",
    "    })\n",
    "    \n",
    "with open('result_on_dev.json', 'w') as outfile:\n",
    "    json.dump(result_dict, outfile, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
