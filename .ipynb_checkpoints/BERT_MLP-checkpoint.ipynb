{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deploy BERT server"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instruction website: https://bert-as-service.readthedocs.io/en/latest/section/get-start.html  \n",
    "Download server and client:\n",
    "``` bash\n",
    "pip install -U bert-serving-server bert-serving-client  \n",
    "```\n",
    "Downlaod and unzip pretrained bert model(BERT-Large, Uncased, 1024 dimensional output):  \n",
    "``` bash\n",
    "cd ${model_path}\n",
    "wget https://storage.googleapis.com/bert_models/2018_10_18/uncased_L-24_H-1024_A-16.zip\n",
    "unzip uncased_L-24_H-1024_A-16.zip  \n",
    "```  \n",
    "\n",
    "Start bert server at local machine: \n",
    "``` bash\n",
    "bert-serving-start -model_dir ${model_path}/uncased_L-24_H-1024_A-16 -max_seq_len=100 -num_worker=1  \n",
    "bert-serving-start -model_dir /share/ShareFolder/uncased_L-24_H-1024_A-16/ -max_seq_len=150 -gpu_memory_fraction=0.9 -num_worker=1\n",
    "```\n",
    "Then, call from client end in python:\n",
    "``` python\n",
    "from bert_serving.client import BertClient\n",
    "bc = BertClient()\n",
    "bc.encode(['First do it', 'then do it right', 'then do it better'])\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-17T05:43:47.817994Z",
     "start_time": "2019-05-17T05:43:47.813975Z"
    }
   },
   "source": [
    "## Load data as Pandas dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-17T08:40:08.621365Z",
     "start_time": "2019-05-17T08:40:05.695425Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "train_file_path = \"./JSONFiles/\" + \"train_with_text.json\"\n",
    "use_test_file = False\n",
    "if use_test_file:\n",
    "    test_file_path = './JSONFiles/' + 'test_with_text.json'\n",
    "else:\n",
    "    test_file_path = './JSONFiles/' + 'dev_with_text.json'\n",
    "\n",
    "with open(train_file_path, mode='r') as f:\n",
    "    train = json.load(f)\n",
    "with open(test_file_path, mode='r') as f:\n",
    "    test = json.load(f)\n",
    "\n",
    "def load_training_data(dataset: dict) -> list:\n",
    "    dataset_list = []\n",
    "    for key in dataset.keys():\n",
    "        record = dataset.get(key)\n",
    "        claim = record.get(\"claim\")\n",
    "        evi_texts = record.get(\"evidence_texts\")\n",
    "        text = ''.join(evi_texts)\n",
    "        if len(text) == 0:\n",
    "            text = \"no word\"\n",
    "\n",
    "        SUP = NOINFO = REF = 0\n",
    "        if record.get(\"label\") == \"SUPPORTS\":\n",
    "            SUP = 1\n",
    "        elif record.get(\"label\") == \"REFUTES\":\n",
    "            REF = 1\n",
    "        else:\n",
    "            NOINFO = 1\n",
    "        dataset_record = {\n",
    "            \"claim\": claim,\n",
    "            \"evi_text\": text,\n",
    "            \"claim_with_evi_text\": claim + \" ||| \" + text,\n",
    "            \"SUP\": SUP,\n",
    "            \"NOINFO\": NOINFO,\n",
    "            \"REF\": REF\n",
    "        }\n",
    "        dataset_list.append(dataset_record)\n",
    "    return dataset_list\n",
    "\n",
    "def load_test_data(dataset: dict) -> list:\n",
    "    dataset_list = []\n",
    "    for key in dataset.keys():\n",
    "        record = dataset.get(key)\n",
    "        claim = record.get(\"claim\")\n",
    "        evi_index = record.get(\"evidence\")\n",
    "        evi_texts = record.get(\"evidence_texts\")\n",
    "        text = ''.join(evi_texts)\n",
    "        if len(text) == 0:\n",
    "            text = \"no word\"\n",
    "            \n",
    "        dataset_record = {\n",
    "            \"key\": key,\n",
    "            \"claim\": claim,\n",
    "            \"evidence\": evi_index,\n",
    "            \"claim_with_evi_text\": claim + \" ||| \" + text,\n",
    "            \"evi_text\": text\n",
    "        }\n",
    "        dataset_list.append(dataset_record)\n",
    "    return dataset_list\n",
    "\n",
    "train_df = pd.DataFrame(load_training_data(train))\n",
    "test_df = pd.DataFrame(load_test_data(test))\n",
    "\n",
    "train_df[0: 10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-17T08:40:08.921591Z",
     "start_time": "2019-05-17T08:40:08.907849Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "test_df[0: 10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-17T10:22:02.299532Z",
     "start_time": "2019-05-17T10:22:02.296894Z"
    }
   },
   "source": [
    "### Construct and save bert features to file for reuse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-17T08:40:43.138821Z",
     "start_time": "2019-05-17T08:40:43.026315Z"
    }
   },
   "outputs": [],
   "source": [
    "from bert_serving.client import BertClient\n",
    "bc = BertClient()\n",
    "\n",
    "# result = bert_embedding(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-17T10:22:01.605677Z",
     "start_time": "2019-05-17T08:53:06.342210Z"
    }
   },
   "outputs": [],
   "source": [
    "# train, test claim encode\n",
    "train_claim_encode = bc.encode(list(train_df['claim']))\n",
    "np.save(\"./BERT_MLP_encodings/train_claim_encode\", train_claim_encode)\n",
    "\n",
    "test_claim_encode = bc.encode(list(test_df['claim']))\n",
    "np.save(\"./BERT_MLP_encodings/test_claim_encode\", test_claim_encode)\n",
    "\n",
    "# train, test evidence encode\n",
    "train_evi_encode = bc.encode(list(train_df['evi_text']))\n",
    "np.save(\"./BERT_MLP_encodings/train_evi_encode\", train_evi_encode)\n",
    "\n",
    "test_evi_encode = bc.encode(list(test_df['evi_text']))\n",
    "np.save(\"./BERT_MLP_encodings/test_evi_endcode\", test_evi_encode)\n",
    "\n",
    "# train, test claim+evidence pair encode\n",
    "train_pair_encode = bc.encode(list(train_df['claim_with_evi_text']))\n",
    "np.save(\"./BERT_MLP_encodings/train_pair_encode\", train_pair_encode)\n",
    "\n",
    "test_pair_encode = bc.encode(list(test_df['claim_with_evi_text']))\n",
    "np.save(\"./BERT_MLP_encodings/test_pair_encode\", test_pair_encode)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load bert features from file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from scipy.sparse import coo_matrix, hstack\n",
    "\n",
    "# train_claim_features = np.load(\"train_claim_encode.npy\")\n",
    "# train_evi_features = np.load(\"train_evi_encode.npy\")\n",
    "# test_claim_features = np.load(\"test_claim_encode.npy\")\n",
    "# test_evi_features = np.load(\"test_evi_encode.npy\")\n",
    "\n",
    "# x_train = hstack([train_claim_features, train_evi_features])\n",
    "# y_train = train_df[train_df.columns[0:3]].values\n",
    "# test_features = hstack([test_claim_features, test_evi_features])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
