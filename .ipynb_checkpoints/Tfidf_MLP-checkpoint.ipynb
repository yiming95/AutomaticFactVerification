{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict Label with extracted evidence texts\n",
    "This notebook builds the MLP model for RTM step according to the FNC competition paper.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data preprocess"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load data as pandas DF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-16T07:08:30.899968Z",
     "start_time": "2019-05-16T07:08:28.562306Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>NOINFO</th>\n",
       "      <th>REF</th>\n",
       "      <th>SUP</th>\n",
       "      <th>claim</th>\n",
       "      <th>evi_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>Ireland does not have relatively low-lying mou...</td>\n",
       "      <td>Ireland 10 The island 's geography comprises r...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>The drama Dark Matter stars Taylor Schilling.</td>\n",
       "      <td>Taylor_Schilling 2 She made her film debut in ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>In 1932, Prussia was taken over.</td>\n",
       "      <td>Prussia 30 In the Weimar Republic , the state ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>IZombie premiered in 2015.</td>\n",
       "      <td>IZombie_-LRB-TV_series-RRB- 2 The series premi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>Ronald Reagan had a nationality.</td>\n",
       "      <td>Ronald_Reagan 0 Ronald Wilson Reagan -LRB- -LS...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>Samoa Joe wrestles professionally.</td>\n",
       "      <td>Samoa_Joe 0 Nuufolau Joel `` Joe '' Seanoa -LR...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>University of Oxford is in the universe.</td>\n",
       "      <td>University_of_Oxford 0 The University of Oxfor...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>The Renaissance began online.</td>\n",
       "      <td>Starwood_-LRB-nightclub-RRB- 1 Many punk bands...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>Portia de Rossi appeared on Scandal.</td>\n",
       "      <td>Portia_de_Rossi 1 She appeared as a regular ca...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>The Berlin Wall was only standing for 10 years.</td>\n",
       "      <td>Berlin_Wall 0 The Berlin Wall -LRB- Berliner M...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   NOINFO  REF  SUP                                              claim  \\\n",
       "0       0    1    0  Ireland does not have relatively low-lying mou...   \n",
       "1       0    0    1      The drama Dark Matter stars Taylor Schilling.   \n",
       "2       0    0    1                   In 1932, Prussia was taken over.   \n",
       "3       0    0    1                         IZombie premiered in 2015.   \n",
       "4       0    0    1                   Ronald Reagan had a nationality.   \n",
       "5       0    0    1                 Samoa Joe wrestles professionally.   \n",
       "6       0    0    1           University of Oxford is in the universe.   \n",
       "7       1    0    0                      The Renaissance began online.   \n",
       "8       0    0    1               Portia de Rossi appeared on Scandal.   \n",
       "9       0    1    0    The Berlin Wall was only standing for 10 years.   \n",
       "\n",
       "                                            evi_text  \n",
       "0  Ireland 10 The island 's geography comprises r...  \n",
       "1  Taylor_Schilling 2 She made her film debut in ...  \n",
       "2  Prussia 30 In the Weimar Republic , the state ...  \n",
       "3  IZombie_-LRB-TV_series-RRB- 2 The series premi...  \n",
       "4  Ronald_Reagan 0 Ronald Wilson Reagan -LRB- -LS...  \n",
       "5  Samoa_Joe 0 Nuufolau Joel `` Joe '' Seanoa -LR...  \n",
       "6  University_of_Oxford 0 The University of Oxfor...  \n",
       "7  Starwood_-LRB-nightclub-RRB- 1 Many punk bands...  \n",
       "8  Portia_de_Rossi 1 She appeared as a regular ca...  \n",
       "9  Berlin_Wall 0 The Berlin Wall -LRB- Berliner M...  "
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "train_file_path = \"./JSONFiles/\" + \"train_with_text.json\"\n",
    "use_test_file = False\n",
    "if use_test_file:\n",
    "    test_file_path = './JSONFiles/' + 'test_with_text.json'\n",
    "else:\n",
    "    test_file_path = './JSONFiles/' + 'dev_with_text.json'\n",
    "\n",
    "with open(train_file_path, mode='r') as f:\n",
    "    train = json.load(f)\n",
    "with open(test_file_path, mode='r') as f:\n",
    "    test = json.load(f)\n",
    "\n",
    "def load_training_data(dataset: dict) -> list:\n",
    "    dataset_list = []\n",
    "    for key in dataset.keys():\n",
    "        record = dataset.get(key)\n",
    "        claim = record.get(\"claim\")\n",
    "        evi_texts = record.get(\"evidence_texts\")\n",
    "        text = \"\"\n",
    "        for evi in evi_texts:\n",
    "            text += evi\n",
    "        SUP = NOINFO = REF = 0\n",
    "        if record.get(\"label\") == \"SUPPORTS\":\n",
    "            SUP = 1\n",
    "        elif record.get(\"label\") == \"REFUTES\":\n",
    "            REF = 1\n",
    "        else:\n",
    "            NOINFO = 1\n",
    "        dataset_record = {\n",
    "            \"claim\": claim,\n",
    "            \"evi_text\": text,\n",
    "            \"SUP\": SUP,\n",
    "            \"NOINFO\": NOINFO,\n",
    "            \"REF\": REF\n",
    "        }\n",
    "        dataset_list.append(dataset_record)\n",
    "    return dataset_list\n",
    "\n",
    "def load_test_data(dataset: dict) -> list:\n",
    "    dataset_list = []\n",
    "    for key in dataset.keys():\n",
    "        record = dataset.get(key)\n",
    "        claim = record.get(\"claim\")\n",
    "        evi_index = record.get(\"evidence\")\n",
    "        evi_texts = record.get(\"evidence_texts\")\n",
    "        text = \"\"\n",
    "        for evi in evi_texts:\n",
    "            text += evi\n",
    "\n",
    "        dataset_record = {\n",
    "            \"key\": key,\n",
    "            \"claim\": claim,\n",
    "            \"evidence\": evi_index,\n",
    "            \"evi_text\": text\n",
    "        }\n",
    "        dataset_list.append(dataset_record)\n",
    "    return dataset_list\n",
    "\n",
    "train_df = pd.DataFrame(load_training_data(train))\n",
    "test_df = pd.DataFrame(load_test_data(test))\n",
    "\n",
    "train_df[0: 10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-16T07:08:30.918477Z",
     "start_time": "2019-05-16T07:08:30.901968Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>claim</th>\n",
       "      <th>evi_text</th>\n",
       "      <th>evidence</th>\n",
       "      <th>key</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Ripon College's student number totaled in at a...</td>\n",
       "      <td>Ripon_College_-LRB-Wisconsin-RRB- 1 As of 2015...</td>\n",
       "      <td>[[Ripon_College_-LRB-Wisconsin-RRB-, 1]]</td>\n",
       "      <td>100038</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Kesha was baptized on March 1st, 1987.</td>\n",
       "      <td>Kesha 0 Kesha Rose Sebert -LRB- -LSB- ˈkɛʃə_ro...</td>\n",
       "      <td>[[Kesha, 0]]</td>\n",
       "      <td>100083</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Birthday Song (2 Chainz song) was banned by So...</td>\n",
       "      <td>Birthday_Song_-LRB-2_Chainz_song-RRB- 1 The so...</td>\n",
       "      <td>[[Birthday_Song_-LRB-2_Chainz_song-RRB-, 1]]</td>\n",
       "      <td>100169</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>The University of Illinois at Chicago is a col...</td>\n",
       "      <td>University_of_Illinois_at_Chicago 0 The Univer...</td>\n",
       "      <td>[[University_of_Illinois_at_Chicago, 0]]</td>\n",
       "      <td>100234</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>French Indochina was officially known as the I...</td>\n",
       "      <td>Harukawa 5 , Japanese actress\\n</td>\n",
       "      <td>[[Harukawa, 5]]</td>\n",
       "      <td>100359</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Damon Albarn has refused to ever work with Bri...</td>\n",
       "      <td>Damon_Albarn 17 His debut solo studio album Ev...</td>\n",
       "      <td>[[Damon_Albarn, 17]]</td>\n",
       "      <td>100366</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Lost (TV series) is a series of plays.</td>\n",
       "      <td>Lost_-LRB-TV_series-RRB- 0 Lost is an American...</td>\n",
       "      <td>[[Lost_-LRB-TV_series-RRB-, 0]]</td>\n",
       "      <td>100429</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Edison Machine Works was barely set up to prod...</td>\n",
       "      <td>List_of_professional_Magic-COLON-_The_Gatherin...</td>\n",
       "      <td>[[List_of_professional_Magic-COLON-_The_Gather...</td>\n",
       "      <td>100457</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>The human brain is set apart from mammalian br...</td>\n",
       "      <td>Barn_River 1 Only 3 km long , it acts as the p...</td>\n",
       "      <td>[[Barn_River, 1]]</td>\n",
       "      <td>100461</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>There are rumors that Augustus' wife, Livia, p...</td>\n",
       "      <td>The_Malpractice_-LRB-band-RRB- 1 His debutalbu...</td>\n",
       "      <td>[[The_Malpractice_-LRB-band-RRB-, 1]]</td>\n",
       "      <td>100481</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               claim  \\\n",
       "0  Ripon College's student number totaled in at a...   \n",
       "1             Kesha was baptized on March 1st, 1987.   \n",
       "2  Birthday Song (2 Chainz song) was banned by So...   \n",
       "3  The University of Illinois at Chicago is a col...   \n",
       "4  French Indochina was officially known as the I...   \n",
       "5  Damon Albarn has refused to ever work with Bri...   \n",
       "6             Lost (TV series) is a series of plays.   \n",
       "7  Edison Machine Works was barely set up to prod...   \n",
       "8  The human brain is set apart from mammalian br...   \n",
       "9  There are rumors that Augustus' wife, Livia, p...   \n",
       "\n",
       "                                            evi_text  \\\n",
       "0  Ripon_College_-LRB-Wisconsin-RRB- 1 As of 2015...   \n",
       "1  Kesha 0 Kesha Rose Sebert -LRB- -LSB- ˈkɛʃə_ro...   \n",
       "2  Birthday_Song_-LRB-2_Chainz_song-RRB- 1 The so...   \n",
       "3  University_of_Illinois_at_Chicago 0 The Univer...   \n",
       "4                    Harukawa 5 , Japanese actress\\n   \n",
       "5  Damon_Albarn 17 His debut solo studio album Ev...   \n",
       "6  Lost_-LRB-TV_series-RRB- 0 Lost is an American...   \n",
       "7  List_of_professional_Magic-COLON-_The_Gatherin...   \n",
       "8  Barn_River 1 Only 3 km long , it acts as the p...   \n",
       "9  The_Malpractice_-LRB-band-RRB- 1 His debutalbu...   \n",
       "\n",
       "                                            evidence     key  \n",
       "0           [[Ripon_College_-LRB-Wisconsin-RRB-, 1]]  100038  \n",
       "1                                       [[Kesha, 0]]  100083  \n",
       "2       [[Birthday_Song_-LRB-2_Chainz_song-RRB-, 1]]  100169  \n",
       "3           [[University_of_Illinois_at_Chicago, 0]]  100234  \n",
       "4                                    [[Harukawa, 5]]  100359  \n",
       "5                               [[Damon_Albarn, 17]]  100366  \n",
       "6                    [[Lost_-LRB-TV_series-RRB-, 0]]  100429  \n",
       "7  [[List_of_professional_Magic-COLON-_The_Gather...  100457  \n",
       "8                                  [[Barn_River, 1]]  100461  \n",
       "9              [[The_Malpractice_-LRB-band-RRB-, 1]]  100481  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df[0: 10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tokenization and Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-16T07:09:43.997206Z",
     "start_time": "2019-05-16T07:08:30.920412Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/wenbin/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>NOINFO</th>\n",
       "      <th>REF</th>\n",
       "      <th>SUP</th>\n",
       "      <th>claim</th>\n",
       "      <th>evi_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>ireland do not have relatively low lie mountain</td>\n",
       "      <td>ireland 10 the island s geography comprise rel...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>the drama dark matter star taylor schilling</td>\n",
       "      <td>taylor_schilling 2 she make her film debut in ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>in 1932 prussia be take over</td>\n",
       "      <td>prussia 30 in the weimar republic the state of...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>izombie premier in 2015</td>\n",
       "      <td>izombie_ lrb tv_series rrb 2 the series premie...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>ronald reagan have a nationality</td>\n",
       "      <td>ronald_reagan 0 ronald wilson reagan lrb lsb ˈ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>samoa joe wrestle professionally</td>\n",
       "      <td>samoa_joe 0 nuufolau joel joe seanoa lrb bear ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>university of oxford be in the universe</td>\n",
       "      <td>university_of_oxford 0 the university of oxfor...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>the renaissance begin online</td>\n",
       "      <td>starwood_ lrb nightclub rrb 1 many punk band a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>portia de rossi appear on scandal</td>\n",
       "      <td>portia_de_rossi 1 she appear a a regular cast ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>the berlin wall be only stand for 10 year</td>\n",
       "      <td>berlin_wall 0 the berlin wall lrb berliner mau...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   NOINFO  REF  SUP                                            claim  \\\n",
       "0       0    1    0  ireland do not have relatively low lie mountain   \n",
       "1       0    0    1      the drama dark matter star taylor schilling   \n",
       "2       0    0    1                     in 1932 prussia be take over   \n",
       "3       0    0    1                          izombie premier in 2015   \n",
       "4       0    0    1                 ronald reagan have a nationality   \n",
       "5       0    0    1                 samoa joe wrestle professionally   \n",
       "6       0    0    1          university of oxford be in the universe   \n",
       "7       1    0    0                     the renaissance begin online   \n",
       "8       0    0    1                portia de rossi appear on scandal   \n",
       "9       0    1    0        the berlin wall be only stand for 10 year   \n",
       "\n",
       "                                            evi_text  \n",
       "0  ireland 10 the island s geography comprise rel...  \n",
       "1  taylor_schilling 2 she make her film debut in ...  \n",
       "2  prussia 30 in the weimar republic the state of...  \n",
       "3  izombie_ lrb tv_series rrb 2 the series premie...  \n",
       "4  ronald_reagan 0 ronald wilson reagan lrb lsb ˈ...  \n",
       "5  samoa_joe 0 nuufolau joel joe seanoa lrb bear ...  \n",
       "6  university_of_oxford 0 the university of oxfor...  \n",
       "7  starwood_ lrb nightclub rrb 1 many punk band a...  \n",
       "8  portia_de_rossi 1 she appear a a regular cast ...  \n",
       "9  berlin_wall 0 the berlin wall lrb berliner mau...  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "\n",
    "tokenizer = nltk.tokenize.RegexpTokenizer(r'\\w+')\n",
    "lemmatizer = nltk.stem.WordNetLemmatizer()\n",
    "\n",
    "def lemmatize(word):\n",
    "    lemma = lemmatizer.lemmatize(word,'v')\n",
    "    if lemma == word:\n",
    "        lemma = lemmatizer.lemmatize(word,'n')\n",
    "    return lemma\n",
    "\n",
    "def pre_process(comment) -> str:\n",
    "    # lower cased\n",
    "    comment = comment.lower()\n",
    "    # tokenize\n",
    "    words = tokenizer.tokenize(comment)\n",
    "    # lemmatize \n",
    "    words = [lemmatize(w) for w in words]\n",
    "    # remove stop words\n",
    "#     stop_words = nltk.corpus.stopwords.words('english')\n",
    "#     words = [w for w in words if not w in stop_words]\n",
    "    # return result\n",
    "    processed_comment = \" \".join(words)\n",
    "    return processed_comment\n",
    "\n",
    "def process_dataset(dataset: pd.DataFrame) -> pd.DataFrame:\n",
    "    corpus = pd.concat([dataset['claim'], dataset['evi_text']])\n",
    "    processed_corpus = corpus.apply(lambda text: pre_process(text))\n",
    "    dataset['claim'] = processed_corpus.iloc[0: len(dataset)]\n",
    "    dataset['evi_text'] = processed_corpus.iloc[len(dataset):,]\n",
    "    return dataset\n",
    "\n",
    "train_df = process_dataset(train_df)\n",
    "test_df = process_dataset(test_df)\n",
    "train_df[0: 10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TF Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-16T07:09:59.882839Z",
     "start_time": "2019-05-16T07:09:43.999200Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from scipy.sparse import coo_matrix, hstack\n",
    "\n",
    "max_features = 5000 \n",
    "\n",
    "train_corpus = pd.concat([train_df['claim'], train_df['evi_text']])\n",
    "test_corpus = pd.concat([test_df['claim'], test_df['evi_text']])\n",
    "\n",
    "tf_vectorizer = CountVectorizer(max_features=max_features)\n",
    "tf_vectorizer.fit(train_corpus)\n",
    "train_claim_tf_features = tf_vectorizer.transform(train_df['claim'])\n",
    "train_evi_tf_features = tf_vectorizer.transform(train_df['evi_text'])\n",
    "test_claim_tf_features = tf_vectorizer.transform(test_df['claim'])\n",
    "test_evi_tf_features = tf_vectorizer.transform(test_df['claim'])\n",
    "\n",
    "train_tf_features = hstack([train_claim_tf_features, train_evi_tf_features])\n",
    "test_tf_features = hstack([test_claim_tf_features, test_evi_tf_features])\n",
    "# claim_tf_vectorizer = CountVectorizer(max_features=max_features)\n",
    "# claim_tf = claim_tf_vectorizer.fit_transform(train_df['claim'])\n",
    "# evi_text_tf_vectorizer = CountVectorizer(max_features=max_features)\n",
    "# evi_text_tf = evi_text_tf_vectorizer.fit_transform(train_df['evi_text'])\n",
    "# tf_features = hstack([claim_tf, evi_text_tf])\n",
    "\n",
    "# tf_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TF_IDF Cosine similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-16T07:11:07.151448Z",
     "start_time": "2019-05-16T07:09:59.884526Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "max_features = 5000\n",
    "\n",
    "all_corpus = pd.concat([train_corpus, test_corpus])\n",
    "\n",
    "def calculate_cosines(claim_tfidf, evi_tfidf) -> np.ndarray:\n",
    "    cosines = np.zeros((claim_tfidf.shape[0], 1))\n",
    "    for i in range(len(cosines)):\n",
    "        claim_vector = claim_tfidf[i]\n",
    "        evi_vector = evi_tfidf[i]\n",
    "        cosine_matrix = cosine_similarity([claim_vector.toarray()[0], evi_vector.toarray()[0]])\n",
    "        cosines[i][0] = cosine_matrix[0][1]\n",
    "    return cosines\n",
    "\n",
    "tfidf_vectorizer = TfidfVectorizer(max_features=max_features, norm='l2')\n",
    "tfidf_vectorizer.fit(all_corpus)\n",
    "\n",
    "train_claim_tfidf = tfidf_vectorizer.transform(train_df['claim'])\n",
    "train_evi_tfidf = tfidf_vectorizer.transform(train_df['evi_text'])\n",
    "train_cosines = calculate_cosines(train_claim_tfidf, train_evi_tfidf)\n",
    "\n",
    "test_claim_tfidf = tfidf_vectorizer.transform(test_df['claim'])\n",
    "test_evi_tfidf = tfidf_vectorizer.transform(test_df['evi_text'])\n",
    "test_cosines = calculate_cosines(test_claim_tfidf, test_evi_tfidf)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-04T09:25:00.223600Z",
     "start_time": "2019-05-04T09:25:00.220788Z"
    }
   },
   "source": [
    "#### Concat features together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-16T07:11:11.574460Z",
     "start_time": "2019-05-16T07:11:07.153473Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(145449, 10001)\n",
      "(145449, 3)\n",
      "(5001, 10001)\n"
     ]
    }
   ],
   "source": [
    "x_train = hstack([train_tf_features, train_cosines]).toarray()\n",
    "y_train = train_df[train_df.columns[0:3]].values\n",
    "x_test = hstack([test_tf_features, test_cosines]).toarray()\n",
    "\n",
    "print(x_train.shape)\n",
    "print(y_train.shape)\n",
    "print(x_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build and Train model\n",
    "Build an MLP with tensor (10001, 1) as input, 1 hidden layer with 100 neurons, and softmax layer for output. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Simple MLP model prototype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-16T07:11:11.584318Z",
     "start_time": "2019-05-16T07:11:11.576658Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# # from keras.layers import Dense, Input, LSTM, Embedding, Dropout, Activation\n",
    "# # from keras.layers import Bidirectional, GlobalMaxPool1D\n",
    "# import keras\n",
    "# from keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "# from keras.models import Sequential\n",
    "# from keras.layers import Dense, Dropout\n",
    "# from IPython.display import SVG\n",
    "# from keras.utils.vis_utils import model_to_dot\n",
    "\n",
    "# model = Sequential()\n",
    "# model.add(Dense(units=100, activation='relu', input_dim=x_train.shape[1]))\n",
    "# model.add(Dropout(0.3))\n",
    "# model.add(Dense(units=3, activation='softmax'))\n",
    "# model.compile(loss=keras.losses.categorical_crossentropy,\n",
    "#               optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# model.summary()\n",
    "# # SVG(model_to_dot(model).create(prog='dot', format='svg'))\n",
    "\n",
    "# # callbacks\n",
    "# filepath=\"best_weights.hdf5\"\n",
    "# checkpoint = ModelCheckpoint(filepath, monitor='val_acc', verbose=1, save_best_only=True, mode='max')\n",
    "# earlyStopping = EarlyStopping(monitor='val_acc', patience=1, verbose=0, mode='min')\n",
    "\n",
    "# callbacks_list = [checkpoint, earlyStopping]\n",
    "\n",
    "# model.fit(x=x_train, y=y_train, batch_size=128, epochs=10, validation_split=0.1, callbacks=callbacks_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tune hyper-parameters mannually"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-16T07:11:11.602390Z",
     "start_time": "2019-05-16T07:11:11.586167Z"
    }
   },
   "outputs": [],
   "source": [
    "import itertools\n",
    "\n",
    "class Cartesian(object):\n",
    "    def __init__(self):\n",
    "        self._data_list = []\n",
    "        self._name_list = []\n",
    "        self.cartesian_result = []\n",
    "\n",
    "    def add_data(self, data, name): #add list for cartesian product\n",
    "        self._data_list.append(data)\n",
    "        self._name_list.append(name)\n",
    "\n",
    "    def build(self): #calculate cartesian product\n",
    "        for item in itertools.product(*self._data_list):\n",
    "            result_dict = {}\n",
    "            for i in range(len(item)):\n",
    "                result_dict.update({\n",
    "                    self._name_list[i]: item[i]\n",
    "                })\n",
    "            self.cartesian_result.append(result_dict)\n",
    "        return self.cartesian_result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-16T07:49:01.043910Z",
     "start_time": "2019-05-16T07:48:36.633059Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "itertion: 0\n",
      "{'units': 25, 'dropout': 0.3, 'batch_size': 16, 'lr': 1e-05}\n",
      "Train on 130904 samples, validate on 14545 samples\n",
      "Epoch 1/50\n",
      " 93760/130904 [====================>.........] - ETA: 9s - loss: 0.4933 - acc: 0.8004"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-46-fb73e25642b2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     53\u001b[0m                          \u001b[0mdropout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcombination\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'dropout'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m                          lr=combination['lr'])\n\u001b[0;32m---> 55\u001b[0;31m     \u001b[0mmodel_history\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfit_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcombination\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'batch_size'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     56\u001b[0m     historys_list.append({\n\u001b[1;32m     57\u001b[0m         \u001b[0;34m'combination'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mcombination\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-46-fb73e25642b2>\u001b[0m in \u001b[0;36mfit_model\u001b[0;34m(model, batch_size)\u001b[0m\n\u001b[1;32m     30\u001b[0m     model_history = model.fit(x=x_train, y=y_train, \n\u001b[1;32m     31\u001b[0m                               \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m                               validation_split=0.1, callbacks=callbacks_list, verbose=1)\n\u001b[0m\u001b[1;32m     33\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mmodel_history\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1037\u001b[0m                                         \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1038\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1039\u001b[0;31m                                         validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m   1040\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1041\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[0;34m(model, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[1;32m    183\u001b[0m                         \u001b[0;31m# Do not slice the training phase flag.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    184\u001b[0m                         ins_batch = slice_arrays(\n\u001b[0;32m--> 185\u001b[0;31m                             ins[:-1], batch_ids) + [ins[-1]]\n\u001b[0m\u001b[1;32m    186\u001b[0m                     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m                         \u001b[0mins_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mslice_arrays\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/keras/utils/generic_utils.py\u001b[0m in \u001b[0;36mslice_arrays\u001b[0;34m(arrays, start, stop)\u001b[0m\n\u001b[1;32m    524\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'shape'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    525\u001b[0m                 \u001b[0mstart\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstart\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 526\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;32mNone\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0marrays\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    527\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    528\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;32mNone\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mstop\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0marrays\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/keras/utils/generic_utils.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    524\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'shape'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    525\u001b[0m                 \u001b[0mstart\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstart\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 526\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;32mNone\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0marrays\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    527\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    528\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;32mNone\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mstop\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0marrays\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# from keras.layers import Dense, Input, LSTM, Embedding, Dropout, Activation\n",
    "# from keras.layers import Bidirectional, GlobalMaxPool1D\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout\n",
    "from keras.utils.vis_utils import model_to_dot\n",
    "from keras.optimizers import Adam\n",
    "import keras\n",
    "\n",
    "# fix random seed for reproducibility\n",
    "seed = 7\n",
    "np.random.seed(seed)\n",
    "\n",
    "def create_model(units = 100, dropout = 0.5, lr = 0.001):\n",
    "    model = Sequential()\n",
    "    model.add(Dense(units=units, activation='relu', input_dim=x_train.shape[1]))\n",
    "    model.add(Dropout(dropout))\n",
    "    model.add(Dense(units=3, activation='softmax'))\n",
    "    optimizer = Adam(lr=lr)\n",
    "    model.compile(loss=keras.losses.categorical_crossentropy,\n",
    "                  optimizer='adam', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "def fit_model(model, batch_size=32):\n",
    "    earlyStopping = EarlyStopping(monitor='val_acc', patience=4, \n",
    "                                  verbose=0, mode='auto')\n",
    "    EarlyStopping()\n",
    "    callbacks_list = [earlyStopping]\n",
    "\n",
    "    model_history = model.fit(x=x_train, y=y_train, \n",
    "                              batch_size=batch_size, epochs=50, \n",
    "                              validation_split=0.1, callbacks=callbacks_list, verbose=1)\n",
    "    return model_history\n",
    "\n",
    "units_list = [25, 50, 100, 250, 500]\n",
    "dropout_list = [0.3, 0.4, 0.5, 0.6, 0.7]\n",
    "batch_size_list = [16, 32, 64, 128, 256, 512]\n",
    "lr_list = [0.0001, 0.001, 0.01, 0.1]\n",
    "    \n",
    "car_product=Cartesian()\n",
    "car_product.add_data(units_list, 'units')\n",
    "car_product.add_data(dropout_list, 'dropout')\n",
    "car_product.add_data(batch_size_list, 'batch_size')\n",
    "car_product.add_data(lr_list, 'lr')\n",
    "parameter_combinations = car_product.build()\n",
    "\n",
    "historys_list = []\n",
    "iternum = 0\n",
    "for combination in parameter_combinations:\n",
    "    print(\"itertion: \" + str(iternum))\n",
    "    print(combination)\n",
    "    model = create_model(units=combination['units'], \n",
    "                         dropout=combination['dropout'], \n",
    "                         lr=combination['lr'])\n",
    "    model_history = fit_model(model=model, batch_size=combination['batch_size'])\n",
    "    historys_list.append({\n",
    "        'combination': combination,\n",
    "        'max_val_acc': max(model_history.history['val_acc'])\n",
    "    })\n",
    "    print(\"result: \" + str(max(model_history.history['val_acc'])))\n",
    "    iternum += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-16T07:45:52.187682Z",
     "start_time": "2019-05-16T07:45:52.180301Z"
    }
   },
   "outputs": [],
   "source": [
    "# sort and output_to_file\n",
    "ordered_history = sorted(historys_list, key= lambda x: x['max_val_acc'], reverse=True)\n",
    "\n",
    "historys_list_dict = {\n",
    "    \"historys\": ordered_history\n",
    "}\n",
    "with open('tune_hps.json', 'w') as hp_result:\n",
    "    json.dump(ordered_history, hp_result, indent=4)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Tune hyper-parameters with sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-16T05:07:49.131986Z",
     "start_time": "2019-05-16T05:02:39.189238Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# from sklearn.model_selection import GridSearchCV\n",
    "# from keras.wrappers.scikit_learn import KerasClassifier\n",
    "# from sklearn.model_selection import train_test_split\n",
    "# import keras\n",
    "# from keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "# from keras.models import Sequential\n",
    "# from keras.layers import Dense, Dropout\n",
    "# from IPython.display import SVG\n",
    "# from keras.utils.vis_utils import model_to_dot\n",
    "\n",
    "\n",
    "# # # fix random seed for reproducibility\n",
    "# seed = 7\n",
    "# np.random.seed(seed)\n",
    "\n",
    "# def create_model():\n",
    "#     model = Sequential()\n",
    "#     model.add(Dense(units=100, activation='relu', input_dim=x_train.shape[1]))\n",
    "#     model.add(Dropout(0.3))\n",
    "#     model.add(Dense(units=3, activation='softmax'))\n",
    "#     model.compile(loss=keras.losses.categorical_crossentropy,\n",
    "#                   optimizer='adam', metrics=['accuracy'])\n",
    "#     return model\n",
    "\n",
    "# model = KerasClassifier(build_fn=create_model, verbose=2)\n",
    "# batch_size = [64, 128]\n",
    "# # epochs = [1, 2]\n",
    "# param_grid = dict(batch_size=batch_size)\n",
    "# grid = GridSearchCV(estimator=model, param_grid=param_grid, n_jobs=-1, cv=2)\n",
    "# grid_result = grid.fit(X=x_train, y=y_train)\n",
    "\n",
    "\n",
    "# # summarize results\n",
    "# # print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n",
    "# # means = grid_result.cv_results_['mean_test_score']\n",
    "# # stds = grid_result.cv_results_['std_test_score']\n",
    "# # params = grid_result.cv_results_['params']\n",
    "# # for mean, stdev, param in zip(means, stds, params):\n",
    "# #     print(\"%f (%f) with: %r\" % (mean, stdev, param))\n",
    "\n",
    "# grid_result.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-16T05:09:57.964156Z",
     "start_time": "2019-05-16T05:09:57.958931Z"
    },
    "cell_style": "center",
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def create_model():\n",
    "    model = Sequential()\n",
    "    model.add(Dense(units=100, activation='relu', input_dim=x_train.shape[1]))\n",
    "    model.add(Dropout(0.3))\n",
    "    model.add(Dense(units=3, activation='softmax'))\n",
    "    model.compile(loss=keras.losses.categorical_crossentropy,\n",
    "                  optimizer='adam', metrics=['accuracy'], verbose=2)\n",
    "    return model\n",
    "\n",
    "def fit_model():\n",
    "    # callbacks\n",
    "    filepath=\"best_weights.hdf5\"\n",
    "    checkpoint = ModelCheckpoint(filepath, monitor='val_acc', verbose=1, save_best_only=True, mode='max')\n",
    "    earlyStopping = EarlyStopping(monitor='val_acc', patience=1, verbose=0, mode='min')\n",
    "\n",
    "    callbacks_list = [checkpoint, earlyStopping]\n",
    "\n",
    "    model.fit(x=x_train, y=y_train, batch_size=128, epochs=10, validation_split=0.1, callbacks=callbacks_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-29T10:39:45.256150Z",
     "start_time": "2019-04-29T10:39:45.252494Z"
    }
   },
   "source": [
    "## Apply model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Apply model on test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-16T04:51:11.893889Z",
     "start_time": "2019-05-16T04:42:44.967Z"
    }
   },
   "outputs": [],
   "source": [
    "model.load_weights(\"best_weights.hdf5\")\n",
    "y_test = model.predict(x_test, batch_size=128, verbose=1)\n",
    "y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-05T07:38:04.076572Z",
     "start_time": "2019-05-05T07:38:04.073683Z"
    },
    "heading_collapsed": true
   },
   "source": [
    "### Output result to file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-16T03:35:02.549514Z",
     "start_time": "2019-05-16T03:35:02.119469Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "result_dict = {}\n",
    "\n",
    "for i in range(len(test_df)):\n",
    "    if np.argmax(y_test[i]) == 0:\n",
    "        label = \"NOT ENOUGH INFO\"\n",
    "    elif np.argmax(y_test[i]) == 1:\n",
    "        label = \"REFUTES\"\n",
    "    else:\n",
    "        label = \"SUPPORTS\"\n",
    "    key = test_df['key'][i]\n",
    "    result_dict.update({\n",
    "        key:{\n",
    "            \"claim\": test_df['claim'][i],\n",
    "            \"label\": label,\n",
    "            \"evidence\": test_df['evidence'][i]\n",
    "        }\n",
    "    })\n",
    "    \n",
    "with open('result_on_dev.json', 'w') as outfile:\n",
    "    json.dump(result_dict, outfile, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "336px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
