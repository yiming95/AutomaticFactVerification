{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict Label with extracted evidence texts\n",
    "This notebook builds the MLP model for RTM step according to the FNC competition paper.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data preprocess"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load data as pandas DF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-17T07:19:50.010181Z",
     "start_time": "2019-05-17T07:19:47.720872Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>NOINFO</th>\n",
       "      <th>REF</th>\n",
       "      <th>SUP</th>\n",
       "      <th>claim</th>\n",
       "      <th>evi_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>Ireland does not have relatively low-lying mou...</td>\n",
       "      <td>The island 's geography comprises relatively l...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>The drama Dark Matter stars Taylor Schilling.</td>\n",
       "      <td>She made her film debut in the 2007 drama Dark...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>In 1932, Prussia was taken over.</td>\n",
       "      <td>In the Weimar Republic , the state of Prussia ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>IZombie premiered in 2015.</td>\n",
       "      <td>The series premiered on March 17 , 2015 .\\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>Ronald Reagan had a nationality.</td>\n",
       "      <td>Ronald Wilson Reagan -LRB- -LSB- ˈrɒnəld_ˈwɪls...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>Samoa Joe wrestles professionally.</td>\n",
       "      <td>Nuufolau Joel `` Joe '' Seanoa -LRB- born Marc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>University of Oxford is in the universe.</td>\n",
       "      <td>The University of Oxford -LRB- informally Oxfo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>The Renaissance began online.</td>\n",
       "      <td>The Hokies were led by 27th-year head coach Fr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>Portia de Rossi appeared on Scandal.</td>\n",
       "      <td>She appeared as a regular cast member on the A...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>The Berlin Wall was only standing for 10 years.</td>\n",
       "      <td>The Berlin Wall -LRB- Berliner Mauer -RRB- was...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   NOINFO  REF  SUP                                              claim  \\\n",
       "0       0    1    0  Ireland does not have relatively low-lying mou...   \n",
       "1       0    0    1      The drama Dark Matter stars Taylor Schilling.   \n",
       "2       0    0    1                   In 1932, Prussia was taken over.   \n",
       "3       0    0    1                         IZombie premiered in 2015.   \n",
       "4       0    0    1                   Ronald Reagan had a nationality.   \n",
       "5       0    0    1                 Samoa Joe wrestles professionally.   \n",
       "6       0    0    1           University of Oxford is in the universe.   \n",
       "7       1    0    0                      The Renaissance began online.   \n",
       "8       0    0    1               Portia de Rossi appeared on Scandal.   \n",
       "9       0    1    0    The Berlin Wall was only standing for 10 years.   \n",
       "\n",
       "                                            evi_text  \n",
       "0  The island 's geography comprises relatively l...  \n",
       "1  She made her film debut in the 2007 drama Dark...  \n",
       "2  In the Weimar Republic , the state of Prussia ...  \n",
       "3        The series premiered on March 17 , 2015 .\\n  \n",
       "4  Ronald Wilson Reagan -LRB- -LSB- ˈrɒnəld_ˈwɪls...  \n",
       "5  Nuufolau Joel `` Joe '' Seanoa -LRB- born Marc...  \n",
       "6  The University of Oxford -LRB- informally Oxfo...  \n",
       "7  The Hokies were led by 27th-year head coach Fr...  \n",
       "8  She appeared as a regular cast member on the A...  \n",
       "9  The Berlin Wall -LRB- Berliner Mauer -RRB- was...  "
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "train_file_path = \"./JSONFiles/\" + \"train_with_text.json\"\n",
    "use_test_file = False\n",
    "if use_test_file:\n",
    "    test_file_path = './JSONFiles/' + 'test_with_text.json'\n",
    "else:\n",
    "    test_file_path = './JSONFiles/' + 'dev_with_text.json'\n",
    "\n",
    "with open(train_file_path, mode='r') as f:\n",
    "    train = json.load(f)\n",
    "with open(test_file_path, mode='r') as f:\n",
    "    test = json.load(f)\n",
    "\n",
    "def load_training_data(dataset: dict) -> list:\n",
    "    dataset_list = []\n",
    "    for key in dataset.keys():\n",
    "        record = dataset.get(key)\n",
    "        claim = record.get(\"claim\")\n",
    "        evi_texts = record.get(\"evidence_texts\")\n",
    "        text = \"\"\n",
    "        for evi in evi_texts:\n",
    "            text += evi\n",
    "        SUP = NOINFO = REF = 0\n",
    "        if record.get(\"label\") == \"SUPPORTS\":\n",
    "            SUP = 1\n",
    "        elif record.get(\"label\") == \"REFUTES\":\n",
    "            REF = 1\n",
    "        else:\n",
    "            NOINFO = 1\n",
    "        dataset_record = {\n",
    "            \"claim\": claim,\n",
    "            \"evi_text\": text,\n",
    "            \"SUP\": SUP,\n",
    "            \"NOINFO\": NOINFO,\n",
    "            \"REF\": REF\n",
    "        }\n",
    "        dataset_list.append(dataset_record)\n",
    "    return dataset_list\n",
    "\n",
    "def load_test_data(dataset: dict) -> list:\n",
    "    dataset_list = []\n",
    "    for key in dataset.keys():\n",
    "        record = dataset.get(key)\n",
    "        claim = record.get(\"claim\")\n",
    "        evi_index = record.get(\"evidence\")\n",
    "        evi_texts = record.get(\"evidence_texts\")\n",
    "        text = \"\"\n",
    "        for evi in evi_texts:\n",
    "            text += evi\n",
    "\n",
    "        dataset_record = {\n",
    "            \"key\": key,\n",
    "            \"claim\": claim,\n",
    "            \"evidence\": evi_index,\n",
    "            \"evi_text\": text\n",
    "        }\n",
    "        dataset_list.append(dataset_record)\n",
    "    return dataset_list\n",
    "\n",
    "train_df = pd.DataFrame(load_training_data(train))\n",
    "test_df = pd.DataFrame(load_test_data(test))\n",
    "\n",
    "train_df[0: 10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-17T07:19:50.029301Z",
     "start_time": "2019-05-17T07:19:50.012344Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>claim</th>\n",
       "      <th>evi_text</th>\n",
       "      <th>evidence</th>\n",
       "      <th>key</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Ripon College's student number totaled in at a...</td>\n",
       "      <td>As of 2015 , Ripon College 's student body sto...</td>\n",
       "      <td>[[Ripon_College_-LRB-Wisconsin-RRB-, 1]]</td>\n",
       "      <td>100038</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Kesha was baptized on March 1st, 1987.</td>\n",
       "      <td>Kesha Rose Sebert -LRB- -LSB- ˈkɛʃə_roʊz_ˈsɛbə...</td>\n",
       "      <td>[[Kesha, 0]]</td>\n",
       "      <td>100083</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Birthday Song (2 Chainz song) was banned by So...</td>\n",
       "      <td>The song , which features fellow American rapp...</td>\n",
       "      <td>[[Birthday_Song_-LRB-2_Chainz_song-RRB-, 1]]</td>\n",
       "      <td>100169</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>The University of Illinois at Chicago is a col...</td>\n",
       "      <td>The University of Illinois at Chicago or UIC i...</td>\n",
       "      <td>[[University_of_Illinois_at_Chicago, 0]]</td>\n",
       "      <td>100234</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>French Indochina was officially known as the I...</td>\n",
       "      <td>Kenya is comfortably the next most successful ...</td>\n",
       "      <td>[[10,000_metres_at_the_World_Championships_in_...</td>\n",
       "      <td>100359</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Damon Albarn has refused to ever work with Bri...</td>\n",
       "      <td>His debut solo studio album Everyday Robots --...</td>\n",
       "      <td>[[Damon_Albarn, 17]]</td>\n",
       "      <td>100366</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Lost (TV series) is a series of plays.</td>\n",
       "      <td>Lost is an American television drama series th...</td>\n",
       "      <td>[[Lost_-LRB-TV_series-RRB-, 0]]</td>\n",
       "      <td>100429</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Edison Machine Works was barely set up to prod...</td>\n",
       "      <td>The neighborhood is located between 22nd Stree...</td>\n",
       "      <td>[[Hospital_Hill, 1]]</td>\n",
       "      <td>100457</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>The human brain is set apart from mammalian br...</td>\n",
       "      <td>Studebaker Building -LRB- Missoula , Montana -...</td>\n",
       "      <td>[[Studebaker_Building, 14]]</td>\n",
       "      <td>100461</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>There are rumors that Augustus' wife, Livia, p...</td>\n",
       "      <td>Between 1945 and 1954 , Leir saw service in fi...</td>\n",
       "      <td>[[Richard_H._Leir, 10]]</td>\n",
       "      <td>100481</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               claim  \\\n",
       "0  Ripon College's student number totaled in at a...   \n",
       "1             Kesha was baptized on March 1st, 1987.   \n",
       "2  Birthday Song (2 Chainz song) was banned by So...   \n",
       "3  The University of Illinois at Chicago is a col...   \n",
       "4  French Indochina was officially known as the I...   \n",
       "5  Damon Albarn has refused to ever work with Bri...   \n",
       "6             Lost (TV series) is a series of plays.   \n",
       "7  Edison Machine Works was barely set up to prod...   \n",
       "8  The human brain is set apart from mammalian br...   \n",
       "9  There are rumors that Augustus' wife, Livia, p...   \n",
       "\n",
       "                                            evi_text  \\\n",
       "0  As of 2015 , Ripon College 's student body sto...   \n",
       "1  Kesha Rose Sebert -LRB- -LSB- ˈkɛʃə_roʊz_ˈsɛbə...   \n",
       "2  The song , which features fellow American rapp...   \n",
       "3  The University of Illinois at Chicago or UIC i...   \n",
       "4  Kenya is comfortably the next most successful ...   \n",
       "5  His debut solo studio album Everyday Robots --...   \n",
       "6  Lost is an American television drama series th...   \n",
       "7  The neighborhood is located between 22nd Stree...   \n",
       "8  Studebaker Building -LRB- Missoula , Montana -...   \n",
       "9  Between 1945 and 1954 , Leir saw service in fi...   \n",
       "\n",
       "                                            evidence     key  \n",
       "0           [[Ripon_College_-LRB-Wisconsin-RRB-, 1]]  100038  \n",
       "1                                       [[Kesha, 0]]  100083  \n",
       "2       [[Birthday_Song_-LRB-2_Chainz_song-RRB-, 1]]  100169  \n",
       "3           [[University_of_Illinois_at_Chicago, 0]]  100234  \n",
       "4  [[10,000_metres_at_the_World_Championships_in_...  100359  \n",
       "5                               [[Damon_Albarn, 17]]  100366  \n",
       "6                    [[Lost_-LRB-TV_series-RRB-, 0]]  100429  \n",
       "7                               [[Hospital_Hill, 1]]  100457  \n",
       "8                        [[Studebaker_Building, 14]]  100461  \n",
       "9                            [[Richard_H._Leir, 10]]  100481  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df[0: 10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tokenization and Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-17T07:20:53.400161Z",
     "start_time": "2019-05-17T07:19:50.031409Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/wenbin/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>NOINFO</th>\n",
       "      <th>REF</th>\n",
       "      <th>SUP</th>\n",
       "      <th>claim</th>\n",
       "      <th>evi_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>ireland do not have relatively low lie mountain</td>\n",
       "      <td>the island s geography comprise relatively low...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>the drama dark matter star taylor schilling</td>\n",
       "      <td>she make her film debut in the 2007 drama dark...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>in 1932 prussia be take over</td>\n",
       "      <td>in the weimar republic the state of prussia lo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>izombie premier in 2015</td>\n",
       "      <td>the series premier on march 17 2015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>ronald reagan have a nationality</td>\n",
       "      <td>ronald wilson reagan lrb lsb ˈrɒnəld_ˈwɪlsən_ˈ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>samoa joe wrestle professionally</td>\n",
       "      <td>nuufolau joel joe seanoa lrb bear march 17 197...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>university of oxford be in the universe</td>\n",
       "      <td>the university of oxford lrb informally oxford...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>the renaissance begin online</td>\n",
       "      <td>the hokies be lead by 27th year head coach fra...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>portia de rossi appear on scandal</td>\n",
       "      <td>she appear a a regular cast member on the amer...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>the berlin wall be only stand for 10 year</td>\n",
       "      <td>the berlin wall lrb berliner mauer rrb be a gu...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   NOINFO  REF  SUP                                            claim  \\\n",
       "0       0    1    0  ireland do not have relatively low lie mountain   \n",
       "1       0    0    1      the drama dark matter star taylor schilling   \n",
       "2       0    0    1                     in 1932 prussia be take over   \n",
       "3       0    0    1                          izombie premier in 2015   \n",
       "4       0    0    1                 ronald reagan have a nationality   \n",
       "5       0    0    1                 samoa joe wrestle professionally   \n",
       "6       0    0    1          university of oxford be in the universe   \n",
       "7       1    0    0                     the renaissance begin online   \n",
       "8       0    0    1                portia de rossi appear on scandal   \n",
       "9       0    1    0        the berlin wall be only stand for 10 year   \n",
       "\n",
       "                                            evi_text  \n",
       "0  the island s geography comprise relatively low...  \n",
       "1  she make her film debut in the 2007 drama dark...  \n",
       "2  in the weimar republic the state of prussia lo...  \n",
       "3                the series premier on march 17 2015  \n",
       "4  ronald wilson reagan lrb lsb ˈrɒnəld_ˈwɪlsən_ˈ...  \n",
       "5  nuufolau joel joe seanoa lrb bear march 17 197...  \n",
       "6  the university of oxford lrb informally oxford...  \n",
       "7  the hokies be lead by 27th year head coach fra...  \n",
       "8  she appear a a regular cast member on the amer...  \n",
       "9  the berlin wall lrb berliner mauer rrb be a gu...  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "\n",
    "tokenizer = nltk.tokenize.RegexpTokenizer(r'\\w+')\n",
    "lemmatizer = nltk.stem.WordNetLemmatizer()\n",
    "\n",
    "def lemmatize(word):\n",
    "    lemma = lemmatizer.lemmatize(word,'v')\n",
    "    if lemma == word:\n",
    "        lemma = lemmatizer.lemmatize(word,'n')\n",
    "    return lemma\n",
    "\n",
    "def pre_process(comment) -> str:\n",
    "    # lower cased\n",
    "    comment = comment.lower()\n",
    "    # tokenize\n",
    "    words = tokenizer.tokenize(comment)\n",
    "    # lemmatize \n",
    "    words = [lemmatize(w) for w in words]\n",
    "    # remove stop words\n",
    "#     stop_words = nltk.corpus.stopwords.words('english')\n",
    "#     words = [w for w in words if not w in stop_words]\n",
    "    # return result\n",
    "    processed_comment = \" \".join(words)\n",
    "    return processed_comment\n",
    "\n",
    "def process_dataset(dataset: pd.DataFrame) -> pd.DataFrame:\n",
    "    corpus = pd.concat([dataset['claim'], dataset['evi_text']])\n",
    "    processed_corpus = corpus.apply(lambda text: pre_process(text))\n",
    "    dataset['claim'] = processed_corpus.iloc[0: len(dataset)]\n",
    "    dataset['evi_text'] = processed_corpus.iloc[len(dataset):,]\n",
    "    return dataset\n",
    "\n",
    "train_df = process_dataset(train_df)\n",
    "test_df = process_dataset(test_df)\n",
    "train_df[0: 10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TF Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-17T07:21:07.116887Z",
     "start_time": "2019-05-17T07:20:53.402438Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from scipy.sparse import coo_matrix, hstack\n",
    "\n",
    "max_features = 5000 \n",
    "\n",
    "train_corpus = pd.concat([train_df['claim'], train_df['evi_text']])\n",
    "test_corpus = pd.concat([test_df['claim'], test_df['evi_text']])\n",
    "\n",
    "tf_vectorizer = CountVectorizer(max_features=max_features)\n",
    "tf_vectorizer.fit(train_corpus)\n",
    "train_claim_tf_features = tf_vectorizer.transform(train_df['claim'])\n",
    "train_evi_tf_features = tf_vectorizer.transform(train_df['evi_text'])\n",
    "test_claim_tf_features = tf_vectorizer.transform(test_df['claim'])\n",
    "test_evi_tf_features = tf_vectorizer.transform(test_df['claim'])\n",
    "\n",
    "train_tf_features = hstack([train_claim_tf_features, train_evi_tf_features])\n",
    "test_tf_features = hstack([test_claim_tf_features, test_evi_tf_features])\n",
    "# claim_tf_vectorizer = CountVectorizer(max_features=max_features)\n",
    "# claim_tf = claim_tf_vectorizer.fit_transform(train_df['claim'])\n",
    "# evi_text_tf_vectorizer = CountVectorizer(max_features=max_features)\n",
    "# evi_text_tf = evi_text_tf_vectorizer.fit_transform(train_df['evi_text'])\n",
    "# tf_features = hstack([claim_tf, evi_text_tf])\n",
    "\n",
    "# tf_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TF_IDF Cosine similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-17T07:22:09.715671Z",
     "start_time": "2019-05-17T07:21:07.118680Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "max_features = 5000\n",
    "\n",
    "all_corpus = pd.concat([train_corpus, test_corpus])\n",
    "\n",
    "def calculate_cosines(claim_tfidf, evi_tfidf) -> np.ndarray:\n",
    "    cosines = np.zeros((claim_tfidf.shape[0], 1))\n",
    "    for i in range(len(cosines)):\n",
    "        claim_vector = claim_tfidf[i]\n",
    "        evi_vector = evi_tfidf[i]\n",
    "        cosine_matrix = cosine_similarity([claim_vector.toarray()[0], evi_vector.toarray()[0]])\n",
    "        cosines[i][0] = cosine_matrix[0][1]\n",
    "    return cosines\n",
    "\n",
    "tfidf_vectorizer = TfidfVectorizer(max_features=max_features, norm='l2')\n",
    "tfidf_vectorizer.fit(all_corpus)\n",
    "\n",
    "train_claim_tfidf = tfidf_vectorizer.transform(train_df['claim'])\n",
    "train_evi_tfidf = tfidf_vectorizer.transform(train_df['evi_text'])\n",
    "train_cosines = calculate_cosines(train_claim_tfidf, train_evi_tfidf)\n",
    "\n",
    "test_claim_tfidf = tfidf_vectorizer.transform(test_df['claim'])\n",
    "test_evi_tfidf = tfidf_vectorizer.transform(test_df['evi_text'])\n",
    "test_cosines = calculate_cosines(test_claim_tfidf, test_evi_tfidf)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-04T09:25:00.223600Z",
     "start_time": "2019-05-04T09:25:00.220788Z"
    }
   },
   "source": [
    "#### Concat features together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-17T07:22:13.822276Z",
     "start_time": "2019-05-17T07:22:09.717493Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(145449, 10001)\n",
      "(145449, 3)\n",
      "(5001, 10001)\n"
     ]
    }
   ],
   "source": [
    "x_train = hstack([train_tf_features, train_cosines]).toarray()\n",
    "y_train = train_df[train_df.columns[0:3]].values\n",
    "x_test = hstack([test_tf_features, test_cosines]).toarray()\n",
    "\n",
    "print(x_train.shape)\n",
    "print(y_train.shape)\n",
    "print(x_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build and Train model\n",
    "Build an MLP with tensor (10001, 1) as input, 1 hidden layer with 100 neurons, and softmax layer for output. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simple MLP model prototype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-17T07:23:56.037134Z",
     "start_time": "2019-05-17T07:22:13.824111Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_1 (Dense)              (None, 100)               1000200   \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 3)                 303       \n",
      "=================================================================\n",
      "Total params: 1,000,503\n",
      "Trainable params: 1,000,503\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Train on 130904 samples, validate on 14545 samples\n",
      "Epoch 1/50\n",
      "130904/130904 [==============================] - 26s 198us/step - loss: 0.5327 - acc: 0.7854 - val_loss: 0.4440 - val_acc: 0.8118\n",
      "\n",
      "Epoch 00001: val_acc improved from -inf to 0.81183, saving model to best_weights.hdf5\n",
      "Epoch 2/50\n",
      "130904/130904 [==============================] - 27s 210us/step - loss: 0.3863 - acc: 0.8447 - val_loss: 0.4203 - val_acc: 0.8228\n",
      "\n",
      "Epoch 00002: val_acc improved from 0.81183 to 0.82283, saving model to best_weights.hdf5\n",
      "Epoch 3/50\n",
      "130904/130904 [==============================] - 24s 181us/step - loss: 0.3472 - acc: 0.8600 - val_loss: 0.4002 - val_acc: 0.8355\n",
      "\n",
      "Epoch 00003: val_acc improved from 0.82283 to 0.83548, saving model to best_weights.hdf5\n",
      "Epoch 4/50\n",
      "130904/130904 [==============================] - 24s 181us/step - loss: 0.3249 - acc: 0.8685 - val_loss: 0.4028 - val_acc: 0.8316\n",
      "\n",
      "Epoch 00004: val_acc did not improve from 0.83548\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x427c31828>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# from keras.layers import Dense, Input, LSTM, Embedding, Dropout, Activation\n",
    "# from keras.layers import Bidirectional, GlobalMaxPool1D\n",
    "\n",
    "# {lr=0.01, batch_size=128, dropout=0.5, units=100}\n",
    "# {lr=0.001, batch_size=256, dropout=0.6, units=100}\n",
    "seed = 7\n",
    "np.random.seed(seed)\n",
    "\n",
    "import keras\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout\n",
    "from IPython.display import SVG\n",
    "from keras.utils.vis_utils import model_to_dot\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(units=100, activation='relu', input_dim=x_train.shape[1]))\n",
    "model.add(Dropout(0.6))\n",
    "model.add(Dense(units=3, activation='softmax'))\n",
    "optimizer = Adam(lr=0.01)\n",
    "model.compile(loss=keras.losses.categorical_crossentropy,\n",
    "              optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "model.summary()\n",
    "# SVG(model_to_dot(model).create(prog='dot', format='svg'))\n",
    "\n",
    "# callbacks\n",
    "filepath=\"best_weights.hdf5\"\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='val_acc', verbose=1, save_best_only=True, mode='max')\n",
    "earlyStopping = EarlyStopping(monitor='val_acc', patience=1, verbose=0, mode='auto')\n",
    "\n",
    "callbacks_list = [checkpoint, earlyStopping]\n",
    "\n",
    "model.fit(x=x_train, y=y_train, batch_size=128, epochs=50, validation_split=0.1, callbacks=callbacks_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Tune hyper-parameters mannually"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-17T07:23:56.054367Z",
     "start_time": "2019-05-17T07:23:56.043285Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# import itertools\n",
    "\n",
    "# class Cartesian(object):\n",
    "#     def __init__(self):\n",
    "#         self._data_list = []\n",
    "#         self._name_list = []\n",
    "#         self.cartesian_result = []\n",
    "\n",
    "#     def add_data(self, data, name): #add list for cartesian product\n",
    "#         self._data_list.append(data)\n",
    "#         self._name_list.append(name)\n",
    "\n",
    "#     def build(self): #calculate cartesian product\n",
    "#         for item in itertools.product(*self._data_list):\n",
    "#             result_dict = {}\n",
    "#             for i in range(len(item)):\n",
    "#                 result_dict.update({\n",
    "#                     self._name_list[i]: item[i]\n",
    "#                 })\n",
    "#             self.cartesian_result.append(result_dict)\n",
    "#         return self.cartesian_result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-17T07:23:56.073601Z",
     "start_time": "2019-05-17T07:23:56.057559Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# # from keras.layers import Dense, Input, LSTM, Embedding, Dropout, Activation\n",
    "# # from keras.layers import Bidirectional, GlobalMaxPool1D\n",
    "# from keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "# from keras.models import Sequential\n",
    "# from keras.layers import Dense, Dropout\n",
    "# from keras.utils.vis_utils import model_to_dot\n",
    "# from keras.optimizers import Adam\n",
    "# import keras\n",
    "\n",
    "# # fix random seed for reproducibility\n",
    "# seed = 7\n",
    "# np.random.seed(seed)\n",
    "\n",
    "# def create_model(units = 100, dropout = 0.5, lr = 0.001):\n",
    "#     model = Sequential()\n",
    "#     model.add(Dense(units=units, activation='relu', input_dim=x_train.shape[1]))\n",
    "#     model.add(Dropout(dropout))\n",
    "#     model.add(Dense(units=3, activation='softmax'))\n",
    "#     optimizer = Adam(lr=lr)\n",
    "#     model.compile(loss=keras.losses.categorical_crossentropy,\n",
    "#                   optimizer='adam', metrics=['accuracy'])\n",
    "#     return model\n",
    "\n",
    "# def fit_model(model, batch_size=32):\n",
    "#     earlyStopping = EarlyStopping(monitor='val_acc', patience=3, \n",
    "#                                   verbose=0, mode='auto')\n",
    "#     EarlyStopping()\n",
    "#     callbacks_list = [earlyStopping]\n",
    "\n",
    "#     model_history = model.fit(x=x_train, y=y_train, \n",
    "#                               batch_size=batch_size, epochs=50, \n",
    "#                               validation_split=0.1, callbacks=callbacks_list, verbose=1)\n",
    "#     return model_history\n",
    "\n",
    "# units_list = [25, 50, 100, 250, 500]\n",
    "# dropout_list = [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7]\n",
    "# batch_size_list = [16, 32, 64, 128, 256, 512]\n",
    "# lr_list = [0.0001, 0.001, 0.01, 0.1]\n",
    "    \n",
    "# car_product=Cartesian()\n",
    "# car_product.add_data(units_list, 'units')\n",
    "# car_product.add_data(dropout_list, 'dropout')\n",
    "# car_product.add_data(batch_size_list, 'batch_size')\n",
    "# car_product.add_data(lr_list, 'lr')\n",
    "# parameter_combinations = car_product.build()\n",
    "\n",
    "# historys_list = []\n",
    "# iternum = 0\n",
    "# for combination in parameter_combinations:\n",
    "#     print(\"itertion: \" + str(iternum))\n",
    "#     print(combination)\n",
    "#     model = create_model(units=combination['units'], \n",
    "#                          dropout=combination['dropout'], \n",
    "#                          lr=combination['lr'])\n",
    "#     model_history = fit_model(model=model, batch_size=combination['batch_size'])\n",
    "#     historys_list.append({\n",
    "#         'combination': combination,\n",
    "#         'max_val_acc': max(model_history.history['val_acc'])\n",
    "#     })\n",
    "#     print(\"result: \" + str(max(model_history.history['val_acc'])))\n",
    "#     iternum += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-17T07:23:56.080040Z",
     "start_time": "2019-05-17T07:23:56.076260Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# # sort and output_to_file\n",
    "# ordered_history = sorted(historys_list, key= lambda x: x['max_val_acc'], reverse=True)\n",
    "\n",
    "# historys_list_dict = {\n",
    "#     \"historys\": ordered_history\n",
    "# }\n",
    "# with open('tune_hps.json', 'w') as hp_result:\n",
    "#     json.dump(ordered_history, hp_result, indent=4)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### Tune hyper-parameters with sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-17T07:23:56.091078Z",
     "start_time": "2019-05-17T07:23:56.082258Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# from sklearn.model_selection import GridSearchCV\n",
    "# from keras.wrappers.scikit_learn import KerasClassifier\n",
    "# from sklearn.model_selection import train_test_split\n",
    "# import keras\n",
    "# from keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "# from keras.models import Sequential\n",
    "# from keras.layers import Dense, Dropout\n",
    "# from IPython.display import SVG\n",
    "# from keras.utils.vis_utils import model_to_dot\n",
    "\n",
    "\n",
    "# # # fix random seed for reproducibility\n",
    "# seed = 7\n",
    "# np.random.seed(seed)\n",
    "\n",
    "# def create_model():\n",
    "#     model = Sequential()\n",
    "#     model.add(Dense(units=100, activation='relu', input_dim=x_train.shape[1]))\n",
    "#     model.add(Dropout(0.3))\n",
    "#     model.add(Dense(units=3, activation='softmax'))\n",
    "#     model.compile(loss=keras.losses.categorical_crossentropy,\n",
    "#                   optimizer='adam', metrics=['accuracy'])\n",
    "#     return model\n",
    "\n",
    "# model = KerasClassifier(build_fn=create_model, verbose=2)\n",
    "# batch_size = [64, 128]\n",
    "# # epochs = [1, 2]\n",
    "# param_grid = dict(batch_size=batch_size)\n",
    "# grid = GridSearchCV(estimator=model, param_grid=param_grid, n_jobs=-1, cv=2)\n",
    "# grid_result = grid.fit(X=x_train, y=y_train)\n",
    "\n",
    "\n",
    "# # summarize results\n",
    "# # print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n",
    "# # means = grid_result.cv_results_['mean_test_score']\n",
    "# # stds = grid_result.cv_results_['std_test_score']\n",
    "# # params = grid_result.cv_results_['params']\n",
    "# # for mean, stdev, param in zip(means, stds, params):\n",
    "# #     print(\"%f (%f) with: %r\" % (mean, stdev, param))\n",
    "\n",
    "# grid_result.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-16T05:09:57.964156Z",
     "start_time": "2019-05-16T05:09:57.958931Z"
    },
    "cell_style": "center",
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-17T07:23:56.097294Z",
     "start_time": "2019-05-17T07:23:56.092933Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# def create_model():\n",
    "#     model = Sequential()\n",
    "#     model.add(Dense(units=100, activation='relu', input_dim=x_train.shape[1]))\n",
    "#     model.add(Dropout(0.3))\n",
    "#     model.add(Dense(units=3, activation='softmax'))\n",
    "#     model.compile(loss=keras.losses.categorical_crossentropy,\n",
    "#                   optimizer='adam', metrics=['accuracy'], verbose=2)\n",
    "#     return model\n",
    "\n",
    "# def fit_model():\n",
    "#     # callbacks\n",
    "#     filepath=\"best_weights.hdf5\"\n",
    "#     checkpoint = ModelCheckpoint(filepath, monitor='val_acc', verbose=1, save_best_only=True, mode='max')\n",
    "#     earlyStopping = EarlyStopping(monitor='val_acc', patience=1, verbose=0, mode='min')\n",
    "\n",
    "#     callbacks_list = [checkpoint, earlyStopping]\n",
    "\n",
    "#     model.fit(x=x_train, y=y_train, batch_size=128, epochs=10, validation_split=0.1, callbacks=callbacks_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-29T10:39:45.256150Z",
     "start_time": "2019-04-29T10:39:45.252494Z"
    }
   },
   "source": [
    "## Apply model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Apply model on test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-17T07:23:56.924704Z",
     "start_time": "2019-05-17T07:23:56.099079Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5001/5001 [==============================] - 1s 149us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[2.6491040e-02, 2.2745822e-01, 7.4605072e-01],\n",
       "       [7.2447814e-02, 4.6273494e-01, 4.6481720e-01],\n",
       "       [1.3670814e-02, 3.6108416e-01, 6.2524509e-01],\n",
       "       ...,\n",
       "       [2.6898188e-04, 9.7940642e-01, 2.0324644e-02],\n",
       "       [6.6550346e-03, 4.6262115e-01, 5.3072381e-01],\n",
       "       [3.6028886e-01, 1.8822050e-01, 4.5149055e-01]], dtype=float32)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_weights(\"best_weights.hdf5\")\n",
    "y_test = model.predict(x_test, batch_size=128, verbose=1)\n",
    "y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-05T07:38:04.076572Z",
     "start_time": "2019-05-05T07:38:04.073683Z"
    }
   },
   "source": [
    "### Output result to file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-17T07:23:57.350551Z",
     "start_time": "2019-05-17T07:23:56.926622Z"
    }
   },
   "outputs": [],
   "source": [
    "result_dict = {}\n",
    "\n",
    "for i in range(len(test_df)):\n",
    "    if np.argmax(y_test[i]) == 0:\n",
    "        label = \"NOT ENOUGH INFO\"\n",
    "    elif np.argmax(y_test[i]) == 1:\n",
    "        label = \"REFUTES\"\n",
    "    else:\n",
    "        label = \"SUPPORTS\"\n",
    "    key = test_df['key'][i]\n",
    "    result_dict.update({\n",
    "        key:{\n",
    "            \"claim\": test_df['claim'][i],\n",
    "            \"label\": label,\n",
    "            \"evidence\": test_df['evidence'][i]\n",
    "        }\n",
    "    })\n",
    "    \n",
    "with open('result_on_dev.json', 'w') as outfile:\n",
    "    json.dump(result_dict, outfile, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "336px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
