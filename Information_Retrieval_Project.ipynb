{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Information Retrieval\n",
    "\n",
    "This is the document retrieval and sentence retrieval part of the project."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  1. Unpack the zip file.\n",
    "    \n",
    "Unpack the 'wiki-pages-text.zip' in the current directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-09T16:18:13.994080Z",
     "start_time": "2019-05-09T16:18:13.991649Z"
    }
   },
   "outputs": [],
   "source": [
    "import zipfile\n",
    "def unpack():\n",
    "    with zipfile.ZipFile('wiki-pages-text.zip') as file:\n",
    "        file.extractall()\n",
    "# unpack()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Load file. \n",
    "\n",
    "Load the training dataset and the wiki txt file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-09T16:18:35.284445Z",
     "start_time": "2019-05-09T16:18:17.713474Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of the train data is: 145449\n",
      "Length of the dev data is: 5001\n",
      "Length of the dev result data is: 5001\n",
      "Length of the test data is: 14997\n",
      "Length of the corpus is: 25248397\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "with open('train.json', 'r') as f:  # load training dataset\n",
    "        train_data = json.load(f)   \n",
    "print(\"Length of the train data is: \" + str(len(train_data)))\n",
    "\n",
    "with open('devset.json', 'r') as f1:  # load dev dataset\n",
    "        dev_data = json.load(f1) \n",
    "print(\"Length of the dev data is: \" + str(len(dev_data)))\n",
    "\n",
    "with open('devset_result.json', 'r') as f2:  # store result \n",
    "        res_data = json.load(f2) \n",
    "print(\"Length of the dev result data is: \" + str(len(res_data)))\n",
    "\n",
    "with open('test-unlabelled.json', 'r') as f3:  # store result \n",
    "     test_data = json.load(f3) \n",
    "print(\"Length of the test data is: \" + str(len(test_data)))\n",
    "        \n",
    "# appeand all the wiki txt sentences to one document\n",
    "def loadfile(folder): \n",
    "    corpus = []\n",
    "    list_of_files = os.listdir(folder)\n",
    "    \n",
    "    for file in list_of_files:\n",
    "        try:\n",
    "            filename = os.path.join(folder, file)\n",
    "            with open(filename, 'r') as doc:\n",
    "                for line in doc:\n",
    "                    corpus.append(line)     \n",
    "        except Exception as e:\n",
    "            print(\"No files found here!\")\n",
    "            raise e\n",
    "    return corpus\n",
    "corpus = loadfile(\"wiki-pages-text\")\n",
    "print(\"Length of the corpus is: \" + str(len(corpus)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pandas process the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-09T17:04:37.667902Z",
     "start_time": "2019-05-09T17:04:37.637236Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1000, 3)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>page_identifier</th>\n",
       "      <th>sentence_number</th>\n",
       "      <th>sentence_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Alexander_McNair</td>\n",
       "      <td>0</td>\n",
       "      <td>Alexander McNair -LRB- May 5   1775 -- March 1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Alexander_McNair</td>\n",
       "      <td>1</td>\n",
       "      <td>He was the first Governor of Missouri from its...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Alexander_McNair</td>\n",
       "      <td>4</td>\n",
       "      <td>McNair was born in Lancaster in the Province o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Alexander_McNair</td>\n",
       "      <td>5</td>\n",
       "      <td>His grandfather   David McNair   Sr.   immigra...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Alexander_McNair</td>\n",
       "      <td>6</td>\n",
       "      <td>David McNair   Jr.   Alexander 's father -LRB-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Alexander_McNair</td>\n",
       "      <td>7</td>\n",
       "      <td>Alexander went to school as a child   and atte...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Alexander_McNair</td>\n",
       "      <td>8</td>\n",
       "      <td>He reached an agreement with his mother and br...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Alexander_McNair</td>\n",
       "      <td>9</td>\n",
       "      <td>Alexander was defeated .\\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Alexander_McNair</td>\n",
       "      <td>10</td>\n",
       "      <td>He became a member of the Pennsylvania militia...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Alexander_McNair</td>\n",
       "      <td>13</td>\n",
       "      <td>In 1804   McNair traveled to what is now Misso...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    page_identifier sentence_number  \\\n",
       "0  Alexander_McNair               0   \n",
       "1  Alexander_McNair               1   \n",
       "2  Alexander_McNair               4   \n",
       "3  Alexander_McNair               5   \n",
       "4  Alexander_McNair               6   \n",
       "5  Alexander_McNair               7   \n",
       "6  Alexander_McNair               8   \n",
       "7  Alexander_McNair               9   \n",
       "8  Alexander_McNair              10   \n",
       "9  Alexander_McNair              13   \n",
       "\n",
       "                                       sentence_text  \n",
       "0  Alexander McNair -LRB- May 5   1775 -- March 1...  \n",
       "1  He was the first Governor of Missouri from its...  \n",
       "2  McNair was born in Lancaster in the Province o...  \n",
       "3  His grandfather   David McNair   Sr.   immigra...  \n",
       "4  David McNair   Jr.   Alexander 's father -LRB-...  \n",
       "5  Alexander went to school as a child   and atte...  \n",
       "6  He reached an agreement with his mother and br...  \n",
       "7                         Alexander was defeated .\\n  \n",
       "8  He became a member of the Pennsylvania militia...  \n",
       "9  In 1804   McNair traveled to what is now Misso...  "
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus_df = pd.DataFrame(corpus[:1000])\n",
    "corpus_df.columns = ['text']\n",
    "\n",
    "corpus_df['page_identifier'] = corpus_df.text.apply(lambda x: x.split(' ')[0])  \n",
    "corpus_df['sentence_number'] = corpus_df.text.apply(lambda x: x.split(' ')[1]) \n",
    "corpus_df['sentence_text'] = corpus_df.text.apply(lambda x: x.split(' ')[2:])  \n",
    "corpus_df['sentence_text'] = [','.join(map(str, l)) for l in corpus_df['sentence_text']]\n",
    "corpus_df[\"sentence_text\"] = corpus_df['sentence_text'].str.replace(',',' ')\n",
    "corpus_df = corpus_df.drop('text', 1)\n",
    "\n",
    "print(corpus_df.shape)\n",
    "corpus_df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Preprocess \n",
    "\n",
    "Preprocess includes: strip punctuations, tokenize,lemma, lower case, remove stop words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-09T17:04:42.158635Z",
     "start_time": "2019-05-09T17:04:42.152263Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/zhangyiming/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "tokenizer = nltk.tokenize.RegexpTokenizer(r'\\w+')\n",
    "lemmatizer = nltk.stem.WordNetLemmatizer()\n",
    "\n",
    "nltk.download('wordnet')\n",
    "def lemmatize(word):\n",
    "    lemma = lemmatizer.lemmatize(word,'v')\n",
    "    if lemma == word:\n",
    "        lemma = lemmatizer.lemmatize(word,'n')\n",
    "    return lemma\n",
    "\n",
    "def pre_process(comment) -> str:\n",
    "    # lower cased\n",
    "    comment = comment.lower()\n",
    "    # tokenize\n",
    "    words = tokenizer.tokenize(comment)\n",
    "    # lemmatize \n",
    "    words = [lemmatize(w) for w in words]\n",
    "    processed_comment = \" \".join(words)\n",
    "    return processed_comment\n",
    "\n",
    "def process_dataset(dataset: pd.DataFrame) -> pd.DataFrame:\n",
    "    processed_corpus = dataset['sentence_text'].apply(lambda text: pre_process(text))\n",
    "    return processed_corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Save the pre-processed corpus dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-09T17:04:44.923621Z",
     "start_time": "2019-05-09T17:04:44.764143Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>alexander mcnair lrb may 5 1775 march 18 1826 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>he be the first governor of missouri from it e...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>mcnair be bear in lancaster in the province of...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>his grandfather david mcnair sr immigrate to p...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>david mcnair jr alexander s father lrb b 1736 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>alexander go to school a a child and attend on...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>he reach an agreement with his mother and brot...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>alexander be defeat</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>he become a member of the pennsylvania militia...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>in 1804 mcnair travel to what be now missouri ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       sentence_text\n",
       "0  alexander mcnair lrb may 5 1775 march 18 1826 ...\n",
       "1  he be the first governor of missouri from it e...\n",
       "2  mcnair be bear in lancaster in the province of...\n",
       "3  his grandfather david mcnair sr immigrate to p...\n",
       "4  david mcnair jr alexander s father lrb b 1736 ...\n",
       "5  alexander go to school a a child and attend on...\n",
       "6  he reach an agreement with his mother and brot...\n",
       "7                                alexander be defeat\n",
       "8  he become a member of the pennsylvania militia...\n",
       "9  in 1804 mcnair travel to what be now missouri ..."
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import datetime\n",
    "# start = datetime.datetime.now()\n",
    "processed_corpus_df = pd.DataFrame(process_dataset(corpus_df))\n",
    "processed_corpus_df.to_pickle(\"./processed_corpus.pkl\")\n",
    "processed_corpus_df.head(10)\n",
    "# end = datetime.datetime.now()\n",
    "# print(end-start)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load pre-processed corpus dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-09T16:26:50.559145Z",
     "start_time": "2019-05-09T16:26:50.547160Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1000, 1)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>alexander mcnair lrb may 5 1775 march 18 1826 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>he be the first governor of missouri from it e...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>mcnair be bear in lancaster in the province of...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>his grandfather david mcnair sr immigrate to p...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>david mcnair jr alexander s father lrb b 1736 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>alexander go to school a a child and attend on...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>he reach an agreement with his mother and brot...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>alexander be defeat</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>he become a member of the pennsylvania militia...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>in 1804 mcnair travel to what be now missouri ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       sentence_text\n",
       "0  alexander mcnair lrb may 5 1775 march 18 1826 ...\n",
       "1  he be the first governor of missouri from it e...\n",
       "2  mcnair be bear in lancaster in the province of...\n",
       "3  his grandfather david mcnair sr immigrate to p...\n",
       "4  david mcnair jr alexander s father lrb b 1736 ...\n",
       "5  alexander go to school a a child and attend on...\n",
       "6  he reach an agreement with his mother and brot...\n",
       "7                                alexander be defeat\n",
       "8  he become a member of the pennsylvania militia...\n",
       "9  in 1804 mcnair travel to what be now missouri ..."
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_processed_corpus_df = pd.read_pickle(\"./processed_corpus.pkl\")\n",
    "print(load_processed_corpus_df.shape)\n",
    "load_processed_corpus_df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Use Sklearn to build tf-idf.\n",
    "\n",
    "tfidf_vectorizer is the tf-idf model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-09T17:12:07.561387Z",
     "start_time": "2019-05-09T17:12:07.525216Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary Size :  4650\n",
      "Shape of Matrix :  (4650, 1000)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import pickle\n",
    "\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "tfidf = tfidf_vectorizer.fit_transform(load_processed_corpus_df['sentence_text'])\n",
    "tfidf = tfidf.T\n",
    "\n",
    "print('Vocabulary Size : ', len(tfidf_vectorizer.get_feature_names()))\n",
    "print('Shape of Matrix : ', tfidf.shape)\n",
    "\n",
    "pickle.dump(tfidf, open(\"tfidf.pickle\",\"wb\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Apply SVD "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-09T17:37:32.340031Z",
     "start_time": "2019-05-09T17:37:31.288097Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4650, 20)\n",
      "(1000, 20)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Applying SVD\n",
    "K= 20 # number of desirable features \n",
    "U, s, VT = np.linalg.svd(tfidf.toarray())\n",
    "tfidf_reduced = np.dot(U[:,:K], np.dot(np.diag(s[:K]), VT[:K, :]))\n",
    "\n",
    "# Getting document and term representation\n",
    "terms_rep = np.dot(U[:,:K], np.diag(s[:K])) # M X K matrix where M = Vocabulary Size and N = Number of documents\n",
    "docs_rep = np.dot(np.diag(s[:K]), VT[:K, :]).T # N x K matrix \n",
    "\n",
    "print(terms_rep.shape)\n",
    "print(docs_rep.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  Test tfidf with query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-09T18:11:18.764172Z",
     "start_time": "2019-05-09T18:11:18.739271Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.08030084  0.07131535 -0.02114174 -0.01343357  0.00086819 -0.00232813\n",
      " -0.10948688 -0.00657705 -0.03682033 -0.01810879 -0.02005786  0.02662247\n",
      " -0.04509342  0.0063877  -0.03200761  0.03458823  0.02123711  0.05318392\n",
      "  0.02635511 -0.02561251]\n",
      "[-0.13757185  0.19350078 -0.02065905  0.12258203 -0.02870684  0.02541737\n",
      " -0.18322714  0.01045996 -0.11076183  0.03505023 -0.06895175  0.04991295\n",
      " -0.05618872  0.05047644 -0.0666606   0.09385609 -0.0368271   0.00063961\n",
      " -0.07268559 -0.12724088]\n",
      "0.7501418340639319\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "can't multiply sequence by non-int of type 'numpy.float64'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-54-1857884d38d5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcosine_similarity\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpre_query\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdocs_rep\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m \u001b[0mquery_doc_cos_dist\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mcosine_similarity\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquery_rep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdoc_rep\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mdoc_rep\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdocs_rep\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m \u001b[0mquery_doc_sort_index\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margsort\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquery_doc_cos_dist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-54-1857884d38d5>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcosine_similarity\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpre_query\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdocs_rep\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m \u001b[0mquery_doc_cos_dist\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mcosine_similarity\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquery_rep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdoc_rep\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mdoc_rep\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdocs_rep\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m \u001b[0mquery_doc_sort_index\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margsort\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquery_doc_cos_dist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-54-1857884d38d5>\u001b[0m in \u001b[0;36mcosine_similarity\u001b[0;34m(v1, v2)\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0mmagnitude_b\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m         \u001b[0mdot_product\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m         \u001b[0mmagnitude_a\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0ma\u001b[0m \u001b[0;34m**\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0mmagnitude_b\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mb\u001b[0m \u001b[0;34m**\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: can't multiply sequence by non-int of type 'numpy.float64'"
     ]
    }
   ],
   "source": [
    "import math\n",
    "\n",
    "load_tfidf = pickle.load(open(\"tfidf.pickle\",\"rb\"))\n",
    "\n",
    "def lsa_query(query):\n",
    "    query_rep = [vectorizer.vocabulary_[x] for x in pre_process(query).split()]\n",
    "    query_rep = np.mean(terms_rep[query_rep],axis=0)\n",
    "    return query_rep\n",
    "\n",
    "# cosine distance of two vectors\n",
    "def cosine_similarity(v1, v2):\n",
    "    dot_product = 0.0\n",
    "    magnitude_a = 0.0\n",
    "    magnitude_b = 0.0\n",
    "    for a, b in zip(v1, v2):\n",
    "        dot_product += a * b\n",
    "        magnitude_a += a ** 2\n",
    "        magnitude_b += b ** 2\n",
    "    if magnitude_a == 0.0 or magnitude_b == 0.0:\n",
    "        return None\n",
    "    else:\n",
    "        return dot_product / (math.sqrt(magnitude_a * magnitude_b))\n",
    "\n",
    "query_rep = \"Alexander  Alatskivi\"\n",
    "pre_query= lsa_query(query_rep)\n",
    "print(pre_query)\n",
    "print(docs_rep[0])\n",
    "print(cosine_similarity(pre_query,docs_rep[0]))\n",
    "\n",
    "query_doc_cos_dist = [cosine_similarity(query_rep, doc_rep) for doc_rep in docs_rep]\n",
    "query_doc_sort_index = np.argsort(np.array(query_doc_cos_dist))\n",
    "\n",
    "print_count = 0\n",
    "for rank, sort_index in enumerate(query_doc_sort_index):\n",
    "    print ('Rank : ', rank, ' Consine : ', query_doc_cos_dist[sort_index],' Sentence : ', load_processed_corpus_df['sentence_text'][sort_index])\n",
    "    if print_count == 4 :\n",
    "        break\n",
    "    else:\n",
    "        print_count += 1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use page identifier to reduce the search space.\n",
    "\n",
    "Build a dictionary that consists the <b>page identifier</b> as the key and the document index(ranges form 0 to 25248397 ） as value.\n",
    "\n",
    "Examples looks like: \"Alexander McNair\" : [1,2,3,4...18] \n",
    "\n",
    "<b>To do:</b> the keywords list may also consider synonyms for each keyword.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-09T08:38:19.362035Z",
     "start_time": "2019-05-09T08:28:46.009296Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of keys = 5396106\n"
     ]
    }
   ],
   "source": [
    "final_dic = {}\n",
    "\n",
    "for id, doc in enumerate(corpus):\n",
    "    final_dic.setdefault((doc.split()[0].replace(\"_\", \" \")), []).append(id)\n",
    "\n",
    "keys = list(final_dic.keys())\n",
    "print(\"Length of keys = {}\".format(len(keys)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Build a dictionary for all the unique words in the keys.\n",
    "\n",
    "Keywords is a dictionary that stores all the unique page identifier word and its corresponing document index list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-09T11:06:55.203668Z",
     "start_time": "2019-05-09T11:06:55.197695Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Alexander McNair', 'Alatskivi', 'An American Girl Story - Maryellen 1955-COLON- Extraordinary Christmas', 'Alta Outcome Document', 'Al ‘Urban', 'Albano buoy system', 'Amuka', 'Aleksandr Abdulkhalikov', 'Amalia Ciardi Duprè', 'Akbil', 'All These Years', 'Ambulance services of Victoria', 'Alabama elections, 2018', 'Akarsu, Ardanuç', 'Albertinovac', 'All-India Yadav Mahasabha', 'Alejandro Rodríguez López', 'Aleksandr Luzin', 'American Thighs', 'Alan Brown -LRB-Australian politician-RRB-']\n",
      "Alexander\n",
      "McNair\n",
      "Alatskivi\n",
      "An\n",
      "American\n",
      "Girl\n",
      "Story\n",
      "-\n",
      "Maryellen\n",
      "1955-COLON-\n",
      "Extraordinary\n",
      "Christmas\n",
      "Alta\n",
      "Outcome\n",
      "Document\n",
      "Al\n",
      "‘Urban\n",
      "Albano\n",
      "buoy\n",
      "system\n",
      "Amuka\n",
      "Aleksandr\n",
      "Abdulkhalikov\n",
      "Amalia\n",
      "Ciardi\n",
      "Duprè\n",
      "Akbil\n",
      "All\n",
      "These\n",
      "Years\n",
      "Ambulance\n",
      "services\n",
      "of\n",
      "Victoria\n",
      "Alabama\n",
      "elections,\n",
      "2018\n",
      "Akarsu,\n",
      "Ardanuç\n",
      "Albertinovac\n",
      "All-India\n",
      "Yadav\n",
      "Mahasabha\n",
      "Alejandro\n",
      "Rodríguez\n",
      "López\n",
      "Aleksandr\n",
      "Luzin\n",
      "American\n",
      "Thighs\n",
      "Alan\n",
      "Brown\n",
      "-LRB-Australian\n",
      "politician-RRB-\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def create_keywords(keys, final_dic):\n",
    "    keywords = {}\n",
    "    for key in keys:\n",
    "        words = key.split()\n",
    "        for word in words:\n",
    "            print(word)\n",
    "            keywords[word] = final_dic[key]\n",
    "    return keywords\n",
    "\n",
    "# keywords should then remove stop words\n",
    "print(keys[:20])\n",
    "keywords = create_keywords(keys[:20], final_dic)\n",
    "#print(\"Length of keywords = {}\".format(len(keywords)))\n",
    "#print(keywords)\n",
    "#print('\\n')\n",
    "#print(keywords.get('Alexander'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-07T13:09:03.477935Z",
     "start_time": "2019-05-07T13:09:01.436343Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub data rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_data_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "def calculate_cosines(claim_tfidf, evi_tfidf) -> np.ndarray:\n",
    "    cosines = np.zeros((claim_tfidf.shape[0], 1))\n",
    "    for i in range(len(cosines)):\n",
    "        claim_vector = claim_tfidf[i]\n",
    "        evi_vector = evi_tfidf[i]\n",
    "        cosine_matrix = cosine_similarity([claim_vector.toarray()[0], evi_vector.toarray()[0]])\n",
    "        cosines[i][0] = cosine_matrix[0][1]\n",
    "    return cosines\n",
    "\n",
    "print(keywords.get('Alexander'))\n",
    "# testQuery = \"Alexander Alatskivi\"\n",
    "# query_tfidf = tfidf_vectorizer.transform(testQuery)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Retrieval Evidence\n",
    "Given a query, tokenize it first, then for each token in the query, find it in the keywords dictionary. \n",
    "\n",
    "\n",
    "To do: smater select the in the range of its alphabet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-07T13:06:29.017162Z",
     "start_time": "2019-05-07T13:06:13.469277Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub data rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_data_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Given a query, returns a list that contains all the document index values.\n",
    "def extract_sentences(query, keys, final_dic):\n",
    "    retrievaled_sentences = []\n",
    "    for word in  nltk.tokenize.word_tokenize(query):\n",
    "        retrievaled_sentences.append([keywords.get(word)])\n",
    "            \n",
    "    retrievaled_sentences = [item for sublist in retrievaled_sentences for item in sublist]\n",
    "    return retrievaled_sentences\n",
    "\n",
    "testQuery = \"Alexander Alatskivi\"\n",
    "print(extract_sentences(testQuery,keys[:100],final_dic))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If there is a match or substring match, then retrieve that sentence by using index value to find the raw sentence in txt file. \n",
    "\n",
    "Find all the related sentences by this way and then builds a inverted index and \n",
    "uses BM25 to rank and to retrieval top K sentences as the evidence.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-06T09:16:35.130407Z",
     "start_time": "2019-05-06T09:16:35.123213Z"
    }
   },
   "outputs": [],
   "source": [
    "# Use the result from above step, find all the raw sentences in txt file.\n",
    "# return a list of the doc id orderd by rnaking tfidf or bm25 score.\n",
    "\n",
    "def retrieval_evidence(query, keys, final_dic):\n",
    "    processed_doc = [] # processed_docs stores the list of processed docs\n",
    "    vocab = {}\n",
    "    unique_id = 0\n",
    "    rank_result = []\n",
    "    \n",
    "    retrievaled_sentences = extract_sentences(query,keys,final_dic)\n",
    "    \n",
    "    # find the row sentences and save them in processed_doc\n",
    "    for retrievaled_sentence in retrievaled_sentences:\n",
    "        norm_sentence = preprocess(document[retrievaled_sentence])\n",
    "        for token in norm_sentence:\n",
    "            if token not in vocab:\n",
    "                vocab.update({token: unique_id})     \n",
    "                unique_id = unique_id + 1\n",
    "        processed_doc.append(norm_sentence) \n",
    "    \n",
    "    # calculate doc term freqs and build an inverted index\n",
    "    doc_term_freqs = doc_term_freq(processed_doc)\n",
    "    invindex = InvertedIndex(vocab, doc_term_freqs)\n",
    "    \n",
    "    processed_query = preprocess(query)\n",
    "    bm25_results = BM25(processed_query, invindex, vocab)\n",
    "    tfidf_results = tfidf(processed_query, invindex, vocab)\n",
    "    \n",
    "    for rank, res in enumerate(tfidf_results):\n",
    "        # print(\"RANK {:2d} DOCID {:8d} SCORE {:.3f} CONTENT {:}\".format(rank+1,res[0],res[1],document[res[0]]))\n",
    "        rank_result.append((res[0]))\n",
    "    return rank_result\n",
    "\n",
    "# test_query = \"Nikolaj Coster-Waldau worked with the Fox Broadcasting Company.\"\n",
    "# test_query = \"Alexander Alatskivi\"\n",
    "# print(retrieval_evidence(test_query,keys[:100],final_dic))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Write the result to json file.\n",
    "\n",
    "The dataset used is 'devset.json', the predicted result is 'devset_result.json'.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-06T09:20:54.413365Z",
     "start_time": "2019-05-06T09:20:10.863991Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_evidence(res_data):\n",
    "    for key in list(res_data)[:10]:\n",
    "        res_data[key][\"evidence\"] = []\n",
    "        tfidf_result = retrieval_evidence(res_data[key][\"claim\"],keys[:5000],final_dic)\n",
    "        for res in tfidf_result:\n",
    "            res_data[key][\"evidence\"].append([document[res].split()[0], document[res].split()[1]])\n",
    "    return res_data\n",
    "\n",
    "#testing, for top 10 instances in the dev, and only consider top 100 sentences in the documents.\n",
    "predicted_train = get_evidence(res_data)\n",
    "\n",
    "#for key in list(predicted_train)[:10]:\n",
    " #   print(predicted_train[key])\n",
    "  #  print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gensim library.\n",
    "\n",
    "Use gensim library to calculate the coscine similarity in IR."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-07T07:53:52.493929Z",
     "start_time": "2019-05-07T07:53:52.468714Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['i', \"'m\", 'taking', 'the', 'show', 'on', 'the', 'road', '.'], ['my', 'socks', 'are', 'a', 'force', 'multiplier', '.'], ['i', 'am', 'the', 'barber', 'who', 'cuts', 'everyone', \"'s\", 'hair', 'who', 'does', \"n't\", 'cut', 'their', 'own', '.'], ['legend', 'has', 'it', 'that', 'the', 'mind', 'is', 'a', 'mad', 'monkey', '.'], ['i', 'make', 'my', 'own', 'fun', '.']]\n",
      "Dictionary(36 unique tokens: [\"'m\", '.', 'i', 'on', 'road']...)\n",
      "13\n",
      "Number of words in dictionary: 36\n",
      "0 'm\n",
      "1 .\n",
      "2 i\n",
      "3 on\n",
      "4 road\n",
      "5 show\n",
      "6 taking\n",
      "7 the\n",
      "8 a\n",
      "9 are\n",
      "10 force\n",
      "11 multiplier\n",
      "12 my\n",
      "13 socks\n",
      "14 's\n",
      "15 am\n",
      "16 barber\n",
      "17 cut\n",
      "18 cuts\n",
      "19 does\n",
      "20 everyone\n",
      "21 hair\n",
      "22 n't\n",
      "23 own\n",
      "24 their\n",
      "25 who\n",
      "26 has\n",
      "27 is\n",
      "28 it\n",
      "29 legend\n",
      "30 mad\n",
      "31 mind\n",
      "32 monkey\n",
      "33 that\n",
      "34 fun\n",
      "35 make\n",
      "[[(0, 1), (1, 1), (2, 1), (3, 1), (4, 1), (5, 1), (6, 1), (7, 2)], [(1, 1), (8, 1), (9, 1), (10, 1), (11, 1), (12, 1), (13, 1)], [(1, 1), (2, 1), (7, 1), (14, 1), (15, 1), (16, 1), (17, 1), (18, 1), (19, 1), (20, 1), (21, 1), (22, 1), (23, 1), (24, 1), (25, 2)], [(1, 1), (7, 1), (8, 1), (26, 1), (27, 1), (28, 1), (29, 1), (30, 1), (31, 1), (32, 1), (33, 1)], [(1, 1), (2, 1), (12, 1), (23, 1), (34, 1), (35, 1)]]\n",
      "TfidfModel(num_docs=5, num_nnz=47)\n"
     ]
    }
   ],
   "source": [
    "import gensim\n",
    "\n",
    "raw_documents = [\"I'm taking the show on the road.\",\n",
    "                 \"My socks are a force multiplier.\",\n",
    "                 \"I am the barber who cuts everyone's hair who doesn't cut their own.\",\n",
    "                 \"Legend has it that the mind is a mad monkey.\",\n",
    "                 \"I make my own fun.\"]\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "gen_docs = [[w.lower() for w in word_tokenize(text)] for text in raw_documents]\n",
    "\n",
    "dictionary = gensim.corpora.Dictionary(gen_docs)\n",
    "print(gen_docs)\n",
    "print(dictionary)\n",
    "print(dictionary.token2id['socks'])\n",
    "print(\"Number of words in dictionary:\",len(dictionary))\n",
    "for i in range(len(dictionary)):\n",
    "    print(i, dictionary[i])\n",
    "\n",
    "corpus = [dictionary.doc2bow(gen_doc) for gen_doc in gen_docs]\n",
    "print(corpus)\n",
    "\n",
    "tf_idf = gensim.models.TfidfModel(corpus)\n",
    "print(tf_idf)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
