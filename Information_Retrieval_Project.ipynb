{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Information Retrieval\n",
    "\n",
    "This is the document retrieval and sentence retrieval part of the project."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  1. Unpack the zip file.\n",
    "    \n",
    "Unpack the 'wiki-pages-text.zip' in the current directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-09T16:18:13.994080Z",
     "start_time": "2019-05-09T16:18:13.991649Z"
    }
   },
   "outputs": [],
   "source": [
    "import zipfile\n",
    "def unpack():\n",
    "    with zipfile.ZipFile('wiki-pages-text.zip') as file:\n",
    "        file.extractall()\n",
    "# unpack()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Load file. \n",
    "\n",
    "Load the training dataset and the wiki txt file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-15T06:26:37.020411Z",
     "start_time": "2019-05-15T06:25:59.271525Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of the train data is: 145449\n",
      "Length of the dev data is: 5001\n",
      "Length of the dev result data is: 5001\n",
      "Length of the test data is: 14997\n",
      "['wiki-009.txt', 'wiki-021.txt', 'wiki-035.txt', 'wiki-034.txt', 'wiki-020.txt', 'wiki-008.txt', 'wiki-036.txt', 'wiki-022.txt', 'wiki-023.txt', 'wiki-037.txt', 'wiki-033.txt', 'wiki-027.txt', 'wiki-026.txt', 'wiki-032.txt', 'wiki-024.txt', 'wiki-030.txt', 'wiki-018.txt', 'wiki-019.txt', 'wiki-031.txt', 'wiki-025.txt', 'wiki-042.txt', 'wiki-056.txt', 'wiki-081.txt', 'wiki-095.txt', 'wiki-094.txt', 'wiki-080.txt', 'wiki-057.txt', 'wiki-043.txt', 'wiki-069.txt', 'wiki-055.txt', 'wiki-041.txt', 'wiki-096.txt', 'wiki-082.txt', 'wiki-109.txt', 'wiki-108.txt', 'wiki-083.txt', 'wiki-097.txt', 'wiki-040.txt', 'wiki-054.txt', 'wiki-068.txt', 'wiki-050.txt', 'wiki-044.txt', 'wiki-078.txt', 'wiki-093.txt', 'wiki-087.txt', 'wiki-086.txt', 'wiki-092.txt', 'wiki-079.txt', 'wiki-045.txt', 'wiki-051.txt', 'wiki-047.txt', 'wiki-053.txt', 'wiki-084.txt', 'wiki-090.txt', 'wiki-091.txt', 'wiki-085.txt', 'wiki-052.txt', 'wiki-046.txt', 'wiki-063.txt', 'wiki-077.txt', 'wiki-088.txt', 'wiki-103.txt', 'wiki-102.txt', 'wiki-089.txt', 'wiki-076.txt', 'wiki-062.txt', 'wiki-048.txt', 'wiki-074.txt', 'wiki-060.txt', 'wiki-100.txt', 'wiki-101.txt', 'wiki-061.txt', 'wiki-075.txt', 'wiki-049.txt', 'wiki-071.txt', 'wiki-065.txt', 'wiki-059.txt', 'wiki-105.txt', 'wiki-104.txt', 'wiki-058.txt', 'wiki-064.txt', 'wiki-070.txt', 'wiki-066.txt', 'wiki-072.txt', 'wiki-099.txt', 'wiki-106.txt', 'wiki-107.txt', 'wiki-098.txt', 'wiki-073.txt', 'wiki-067.txt', 'wiki-028.txt', 'wiki-014.txt', 'wiki-015.txt', 'wiki-001.txt', 'wiki-029.txt', 'wiki-017.txt', 'wiki-003.txt', 'wiki-002.txt', 'wiki-016.txt', 'wiki-012.txt', 'wiki-006.txt', 'wiki-007.txt', 'wiki-013.txt', 'wiki-005.txt', 'wiki-011.txt', 'wiki-039.txt', 'wiki-038.txt', 'wiki-010.txt', 'wiki-004.txt']\n",
      "Length of the corpus is: 25248397\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "with open('train.json', 'r') as f:  # load training dataset\n",
    "        train_data = json.load(f)   \n",
    "print(\"Length of the train data is: \" + str(len(train_data)))\n",
    "\n",
    "with open('devset.json', 'r') as f1:  # load dev dataset\n",
    "        dev_data = json.load(f1) \n",
    "print(\"Length of the dev data is: \" + str(len(dev_data)))\n",
    "\n",
    "with open('devset_result.json', 'r') as f2:  # store result \n",
    "        res_data = json.load(f2) \n",
    "print(\"Length of the dev result data is: \" + str(len(res_data)))\n",
    "\n",
    "with open('test-unlabelled.json', 'r') as f3:  # store result \n",
    "     test_data = json.load(f3) \n",
    "print(\"Length of the test data is: \" + str(len(test_data)))\n",
    "        \n",
    "# appeand all the wiki txt sentences to one document\n",
    "def loadfile(folder): \n",
    "    corpus = []\n",
    "    list_of_files = os.listdir(folder)\n",
    "    print(list_of_files)\n",
    "    \n",
    "    for file in list_of_files:\n",
    "        try:\n",
    "            filename = os.path.join(folder, file)\n",
    "            with open(filename, 'r') as doc:\n",
    "                for line in doc:\n",
    "                    corpus.append(line)     \n",
    "        except Exception as e:\n",
    "            print(\"No files found here!\")\n",
    "            raise e\n",
    "    return corpus\n",
    "corpus = loadfile(\"wiki-pages-text\")\n",
    "print(\"Length of the corpus is: \" + str(len(corpus)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pandas process the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-15T06:44:21.036789Z",
     "start_time": "2019-05-15T06:44:10.030352Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50, 1)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Alexander_McNair 0 Alexander McNair -LRB- May ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Alexander_McNair 1 He was the first Governor o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Alexander_McNair 4 McNair was born in Lancaste...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Alexander_McNair 5 His grandfather , David McN...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Alexander_McNair 6 David McNair , Jr. , Alexan...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Alexander_McNair 7 Alexander went to school as...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Alexander_McNair 8 He reached an agreement wit...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Alexander_McNair 9 Alexander was defeated .\\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Alexander_McNair 10 He became a member of the ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Alexander_McNair 13 In 1804 , McNair traveled ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text\n",
       "0  Alexander_McNair 0 Alexander McNair -LRB- May ...\n",
       "1  Alexander_McNair 1 He was the first Governor o...\n",
       "2  Alexander_McNair 4 McNair was born in Lancaste...\n",
       "3  Alexander_McNair 5 His grandfather , David McN...\n",
       "4  Alexander_McNair 6 David McNair , Jr. , Alexan...\n",
       "5  Alexander_McNair 7 Alexander went to school as...\n",
       "6  Alexander_McNair 8 He reached an agreement wit...\n",
       "7      Alexander_McNair 9 Alexander was defeated .\\n\n",
       "8  Alexander_McNair 10 He became a member of the ...\n",
       "9  Alexander_McNair 13 In 1804 , McNair traveled ..."
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus_df = pd.DataFrame(corpus[:50])\n",
    "corpus_df.columns = ['text']\n",
    "\n",
    "\"\"\"\n",
    "corpus_df['page_identifier'] = corpus_df.text.apply(lambda x: x.split(' ')[0])  \n",
    "corpus_df['sentence_number'] = corpus_df.text.apply(lambda x: x.split(' ')[1]) \n",
    "corpus_df['sentence_text'] = corpus_df.text.apply(lambda x: x.split(' ')[2:])  \n",
    "corpus_df['sentence_text'] = [','.join(map(str, l)) for l in corpus_df['sentence_text']]\n",
    "corpus_df[\"sentence_text\"] = corpus_df['sentence_text'].str.replace(',',' ')\n",
    "corpus_df = corpus_df.drop('text', 1)\n",
    "\"\"\"\n",
    "\n",
    "print(corpus_df.shape)\n",
    "corpus_df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Preprocess \n",
    "\n",
    "Preprocess includes: strip punctuations, tokenize,lemma, lower case, remove stop words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-15T09:32:25.199905Z",
     "start_time": "2019-05-15T09:32:25.193775Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/zhangyiming/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "import re\n",
    "\n",
    "tokenizer = nltk.tokenize.RegexpTokenizer(r'\\w+')\n",
    "lemmatizer = nltk.stem.WordNetLemmatizer()\n",
    "\n",
    "nltk.download('wordnet')\n",
    "def lemmatize(word):\n",
    "    lemma = lemmatizer.lemmatize(word,'v')\n",
    "    if lemma == word:\n",
    "        lemma = lemmatizer.lemmatize(word,'n')\n",
    "    return lemma\n",
    "\n",
    "def pre_process(comment):\n",
    "    processed_comment = []\n",
    "    # strip punctuations\n",
    "    comment = re.sub(r'[^\\w\\s]', '', comment)\n",
    "    # tokenize\n",
    "    words = tokenizer.tokenize(comment)\n",
    "    # lemmatize \n",
    "    words = [lemmatize(w) for w in words]\n",
    "    for word in words:\n",
    "        word = word.lower()\n",
    "        processed_comment.append(word)\n",
    "    return processed_comment\n",
    "\n",
    "def process_dataset(dataset: pd.DataFrame) -> pd.DataFrame:\n",
    "    corpus = dataset['text']\n",
    "    processed_corpus = corpus.apply(lambda text: pre_process(text))\n",
    "    #dataset.text = dataset.text.str.replace('_', ' ')\n",
    "    dataset['text'] = processed_corpus.iloc[0: len(dataset)]\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Save the pre-processed corpus dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-15T09:32:28.414708Z",
     "start_time": "2019-05-15T09:32:28.387299Z"
    }
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "expected string or bytes-like object",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-dbf5d7870eb3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# import datetime\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m# start = datetime.datetime.now()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mprocessed_corpus_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprocess_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcorpus_df\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mprocessed_corpus_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_pickle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"./processed_corpus.pkl\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mprocessed_corpus_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-9-359734c98dba>\u001b[0m in \u001b[0;36mprocess_dataset\u001b[0;34m(dataset)\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mprocess_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m     \u001b[0mcorpus\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'text'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m     \u001b[0mprocessed_corpus\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcorpus\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mpre_process\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m     \u001b[0;31m#dataset.text = dataset.text.str.replace('_', ' ')\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m     \u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'text'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprocessed_corpus\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.7/site-packages/pandas/core/series.py\u001b[0m in \u001b[0;36mapply\u001b[0;34m(self, func, convert_dtype, args, **kwds)\u001b[0m\n\u001b[1;32m   3192\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3193\u001b[0m                 \u001b[0mvalues\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobject\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3194\u001b[0;31m                 \u001b[0mmapped\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap_infer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconvert\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconvert_dtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3195\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3196\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmapped\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmapped\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSeries\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/src/inference.pyx\u001b[0m in \u001b[0;36mpandas._libs.lib.map_infer\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m<ipython-input-9-359734c98dba>\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(text)\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mprocess_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m     \u001b[0mcorpus\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'text'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m     \u001b[0mprocessed_corpus\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcorpus\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mpre_process\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m     \u001b[0;31m#dataset.text = dataset.text.str.replace('_', ' ')\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m     \u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'text'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprocessed_corpus\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-9-359734c98dba>\u001b[0m in \u001b[0;36mpre_process\u001b[0;34m(comment)\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0mprocessed_comment\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0;31m# strip punctuations\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m     \u001b[0mcomment\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mre\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msub\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mr'[^\\w\\s]'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcomment\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m     \u001b[0;31m# tokenize\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0mwords\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcomment\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/lib/python3.7/re.py\u001b[0m in \u001b[0;36msub\u001b[0;34m(pattern, repl, string, count, flags)\u001b[0m\n\u001b[1;32m    190\u001b[0m     \u001b[0ma\u001b[0m \u001b[0mcallable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mit\u001b[0m\u001b[0;31m'\u001b[0m\u001b[0ms\u001b[0m \u001b[0mpassed\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mMatch\u001b[0m \u001b[0mobject\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mmust\u001b[0m \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    191\u001b[0m     a replacement string to be used.\"\"\"\n\u001b[0;32m--> 192\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_compile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpattern\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflags\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msub\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrepl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstring\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcount\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    193\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    194\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0msubn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpattern\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrepl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstring\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflags\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: expected string or bytes-like object"
     ]
    }
   ],
   "source": [
    "# import datetime\n",
    "# start = datetime.datetime.now()\n",
    "processed_corpus_df = pd.DataFrame(process_dataset(corpus_df))\n",
    "processed_corpus_df.to_pickle(\"./processed_corpus.pkl\")\n",
    "processed_corpus_df.head(10)\n",
    "# end = datetime.datetime.now()\n",
    "# print(end-start)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load pre-processed corpus dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-15T09:31:27.637279Z",
     "start_time": "2019-05-15T09:31:27.615166Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50, 1)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[alexander_mcnair, 0, alexander, mcnair, lrb, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[alexander_mcnair, 1, he, be, the, first, gove...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[alexander_mcnair, 4, mcnair, be, bear, in, la...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[alexander_mcnair, 5, his, grandfather, david,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[alexander_mcnair, 6, david, mcnair, jr, alexa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>[alexander_mcnair, 7, alexander, go, to, schoo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>[alexander_mcnair, 8, he, reach, an, agreement...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>[alexander_mcnair, 9, alexander, be, defeat]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>[alexander_mcnair, 10, he, become, a, member, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>[alexander_mcnair, 13, in, 1804, mcnair, trave...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text\n",
       "0  [alexander_mcnair, 0, alexander, mcnair, lrb, ...\n",
       "1  [alexander_mcnair, 1, he, be, the, first, gove...\n",
       "2  [alexander_mcnair, 4, mcnair, be, bear, in, la...\n",
       "3  [alexander_mcnair, 5, his, grandfather, david,...\n",
       "4  [alexander_mcnair, 6, david, mcnair, jr, alexa...\n",
       "5  [alexander_mcnair, 7, alexander, go, to, schoo...\n",
       "6  [alexander_mcnair, 8, he, reach, an, agreement...\n",
       "7       [alexander_mcnair, 9, alexander, be, defeat]\n",
       "8  [alexander_mcnair, 10, he, become, a, member, ...\n",
       "9  [alexander_mcnair, 13, in, 1804, mcnair, trave..."
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_processed_corpus_df = pd.read_pickle(\"./processed_corpus.pkl\")\n",
    "print(load_processed_corpus_df.shape)\n",
    "load_processed_corpus_df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Try inverted index, doc term freqs and BM25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-12T10:59:39.425794Z",
     "start_time": "2019-05-12T10:59:39.417055Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of doc term freqs = 50\n",
      "\n",
      "\n",
      "Counter({'alexander_mcnair': 1, '0': 1, 'alexander': 1, 'mcnair': 1, 'lrb': 1, 'may': 1, '5': 1, '1775': 1, 'march': 1, '18': 1, '1826': 1, 'rrb': 1, 'be': 1, 'an': 1, 'american': 1, 'frontiersman': 1, 'and': 1, 'politician': 1})\n",
      "1\n",
      "Counter({'a': 2, 'alexander_mcnair': 1, '1': 1, 'he': 1, 'be': 1, 'the': 1, 'first': 1, 'governor': 1, 'of': 1, 'missouri': 1, 'from': 1, 'it': 1, 'entry': 1, 'state': 1, 'in': 1, '1820': 1, 'until': 1, '1824': 1})\n",
      "Counter({'in': 3, 'alexander_mcnair': 1, '4': 1, 'mcnair': 1, 'be': 1, 'bear': 1, 'lancaster': 1, 'the': 1, 'province': 1, 'of': 1, 'pennsylvania': 1, 'and': 1, 'grow': 1, 'up': 1, 'mifflin': 1, 'county': 1})\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "def doc_term_freq(df):\n",
    "    doc_term_freqs = []\n",
    "    for index, row in df.iterrows():\n",
    "        doc_term_freqs.append(Counter(row[\"text\"])) \n",
    "\n",
    "    return doc_term_freqs\n",
    "\n",
    "doc_term_freqs = doc_term_freq(load_processed_corpus_df)\n",
    "\n",
    "print(\"Number of doc term freqs = {}\".format(len(doc_term_freqs)))\n",
    "print(\"\\n\")\n",
    "print(doc_term_freqs[0])\n",
    "print(doc_term_freqs[0]['lrb'])\n",
    "print(doc_term_freqs[1])\n",
    "print(doc_term_freqs[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Use Sklearn to build tf-idf.\n",
    "\n",
    "tfidf_vectorizer is the tf-idf model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-10T07:07:48.026650Z",
     "start_time": "2019-05-10T07:07:47.986084Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary Size :  4650\n",
      "Shape of Matrix :  (4650, 1000)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import pickle\n",
    "\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "tfidf = tfidf_vectorizer.fit_transform(load_processed_corpus_df['sentence_text'])\n",
    "tfidf = tfidf.T\n",
    "\n",
    "print('Vocabulary Size : ', len(tfidf_vectorizer.get_feature_names()))\n",
    "print('Shape of Matrix : ', tfidf.shape)\n",
    "\n",
    "pickle.dump(tfidf, open(\"tfidf.pickle\",\"wb\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Apply SVD "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-10T12:25:16.598764Z",
     "start_time": "2019-05-10T12:25:15.491865Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4650, 4650)\n",
      "(1000,)\n",
      "(1000, 1000)\n",
      "(4650, 20)\n",
      "(1000, 20)\n",
      "[-0.07261009 -0.00454805  0.03801582  0.01416727  0.05270882 -0.00769401\n",
      "  0.01417197 -0.04272573 -0.06352088  0.00386057  0.01474623  0.01431363\n",
      "  0.12041152 -0.0422148   0.05369463  0.02702167  0.01134609  0.01666593\n",
      "  0.00552886  0.00071745]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "load_tfidf = pickle.load(open(\"tfidf.pickle\",\"rb\"))\n",
    "# Applying SVD\n",
    "K= 20 # number of desirable features \n",
    "U, s, VT = np.linalg.svd(load_tfidf.toarray())\n",
    "# tfidf_reduced = np.dot(U[:,:K], np.dot(np.diag(s[:K]), VT[:K, :]))\n",
    "\n",
    "print(U.shape)\n",
    "print(s.shape)\n",
    "print(VT.shape)\n",
    "\n",
    "# Getting document and term representation\n",
    "terms_rep = np.dot(U[:,:K], np.diag(s[:K])) # M X K matrix where M = Vocabulary Size and N = Number of documents\n",
    "docs_rep = np.dot(np.diag(s[:K]), VT[:K, :]).T # N x K matrix \n",
    "\n",
    "print(terms_rep.shape)\n",
    "print(docs_rep.shape)\n",
    "print(terms_rep[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PCA , TurncatedSVD, Scipy SVD, SparseSVD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-10T12:25:20.997444Z",
     "start_time": "2019-05-10T12:25:20.881172Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4650, 20)\n",
      "(20,)\n",
      "(20, 1000)\n",
      "(4650, 20)\n",
      "(1000, 20)\n",
      "[ 0.07261009 -0.00454648  0.03795112  0.01444103  0.05371103 -0.00810654\n",
      "  0.01351493  0.04319767 -0.06231174 -0.00734784  0.02139308  0.00312849\n",
      "  0.12001401 -0.02885562  0.0619723  -0.01373013 -0.00822798 -0.00194202\n",
      " -0.05417265 -0.05236209]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.decomposition import PCA \n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sparsesvd import sparsesvd\n",
    "import numpy\n",
    "from scipy.sparse.linalg import svds\n",
    "from sklearn.utils.extmath import randomized_svd\n",
    "\n",
    "# pca = PCA(n_components='mle')\n",
    "# pca_tfidf = pca.fit_transform(load_tfidf)\n",
    "# print(pca_tfidf)\n",
    "\n",
    "k1= 20 # number of desirable features \n",
    "svd = TruncatedSVD(n_components=k1, n_iter=7, random_state=42)\n",
    "svd_tfidf = svd.fit_transform(load_tfidf)\n",
    "# print(svd_tfidf.shape)\n",
    "# smat = scipy.sparse.csc_matrix(load_tfidf.toarray()) # convert to sparse CSC format\n",
    "# U1, s1, VT1 = sparsesvd(smat, k1)\n",
    "# U1 = U1.T\n",
    "\n",
    "U1, s1, VT1 = randomized_svd(load_tfidf.toarray(), k1)\n",
    "\n",
    "print(U1.shape)\n",
    "print(s1.shape)\n",
    "print(VT1.shape)\n",
    "# Getting document and term representation\n",
    "terms_rep1 = np.dot(U1[:,:k1], np.diag(s1[:k1])) # M X K matrix where M = Vocabulary Size and N = Number of documents\n",
    "docs_rep1 = np.dot(np.diag(s1[:k1]), VT1[:k1, :]).T # N x K matrix \n",
    "print(terms_rep1.shape)\n",
    "print(docs_rep1.shape)\n",
    "print(terms_rep1[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  6. Test tfidf with query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-10T12:48:37.815894Z",
     "start_time": "2019-05-10T12:48:37.756076Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.08030084  0.07131535 -0.02114174 -0.01343357  0.00086819 -0.00232813\n",
      " -0.10948688 -0.00657705 -0.03682033 -0.01810879 -0.02005786  0.02662247\n",
      " -0.04509342  0.0063877  -0.03200761  0.03458823  0.02123711  0.05318392\n",
      "  0.02635511 -0.02561251]\n",
      "[-0.13757185  0.19350078 -0.02065905  0.12258203 -0.02870684  0.02541737\n",
      " -0.18322714  0.01045996 -0.11076183  0.03505023 -0.06895175  0.04991295\n",
      " -0.05618872  0.05047644 -0.0666606   0.09385609 -0.0368271   0.00063961\n",
      " -0.07268559 -0.12724088]\n",
      "0.750141834063932\n",
      "1000\n",
      "0.24985816593606802\n",
      "0.9036018061784973\n",
      "Rank :  0  Consine :  0.8558589155525097 Page Identifier:  Alexander_McNair Sentence number:  9  Sentence :  alexander be defeat\n",
      "Rank :  1  Consine :  0.8481061499565726 Page Identifier:  Alexander_Carson_-LRB-filmmaker-RRB- Sentence number:  0  Sentence :  alexander carson be a canadian filmmaker\n",
      "Rank :  2  Consine :  0.8476412327038444 Page Identifier:  Alexander_Frederick,_Landgrave_of_Hesse Sentence number:  0  Sentence :  alexander frederick landgrave of hesse lrb alexander friedrich wilhelm albrecht georg landgraf von hessen 25 january 1863 copenhagen 26 march 1945 rrb be a hesse kassel and prussian prince\n",
      "Rank :  3  Consine :  0.8278227890749886 Page Identifier:  Alexander_Martin_-LRB-sport_shooter-RRB- Sentence number:  0  Sentence :  alexander elsdon martin lrb bear 21 january 1895 date of death unknown rrb be a british sport shooter\n",
      "Rank :  4  Consine :  0.8177751773253434 Page Identifier:  Aleksandr_Krotov Sentence number:  0  Sentence :  aleksandr ivanovich krotov lrb bear 1895 die 1959 rrb be an association football player\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.7/site-packages/scipy/spatial/distance.py:698: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  dist = 1.0 - uv / np.sqrt(uu * vv)\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "from scipy.spatial.distance import cosine\n",
    "\n",
    "def lsa_query(query):\n",
    "    query_rep = []\n",
    "    for q in pre_process(query).split():\n",
    "        if q in tfidf_vectorizer.vocabulary_:\n",
    "            query_rep.append(tfidf_vectorizer.vocabulary_[q])\n",
    "        else:\n",
    "            continue\n",
    "    query_rep = np.mean(terms_rep[query_rep],axis=0)\n",
    "    return query_rep\n",
    "\n",
    "query_rep = \"Alexander Alatskivi\"\n",
    "pre_query= lsa_query(query_rep)\n",
    "print(pre_query)\n",
    "print(docs_rep[0])\n",
    "print(1- cosine(pre_query,docs_rep[0]))\n",
    "\n",
    "query_doc_cos_dist = []\n",
    "\n",
    "for doc_rep in docs_rep:\n",
    "    query_doc_cos_dist.append(cosine(pre_query, doc_rep))\n",
    "\n",
    "print(len(query_doc_cos_dist))\n",
    "print(query_doc_cos_dist[0])\n",
    "print(query_doc_cos_dist[1])\n",
    "\n",
    "query_doc_sort_index = np.argsort(np.array(query_doc_cos_dist))\n",
    "\n",
    "count = 0\n",
    "for rank, sort_index in enumerate(query_doc_sort_index):\n",
    "    print ('Rank : ', rank, ' Consine : ', 1 - query_doc_cos_dist[sort_index], 'Page Identifier: ',load_processed_corpus_df['page_identifier'][sort_index], 'Sentence number: ', load_processed_corpus_df['sentence_number'][sort_index], ' Sentence : ', load_processed_corpus_df['sentence_text'][sort_index])\n",
    "    if count == 4 :\n",
    "        break\n",
    "    else:\n",
    "        count += 1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  7. Retrieval Evidence and write the result to json file.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-10T12:21:49.710148Z",
     "start_time": "2019-05-10T12:21:49.043236Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.7/site-packages/scipy/spatial/distance.py:698: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  dist = 1.0 - uv / np.sqrt(uu * vv)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'claim': 'Colin Kaepernick became a starting quarterback during the 49ers 63rd season in the National Football League.', 'label': 'NOT ENOUGH INFO', 'evidence': [['Bavarian_Film_Awards_-LRB-Production_Design-RRB-', 0], ['Beverly_Lynne', 0], ['Billy_Hogg_-LRB-Scottish_footballer-RRB-', 5], ['Baumgarten_-LRB-surname-RRB-', 0], ['Benedetta_of_Cagliari', 12]]}\n",
      "\n",
      "\n",
      "{'claim': 'Tilda Swinton is a vegan.', 'label': 'NOT ENOUGH INFO', 'evidence': [['Bavarian_Film_Awards_-LRB-Production_Design-RRB-', 0], ['Beverly_Lynne', 0], ['Billy_Hogg_-LRB-Scottish_footballer-RRB-', 5], ['Baumgarten_-LRB-surname-RRB-', 0], ['Benedetta_of_Cagliari', 12]]}\n",
      "\n",
      "\n",
      "{'claim': 'Fox 2000 Pictures released the film Soul Food.', 'label': 'SUPPORTS', 'evidence': [['Bavarian_Film_Awards_-LRB-Production_Design-RRB-', 0], ['Beverly_Lynne', 0], ['Billy_Hogg_-LRB-Scottish_footballer-RRB-', 5], ['Baumgarten_-LRB-surname-RRB-', 0], ['Benedetta_of_Cagliari', 12]]}\n",
      "\n",
      "\n",
      "{'claim': 'Anne Rice was born in New Jersey.', 'label': 'NOT ENOUGH INFO', 'evidence': [['Bavarian_Film_Awards_-LRB-Production_Design-RRB-', 0], ['Beverly_Lynne', 0], ['Billy_Hogg_-LRB-Scottish_footballer-RRB-', 5], ['Baumgarten_-LRB-surname-RRB-', 0], ['Benedetta_of_Cagliari', 12]]}\n",
      "\n",
      "\n",
      "{'claim': 'Telemundo is a English-language television network.', 'label': 'REFUTES', 'evidence': [['Bavarian_Film_Awards_-LRB-Production_Design-RRB-', 0], ['Beverly_Lynne', 0], ['Billy_Hogg_-LRB-Scottish_footballer-RRB-', 5], ['Baumgarten_-LRB-surname-RRB-', 0], ['Benedetta_of_Cagliari', 12]]}\n",
      "\n",
      "\n",
      "{'claim': \"Damon Albarn's debut album was released in 2011.\", 'label': 'REFUTES', 'evidence': [['Bavarian_Film_Awards_-LRB-Production_Design-RRB-', 0], ['Beverly_Lynne', 0], ['Billy_Hogg_-LRB-Scottish_footballer-RRB-', 5], ['Baumgarten_-LRB-surname-RRB-', 0], ['Benedetta_of_Cagliari', 12]]}\n",
      "\n",
      "\n",
      "{'claim': 'There is a capital called Mogadishu.', 'label': 'SUPPORTS', 'evidence': [['Bavarian_Film_Awards_-LRB-Production_Design-RRB-', 0], ['Beverly_Lynne', 0], ['Billy_Hogg_-LRB-Scottish_footballer-RRB-', 5], ['Baumgarten_-LRB-surname-RRB-', 0], ['Benedetta_of_Cagliari', 12]]}\n",
      "\n",
      "\n",
      "{'claim': 'Savages was exclusively a German film.', 'label': 'REFUTES', 'evidence': [['Bavarian_Film_Awards_-LRB-Production_Design-RRB-', 0], ['Beverly_Lynne', 0], ['Billy_Hogg_-LRB-Scottish_footballer-RRB-', 5], ['Baumgarten_-LRB-surname-RRB-', 0], ['Benedetta_of_Cagliari', 12]]}\n",
      "\n",
      "\n",
      "{'claim': 'Happiness in Slavery is a gospel song by Nine Inch Nails.', 'label': 'NOT ENOUGH INFO', 'evidence': [['Bavarian_Film_Awards_-LRB-Production_Design-RRB-', 0], ['Beverly_Lynne', 0], ['Billy_Hogg_-LRB-Scottish_footballer-RRB-', 5], ['Baumgarten_-LRB-surname-RRB-', 0], ['Benedetta_of_Cagliari', 12]]}\n",
      "\n",
      "\n",
      "{'claim': 'Andrew Kevin Walker is only Chinese.', 'label': 'REFUTES', 'evidence': [['Bavarian_Film_Awards_-LRB-Production_Design-RRB-', 0], ['Beverly_Lynne', 0], ['Billy_Hogg_-LRB-Scottish_footballer-RRB-', 5], ['Baumgarten_-LRB-surname-RRB-', 0], ['Benedetta_of_Cagliari', 12]]}\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from scipy.spatial.distance import cosine\n",
    "\n",
    "load_tfidf = pickle.load(open(\"tfidf.pickle\",\"rb\"))\n",
    "\n",
    "def lsa_query(query):\n",
    "    query_rep = []\n",
    "    for q in pre_process(query).split():\n",
    "        if q in tfidf_vectorizer.vocabulary_:\n",
    "            query_rep.append(tfidf_vectorizer.vocabulary_[q])\n",
    "        else:\n",
    "            continue\n",
    "    query_rep = np.mean(terms_rep1[query_rep],axis=0)\n",
    "    return query_rep\n",
    "\n",
    "def retrieval_evidence(res_data):\n",
    "    for key in list(res_data)[:10]:\n",
    "        res_data[key][\"evidence\"] = []\n",
    "        lsa_query(res_data[key][\"claim\"])\n",
    "        \n",
    "        query_doc_cos_dist = []\n",
    "        for doc_rep in docs_rep1:\n",
    "            query_doc_cos_dist.append(cosine(pre_query, doc_rep))\n",
    "        query_doc_sort_index = np.argsort(np.array(query_doc_cos_dist))\n",
    "        \n",
    "        # retrieval top 5 evidence\n",
    "        count = 0\n",
    "        for rank, sort_index in enumerate(query_doc_sort_index):\n",
    "            res_data[key][\"evidence\"].append([load_processed_corpus_df['page_identifier'][sort_index],int(load_processed_corpus_df['sentence_number'][sort_index])])\n",
    "            if count == 4 :\n",
    "                break\n",
    "            else:\n",
    "                count += 1\n",
    "                \n",
    "    return res_data\n",
    "\n",
    "#testing, for top 10 instances in the dev, and only consider top 100 sentences in the documents.\n",
    "predicted_train = retrieval_evidence(res_data)\n",
    "\n",
    "for key in list(predicted_train)[:10]:\n",
    "    print(predicted_train[key])\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8. Another method: Gensim library tfidf model and LSA .\n",
    "\n",
    "Use gensim library to calculate the coscine similarity in IR."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-07T07:53:52.493929Z",
     "start_time": "2019-05-07T07:53:52.468714Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['i', \"'m\", 'taking', 'the', 'show', 'on', 'the', 'road', '.'], ['my', 'socks', 'are', 'a', 'force', 'multiplier', '.'], ['i', 'am', 'the', 'barber', 'who', 'cuts', 'everyone', \"'s\", 'hair', 'who', 'does', \"n't\", 'cut', 'their', 'own', '.'], ['legend', 'has', 'it', 'that', 'the', 'mind', 'is', 'a', 'mad', 'monkey', '.'], ['i', 'make', 'my', 'own', 'fun', '.']]\n",
      "Dictionary(36 unique tokens: [\"'m\", '.', 'i', 'on', 'road']...)\n",
      "13\n",
      "Number of words in dictionary: 36\n",
      "0 'm\n",
      "1 .\n",
      "2 i\n",
      "3 on\n",
      "4 road\n",
      "5 show\n",
      "6 taking\n",
      "7 the\n",
      "8 a\n",
      "9 are\n",
      "10 force\n",
      "11 multiplier\n",
      "12 my\n",
      "13 socks\n",
      "14 's\n",
      "15 am\n",
      "16 barber\n",
      "17 cut\n",
      "18 cuts\n",
      "19 does\n",
      "20 everyone\n",
      "21 hair\n",
      "22 n't\n",
      "23 own\n",
      "24 their\n",
      "25 who\n",
      "26 has\n",
      "27 is\n",
      "28 it\n",
      "29 legend\n",
      "30 mad\n",
      "31 mind\n",
      "32 monkey\n",
      "33 that\n",
      "34 fun\n",
      "35 make\n",
      "[[(0, 1), (1, 1), (2, 1), (3, 1), (4, 1), (5, 1), (6, 1), (7, 2)], [(1, 1), (8, 1), (9, 1), (10, 1), (11, 1), (12, 1), (13, 1)], [(1, 1), (2, 1), (7, 1), (14, 1), (15, 1), (16, 1), (17, 1), (18, 1), (19, 1), (20, 1), (21, 1), (22, 1), (23, 1), (24, 1), (25, 2)], [(1, 1), (7, 1), (8, 1), (26, 1), (27, 1), (28, 1), (29, 1), (30, 1), (31, 1), (32, 1), (33, 1)], [(1, 1), (2, 1), (12, 1), (23, 1), (34, 1), (35, 1)]]\n",
      "TfidfModel(num_docs=5, num_nnz=47)\n"
     ]
    }
   ],
   "source": [
    "import gensim\n",
    "\n",
    "raw_documents = [\"I'm taking the show on the road.\",\n",
    "                 \"My socks are a force multiplier.\",\n",
    "                 \"I am the barber who cuts everyone's hair who doesn't cut their own.\",\n",
    "                 \"Legend has it that the mind is a mad monkey.\",\n",
    "                 \"I make my own fun.\"]\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "gen_docs = [[w.lower() for w in word_tokenize(text)] for text in raw_documents]\n",
    "\n",
    "dictionary = gensim.corpora.Dictionary(gen_docs)\n",
    "print(gen_docs)\n",
    "print(dictionary)\n",
    "print(dictionary.token2id['socks'])\n",
    "print(\"Number of words in dictionary:\",len(dictionary))\n",
    "for i in range(len(dictionary)):\n",
    "    print(i, dictionary[i])\n",
    "\n",
    "corpus = [dictionary.doc2bow(gen_doc) for gen_doc in gen_docs]\n",
    "print(corpus)\n",
    "\n",
    "tf_idf = gensim.models.TfidfModel(corpus)\n",
    "print(tf_idf)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-10T14:33:24.262182Z",
     "start_time": "2019-05-10T14:33:24.251818Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1000, 3)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>page_identifier</th>\n",
       "      <th>sentence_number</th>\n",
       "      <th>sentence_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Alexander_McNair</td>\n",
       "      <td>0</td>\n",
       "      <td>alexander mcnair lrb may 5 1775 march 18 1826 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Alexander_McNair</td>\n",
       "      <td>1</td>\n",
       "      <td>he be the first governor of missouri from it e...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Alexander_McNair</td>\n",
       "      <td>4</td>\n",
       "      <td>mcnair be bear in lancaster in the province of...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Alexander_McNair</td>\n",
       "      <td>5</td>\n",
       "      <td>his grandfather david mcnair sr immigrate to p...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Alexander_McNair</td>\n",
       "      <td>6</td>\n",
       "      <td>david mcnair jr alexander s father lrb b 1736 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Alexander_McNair</td>\n",
       "      <td>7</td>\n",
       "      <td>alexander go to school a a child and attend on...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Alexander_McNair</td>\n",
       "      <td>8</td>\n",
       "      <td>he reach an agreement with his mother and brot...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Alexander_McNair</td>\n",
       "      <td>9</td>\n",
       "      <td>alexander be defeat</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Alexander_McNair</td>\n",
       "      <td>10</td>\n",
       "      <td>he become a member of the pennsylvania militia...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Alexander_McNair</td>\n",
       "      <td>13</td>\n",
       "      <td>in 1804 mcnair travel to what be now missouri ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    page_identifier sentence_number  \\\n",
       "0  Alexander_McNair               0   \n",
       "1  Alexander_McNair               1   \n",
       "2  Alexander_McNair               4   \n",
       "3  Alexander_McNair               5   \n",
       "4  Alexander_McNair               6   \n",
       "5  Alexander_McNair               7   \n",
       "6  Alexander_McNair               8   \n",
       "7  Alexander_McNair               9   \n",
       "8  Alexander_McNair              10   \n",
       "9  Alexander_McNair              13   \n",
       "\n",
       "                                       sentence_text  \n",
       "0  alexander mcnair lrb may 5 1775 march 18 1826 ...  \n",
       "1  he be the first governor of missouri from it e...  \n",
       "2  mcnair be bear in lancaster in the province of...  \n",
       "3  his grandfather david mcnair sr immigrate to p...  \n",
       "4  david mcnair jr alexander s father lrb b 1736 ...  \n",
       "5  alexander go to school a a child and attend on...  \n",
       "6  he reach an agreement with his mother and brot...  \n",
       "7                                alexander be defeat  \n",
       "8  he become a member of the pennsylvania militia...  \n",
       "9  in 1804 mcnair travel to what be now missouri ...  "
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(load_processed_corpus_df.shape)\n",
    "processed_corpus_df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Whoosh Library.\n",
    "\n",
    "#### Build Schema."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-12T09:48:58.342532Z",
     "start_time": "2019-05-12T09:48:58.259399Z"
    }
   },
   "outputs": [],
   "source": [
    "from whoosh.qparser import *\n",
    "from whoosh.fields import Schema, TEXT, KEYWORD, ID, STORED,NUMERIC\n",
    "from whoosh.analysis import StemmingAnalyzer,StandardAnalyzer\n",
    "\n",
    "def init_search():\n",
    "    schema = Schema(page_identifier=TEXT(stored=True),\n",
    "                    sentence_number=NUMERIC(stored=True),\n",
    "                    sentence_text=TEXT(stored=True),\n",
    "                   )\n",
    "    return schema\n",
    "    \n",
    "schema = init_search()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Index document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-12T09:49:00.763451Z",
     "start_time": "2019-05-12T09:49:00.692590Z"
    }
   },
   "outputs": [],
   "source": [
    "from whoosh import index\n",
    "import os, os.path\n",
    "\n",
    "def create_index(schema):\n",
    "    #to create an index in a dictionary\n",
    "    if not os.path.exists(\"indexdir\"):\n",
    "        os.mkdir(\"indexdir\")\n",
    "    ix = index.create_in(\"indexdir\", schema)\n",
    "    #open an existing index object\n",
    "    ix = index.open_dir(\"indexdir\")\n",
    "    return ix\n",
    "ix = create_index(schema)\n",
    "\n",
    "def write_index(index):\n",
    "    #create a writer object to add documents to the index\n",
    "    writer = ix.writer()\n",
    "    \n",
    "    writer.add_document(page_identifier=u\"Alexander_McNair\",\n",
    "                sentence_number=u\"0\",\n",
    "                sentence_text=u\"Alexander McNair -LRB- May 5 , 1775 -- March 18 , 1826 -RRB- was an American frontiersman and politician.\")\n",
    "    \n",
    "    writer.add_document(page_identifier=u\"Alexander_McNair\",\n",
    "                sentence_number=u\"1\",\n",
    "                sentence_text=u\"Alexander He was the first Governor of Missouri from its entry as a state in 1820 , until 1824 .\")\n",
    "    writer.commit()\n",
    "       \n",
    "write_index(ix)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Parse the query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-12T10:24:38.059211Z",
     "start_time": "2019-05-12T10:24:38.055063Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sentence_text:alexander\n"
     ]
    }
   ],
   "source": [
    "#parsing the query, simple parser with default field\n",
    "parser=QueryParser(\"sentence_text\",schema=schema) \n",
    "# parser1=QueryParser(u\"page_identifier\",schema=schema) \n",
    "result=parser.parse(u\"Alexander\")\n",
    "result1=parser1.parse(u\"alexander_mcnair\")\n",
    "print(result)\n",
    "# print(result1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Searcher"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-12T10:24:55.981132Z",
     "start_time": "2019-05-12T10:24:55.954196Z"
    }
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Hit' object has no attribute 'page_identifier'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-147-a817dfd0a0c7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresults\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresults\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpage_identifier\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m     \u001b[0;31m# print(results[0])\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0;31m# print(results[1])\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'Hit' object has no attribute 'page_identifier'"
     ]
    }
   ],
   "source": [
    "#searcher object is used for searching the matched documents\n",
    "#you can open the searcher using a with statement so the searcher is automatically closed when you’re done with it\n",
    "#ix is the document index we created before\n",
    "\n",
    "with ix.searcher() as searcher:\n",
    "    results=searcher.search(result)#The Results object acts like a list of the matched documents.\n",
    "    # print(len(results))\n",
    "    \n",
    "    if len(results) > 0:\n",
    "        print(results[0].page_identifier)\n",
    "    # print(results[0])\n",
    "    # print(results[1])\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
