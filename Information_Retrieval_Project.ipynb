{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Information Retrieval\n",
    "\n",
    "This is the document retrieval and sentence retrieval part of the project.\n",
    "Method "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Unpack the zip file.\n",
    "    \n",
    "Unpack the 'wiki-pages-text.zip' in the current directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import zipfile\n",
    "\n",
    "def unpack():\n",
    "    with zipfile.ZipFile('wiki-pages-text.zip') as file:\n",
    "        file.extractall()\n",
    "unpack()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Load file. \n",
    "\n",
    "Load the training dataset and the wiki txt file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of the train data is: 145449\n",
      "Top 3 instances in train data\n",
      "75397 {'claim': 'Nikolaj Coster-Waldau worked with the Fox Broadcasting Company.', 'label': 'SUPPORTS', 'evidence': [['Fox_Broadcasting_Company', 0], ['Nikolaj_Coster-Waldau', 7]]}\n",
      "150448 {'claim': 'Roman Atwood is a content creator.', 'label': 'SUPPORTS', 'evidence': [['Roman_Atwood', 1], ['Roman_Atwood', 3]]}\n",
      "214861 {'claim': 'History of art includes architecture, dance, sculpture, music, painting, poetry literature, theatre, narrative, film, photography and graphic arts.', 'label': 'SUPPORTS', 'evidence': [['History_of_art', 2]]}\n",
      "\n",
      "\n",
      "['wiki-009.txt', 'wiki-021.txt', 'wiki-035.txt', 'wiki-034.txt', 'wiki-020.txt', 'wiki-008.txt', 'wiki-036.txt', 'wiki-022.txt', 'wiki-023.txt', 'wiki-037.txt', 'wiki-033.txt', 'wiki-027.txt', 'wiki-026.txt', 'wiki-032.txt', 'wiki-024.txt', 'wiki-030.txt', 'wiki-018.txt', 'wiki-019.txt', 'wiki-031.txt', 'wiki-025.txt', 'wiki-042.txt', 'wiki-056.txt', 'wiki-081.txt', 'wiki-095.txt', 'wiki-094.txt', 'wiki-080.txt', 'wiki-057.txt', 'wiki-043.txt', 'wiki-069.txt', 'wiki-055.txt', 'wiki-041.txt', 'wiki-096.txt', 'wiki-082.txt', 'wiki-109.txt', 'wiki-108.txt', 'wiki-083.txt', 'wiki-097.txt', 'wiki-040.txt', 'wiki-054.txt', 'wiki-068.txt', 'wiki-050.txt', 'wiki-044.txt', 'wiki-078.txt', 'wiki-093.txt', 'wiki-087.txt', 'wiki-086.txt', 'wiki-092.txt', 'wiki-079.txt', 'wiki-045.txt', 'wiki-051.txt', 'wiki-047.txt', 'wiki-053.txt', 'wiki-084.txt', 'wiki-090.txt', 'wiki-091.txt', 'wiki-085.txt', 'wiki-052.txt', 'wiki-046.txt', 'wiki-063.txt', 'wiki-077.txt', 'wiki-088.txt', 'wiki-103.txt', 'wiki-102.txt', 'wiki-089.txt', 'wiki-076.txt', 'wiki-062.txt', 'wiki-048.txt', 'wiki-074.txt', 'wiki-060.txt', 'wiki-100.txt', 'wiki-101.txt', 'wiki-061.txt', 'wiki-075.txt', 'wiki-049.txt', 'wiki-071.txt', 'wiki-065.txt', 'wiki-059.txt', 'wiki-105.txt', 'wiki-104.txt', 'wiki-058.txt', 'wiki-064.txt', 'wiki-070.txt', 'wiki-066.txt', 'wiki-072.txt', 'wiki-099.txt', 'wiki-106.txt', 'wiki-107.txt', 'wiki-098.txt', 'wiki-073.txt', 'wiki-067.txt', 'wiki-028.txt', 'wiki-014.txt', 'wiki-015.txt', 'wiki-001.txt', 'wiki-029.txt', 'wiki-017.txt', 'wiki-003.txt', 'wiki-002.txt', 'wiki-016.txt', 'wiki-012.txt', 'wiki-006.txt', 'wiki-007.txt', 'wiki-013.txt', 'wiki-005.txt', 'wiki-011.txt', 'wiki-039.txt', 'wiki-038.txt', 'wiki-010.txt', 'wiki-004.txt']\n",
      "\n",
      "\n",
      "Length of the document is: 25248397\n",
      "\n",
      "\n",
      "Alexander_McNair 0 Alexander McNair -LRB- May 5 , 1775 -- March 18 , 1826 -RRB- was an American frontiersman and politician .\n",
      "\n",
      "Alexander_McNair 1 He was the first Governor of Missouri from its entry as a state in 1820 , until 1824 .\n",
      "\n",
      "Alexander_McNair 4 McNair was born in Lancaster in the Province of Pennsylvania and grew up in Mifflin County .\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "\n",
    "# load training dataset\n",
    "with open('train.json', 'r') as f:\n",
    "        train_data = json.load(f)  \n",
    "\n",
    "print(\"Length of the train data is: \" + str(len(train_data)))\n",
    "\n",
    "# Top 3 instances in train data\n",
    "print(\"Top 3 instances in train data\")\n",
    "for key in list(train_data)[:3]:\n",
    "    print(key, train_data[key])\n",
    "\n",
    "print(\"\\n\")\n",
    "\n",
    "# appeand all the wiki txt sentences to one document\n",
    "def loadfile(folder): \n",
    "    document = []\n",
    "    list_of_files = os.listdir(folder)\n",
    "    print(list_of_files)\n",
    "    \n",
    "    for file in list_of_files:\n",
    "        try:\n",
    "            filename = os.path.join(folder, file)\n",
    "            with open(filename, 'r') as doc:\n",
    "                for line in doc:\n",
    "                    document.append(line)\n",
    "                 \n",
    "        except Exception as e:\n",
    "            print(\"No files found here!\")\n",
    "            raise e\n",
    "            \n",
    "    return document\n",
    "\n",
    "document = loadfile(\"wiki-pages-text\")\n",
    "\n",
    "print(\"\\n\")\n",
    "print(\"Length of the document is: \" + str(len(document)))\n",
    "print(\"\\n\")\n",
    "\n",
    "# index0 wiki file, index1 sentence\n",
    "print(document[0])\n",
    "print(document[1])\n",
    "print(document[2])\n",
    "print(\"\\n\")\n",
    "\n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>3. Method one : TF-IDF  </b>\n",
    "\n",
    "3.1 Preprocess the sentence: strip punctuations, tokenize,stem, lower case, remove stop words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of sentences = 100\n",
      "Number of unique terms = 800\n",
      "\n",
      "\n",
      "['alexander_mcnair', '0', 'alexand', 'mcnair', 'lrb', 'may', '5', '1775', 'march', '18', '1826', 'rrb', 'wa', 'american', 'frontiersman', 'politician']\n",
      "['alexander_mcnair', '1', 'wa', 'first', 'governor', 'missouri', 'entri', 'state', '1820', '1824']\n",
      "['alexander_mcnair', '4', 'mcnair', 'wa', 'born', 'lancast', 'provinc', 'pennsylvania', 'grew', 'mifflin', 'counti']\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "stemmer = nltk.stem.PorterStemmer()\n",
    "stop_words = set(stopwords.words('english')) \n",
    "\n",
    "# processed_docs stores the list of processed docs\n",
    "preprocess = []\n",
    "processed_doc = []\n",
    "vocab = {}\n",
    "\n",
    "def preprocess(document):\n",
    "    # vocab contains (term, term id) pairs\n",
    "    vocab = {}\n",
    "    # unique id for each term\n",
    "    unique_id = 0\n",
    "    \n",
    "    # for each sentence in the document\n",
    "    for sentence in document: \n",
    "        \n",
    "        norm_sentence = []\n",
    "        \n",
    "        # strip punctuations: remove ',' '.',etc.\n",
    "        sentence = re.sub(r'[^\\w\\s]', '', sentence)\n",
    "        \n",
    "        # tokenize words\n",
    "        tokens = nltk.tokenize.word_tokenize(sentence)  \n",
    "                \n",
    "        for token in tokens:\n",
    "            # stem the token\n",
    "            token = stemmer.stem(token)  \n",
    "            # lower case the token\n",
    "            token = token.lower()  \n",
    "        \n",
    "            # remove stop words\n",
    "            if token not in stop_words:\n",
    "                norm_sentence.append(token)  \n",
    "                if token not in vocab:\n",
    "                    vocab.update({token: unique_id})     \n",
    "                    unique_id = unique_id + 1\n",
    "                \n",
    "        processed_doc.append(norm_sentence)\n",
    "        \n",
    "    return processed_doc, vocab\n",
    "\n",
    "# testing: only first 100 sentences\n",
    "preprocess = preprocess(document[:100])\n",
    "\n",
    "# processed_doc is the document that all the sentences after preprocessed.\n",
    "processed_doc = preprocess[0]\n",
    "# vocab is all the unique tokens of the document\n",
    "vocab = preprocess[1]\n",
    "\n",
    "print(\"Number of sentences = {}\".format(len(processed_doc)))\n",
    "print(\"Number of unique terms = {}\".format(len(vocab)))\n",
    "\n",
    "print(\"\\n\")\n",
    "print(processed_doc[0])\n",
    "print(processed_doc[1])\n",
    "print(processed_doc[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3.2 Calculate document term frequence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of doc term freqs = 100\n",
      "\n",
      "\n",
      "Counter({'alexander_mcnair': 1, '0': 1, 'alexand': 1, 'mcnair': 1, 'lrb': 1, 'may': 1, '5': 1, '1775': 1, 'march': 1, '18': 1, '1826': 1, 'rrb': 1, 'wa': 1, 'american': 1, 'frontiersman': 1, 'politician': 1})\n",
      "Counter({'alexander_mcnair': 1, '1': 1, 'wa': 1, 'first': 1, 'governor': 1, 'missouri': 1, 'entri': 1, 'state': 1, '1820': 1, '1824': 1})\n",
      "Counter({'alexander_mcnair': 1, '4': 1, 'mcnair': 1, 'wa': 1, 'born': 1, 'lancast': 1, 'provinc': 1, 'pennsylvania': 1, 'grew': 1, 'mifflin': 1, 'counti': 1})\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "def doc_term_freq(doc):\n",
    "    doc_term_freqs = []\n",
    "    \n",
    "    for sentence in doc:\n",
    "        doc_term_freqs.append(Counter(sentence)) \n",
    "    \n",
    "    return doc_term_freqs\n",
    "\n",
    "doc_term_freqs = doc_term_freq(processed_doc)\n",
    "\n",
    "# first 100 sentences\n",
    "print(\"Number of doc term freqs = {}\".format(len(doc_term_freqs)))\n",
    "print(\"\\n\")\n",
    "\n",
    "print(doc_term_freqs[0])\n",
    "print(doc_term_freqs[1])\n",
    "print(doc_term_freqs[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3.3 Build Inverted Index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "documents = 100\n",
      "number of terms = 800\n",
      "longest document length = 37\n",
      "\n",
      "\n",
      "{'alexander_mcnair': 0, '0': 1, 'alexand': 2, 'mcnair': 3, 'lrb': 4, 'may': 5, '5': 6, '1775': 7, 'march': 8, '18': 9, '1826': 10, 'rrb': 11, 'wa': 12, 'american': 13, 'frontiersman': 14, 'politician': 15, '1': 16, 'first': 17, 'governor': 18, 'missouri': 19, 'entri': 20, 'state': 21, '1820': 22, '1824': 23, '4': 24, 'born': 25, 'lancast': 26, 'provinc': 27, 'pennsylvania': 28, 'grew': 29, 'mifflin': 30, 'counti': 31, 'hi': 32, 'grandfath': 33, 'david': 34, 'sr': 35, 'immigr': 36, 'donaghmor': 37, 'doneg': 38, 'ireland': 39, 'around': 40, '1733': 41, 'scottish': 42, 'ancestor': 43, 'loch': 44, 'lomond': 45, '6': 46, 'jr': 47, 'father': 48, 'b': 49, '1736': 50, 'fought': 51, 'gener': 52, 'georg': 53, 'washington': 54, 'trenton': 55, 'princeton': 56, 'campaign': 57, 'winter': 58, '1776': 59, '77': 60, 'die': 61, 'februari': 62, '1777': 63, 'result': 64, 'wound': 65, 'receiv': 66, 'battl': 67, 'exposur': 68, 'less': 69, 'two': 70, 'year': 71, 'old': 72, '7': 73, 'went': 74, 'school': 75, 'child': 76, 'attend': 77, 'one': 78, 'term': 79, 'colleg': 80, 'philadelphia': 81, 'univers': 82, '8': 83, 'reach': 84, 'agreement': 85, 'mother': 86, 'brother': 87, 'would': 88, 'box': 89, 'match': 90, 'winner': 91, 'properti': 92, '9': 93, 'defeat': 94, '10': 95, 'becam': 96, 'member': 97, 'militia': 98, 'govern': 99, 'whiskey': 100, 'rebellion': 101, '1791': 102, '1794': 103, '13': 104, '1804': 105, 'travel': 106, 'unit': 107, 'acquir': 108, 'follow': 109, 'louisiana': 110, 'purchas': 111, '14': 112, 'marri': 113, 'marguerit': 114, 'suzann': 115, 'de': 116, 'reihl': 117, 'regal': 118, 'daughter': 119, 'french': 120, 'marqui': 121, '15': 122, 'live': 123, 'st': 124, 'loui': 125, 'particip': 126, 'freemasonri': 127, 'lodg': 128, '111': 129, 'serv': 130, 'marshal': 131, '16': 132, 'also': 133, 'success': 134, 'businessman': 135, 'board': 136, 'truste': 137, 'town': 138, '1808': 139, '1813': 140, '19': 141, '24': 142, 'stephen': 143, 'f': 144, 'austin': 145, 'later': 146, 'respons': 147, 'colon': 148, 'texa': 149, 'name': 150, 'commiss': 151, 'ensign': 152, '20': 153, 'septemb': 154, 'enlist': 155, 'privat': 156, 'regiment': 157, 'mount': 158, 'command': 159, 'colonel': 160, '23': 161, 'elect': 162, '72': 163, 'vote': 164, 'famou': 165, 'explor': 166, 'william': 167, 'clark': 168, 'time': 169, 'work': 170, 'indian': 171, 'depart': 172, 'death': 173, '25': 174, 'influenza': 175, 'buri': 176, 'calvari': 177, 'cemeteri': 178, 'alatskivi': 179, 'vallavanem': 180, 'andu': 181, 'tõrva': 182, '3': 183, 'small': 184, 'borough': 185, 'alevik': 186, 'tartu': 187, 'estonia': 188, 'administr': 189, 'centr': 190, 'parish': 191, 'main': 192, 'sight': 193, 'gothic': 194, 'style': 195, 'castl': 196, 'local': 197, 'manor': 198, 'an_american_girl_story__maryellen_1955colon_extraordinary_christma': 199, 'girl': 200, 'stori': 201, 'maryellen': 202, '1955': 203, 'extraordinari': 204, 'christma': 205, '2016': 206, 'familydrama': 207, 'film': 208, 'star': 209, 'alyvia': 210, 'alyn': 211, 'lind': 212, 'titl': 213, 'role': 214, 'along': 215, 'mari': 216, 'mccormack': 217, 'madison': 218, 'lawlor': 219, 'support': 220, 'take': 221, 'place': 222, 'mid1950': 223, 'daytona': 224, 'beach': 225, 'florida': 226, 'center': 227, 'larkin': 228, 'life': 229, 'togeth': 230, 'big': 231, 'famili': 232, 'long': 233, 'stand': 234, 'among': 235, 'sibl': 236, 'rel': 237, 'help': 238, 'number': 239, 'young': 240, 'patient': 241, 'polio': 242, 'ward': 243, 'victim': 244, 'second': 245, 'seri': 246, 'releas': 247, 'exclus': 248, 'onlin': 249, 'stream': 250, 'servic': 251, 'twelfth': 252, 'overal': 253, 'featur': 254, 'charact': 255, 'short': 256, 'brightest': 257, 'harli': 258, 'galloway': 259, 'alta_outcome_docu': 260, 'alta': 261, 'outcom': 262, 'document': 263, 'indigen': 264, 'peopl': 265, 'recommend': 266, 'highlevel': 267, 'plenari': 268, 'meet': 269, 'assembl': 270, 'call': 271, 'world': 272, 'confer': 273, '2014': 274, 'produc': 275, 'global': 276, 'preparatori': 277, 'norway': 278, '12': 279, 'june': 280, '2013': 281, '300': 282, 'repres': 283, 'geopolit': 284, 'region': 285, 'creat': 286, '600': 287, 'deleg': 288, 'observ': 289, 'norwegian': 290, 'sami': 291, 'parliament': 292, 'host': 293, 'event': 294, 'draft': 295, 'democrat': 296, 'process': 297, 'facilit': 298, 'write': 299, 'group': 300, 'consist': 301, 'question': 302, 'whether': 303, 'reflect': 304, 'genuin': 305, 'consensu': 306, 'given': 307, 'divers': 308, 'ambit': 309, 'involv': 310, 'accept': 311, 'offici': 312, 'nation': 313, 'key': 314, 'topic': 315, 'includ': 316, '17': 317, 'land': 318, 'territori': 319, 'resourc': 320, 'ocean': 321, 'water': 322, 'implement': 323, 'right': 324, 'un': 325, 'action': 326, '21': 327, 'prioriti': 328, 'develop': 329, 'free': 330, 'prior': 331, 'inform': 332, 'consent': 333, 'relationship': 334, 'extract': 335, 'industri': 336, 'regard': 337, 'access': 338, 'decisionmak': 339, 'distribut': 340, 'incom': 341, '26': 342, 'increas': 343, 'integr': 344, 'ancestr': 345, 'domain': 346, '27': 347, 'exampl': 348, 'indonesian': 349, 'allianc': 350, 'archipelago': 351, 'wrote': 352, 'letter': 353, 'presid': 354, 'susilo': 355, 'bambang': 356, 'yudhoyono': 357, 'indonesia': 358, 'object': 359, 'plan': 360, 'claim': 361, 'al_urban': 362, 'al': 363, 'urban': 364, 'northwestern': 365, 'libya': 366, 'jabal': 367, 'gharbi': 368, 'district': 369, '2007': 370, 'locat': 371, 'gharyan': 372, '2': 373, 'close': 374, 'capit': 375, 'roughli': 376, '87': 377, 'kilometr': 378, 'south': 379, 'countri': 380, 'tripoli': 381, 'albano_buoy_system': 382, 'albano': 383, 'lane': 384, 'system': 385, 'method': 386, 'mark': 387, 'kayak': 388, 'cano': 389, 'row': 390, 'race': 391, 'cours': 392, 'use': 393, 'line': 394, 'buoy': 395, 'intern': 396, '1960': 397, 'summer': 398, 'olymp': 399, 'game': 400, 'held': 401, 'lake': 402, 'itali': 403, 'ha': 404, 'sinc': 405, 'becom': 406, 'standard': 407, 'fisa': 408, 'amuka': 409, 'stage': 410, 'singersongwrit': 411, 'sheila': 412, 'brodi': 413, 'coin': 414, 'artistproduc': 415, 'jahkey': 416, 'cowrot': 417, 'song': 418, 'appreci': 419, 'origin': 420, 'present': 421, 'detroit': 422, 'discov': 423, 'funk': 424, 'musician': 425, 'clinton': 426, 'bride': 427, 'funkenstein': 428, 'pfunk': 429, 'spinoff': 430, 'act': 431, 'aleksandr_abdulkhalikov': 432, 'aleksandr': 433, 'rustamovich': 434, 'abdulkhalikov': 435, 'александр': 436, 'рустамович': 437, 'абдулхаликов': 438, '1975': 439, 'russian': 440, 'profession': 441, 'footbal': 442, 'player': 443, 'last': 444, 'play': 445, 'divis': 446, 'fc': 447, 'khimik': 448, 'dzerzhinsk': 449, 'hold': 450, 'uzbekistani': 451, 'citizenship': 452, 'twin': 453, 'aleksei': 454, 'amalia_ciardi_duprè': 455, 'amalia': 456, 'ciardi': 457, 'duprè': 458, '1934': 459, 'italian': 460, 'sculptor': 461, 'painter': 462, 'akbil': 463, 'commun': 464, 'tizi': 465, 'ouzou': 466, 'northern': 467, 'algeria': 468, 'all_these_year': 469, 'written': 470, 'mac': 471, 'mcanal': 472, 'record': 473, '1992': 474, 'album': 475, 'learn': 476, 'music': 477, 'sawyer': 478, 'brown': 479, 'novemb': 480, 'singl': 481, 'cafe': 482, 'corner': 483, 'version': 484, 'peak': 485, 'billboard': 486, 'hot': 487, 'track': 488, 'chart': 489, 'addit': 490, 'minor': 491, 'ac': 492, 'hit': 493, '42': 494, 'adult': 495, 'contemporari': 496, 'ambulance_services_of_victoria': 497, 'juli': 498, '2008': 499, 'emerg': 500, 'ambul': 501, 'victoria': 502, 'provid': 503, 'known': 504, 'form': 505, 'three': 506, 'previou': 507, 'metropolitan': 508, 'rural': 509, 'rav': 510, 'alexandra': 511, 'ada': 512, 'melbourn': 513, 'outer': 514, 'suburb': 515, 'area': 516, 'except': 517, 'marysvil': 518, 'eildon': 519, 'oper': 520, 'pursuant': 521, '1986': 522, 'nonemerg': 523, 'transport': 524, 'compani': 525, '2003': 526, 'convent': 527, 'equip': 528, 'light': 529, 'siren': 530, 'sometim': 531, 'case': 532, 'alabama_elections_2018': 533, 'us': 534, 'alabama': 535, '2018': 536, 'execut': 537, 'offic': 538, 'well': 539, 'seven': 540, 'seat': 541, 'hous': 542, 'akarsu_ardanuç': 543, 'akarsu': 544, 'villag': 545, 'ardanuç': 546, 'artvin': 547, 'turkey': 548, '2010': 549, 'popul': 550, '114': 551, 'albertinovac': 552, 'uninhabit': 553, 'settlement': 554, 'croatia': 555, 'onli': 556, 'abandon': 557, 'remain': 558, 'part': 559, 'ledinik': 560, 'categori': 561, 'ghost': 562, 'allindia_yadav_mahasabha': 563, 'allindia': 564, 'yadav': 565, 'mahasabha': 566, 'cast': 567, 'associ': 568, 'establish': 569, '1924': 570, 'broad': 571, 'bodi': 572, 'social': 573, 'mainli': 574, 'compos': 575, 'nandvanshi': 576, 'gwalvanshi': 577, 'yaduvanshi': 578, 'collect': 579, 'englisheduc': 580, 'elit': 581, 'led': 582, 'format': 583, 'allahabad': 584, 'aiym': 585, 'immedi': 586, 'engag': 587, 'issu': 588, 'appeal': 589, 'castemen': 590, 'add': 591, 'launch': 592, 'major': 593, 'programm': 594, 'reform': 595, 'organis': 596, 'golla': 597, 'hyderabad': 598, 'rashtra': 599, 'mahajana': 600, 'samajam': 601, 'censu': 602, 'gowli': 603, 'gollawar': 604, 'ahir': 605, 'chang': 606, 'yadava': 607, 'mid20th': 608, 'centuri': 609, 'press': 610, 'armi': 611, 'show': 612, 'perform': 613, '1962': 614, 'indochina': 615, 'war': 616, 'alejandro_rodríguez_lópez': 617, 'alejandro': 618, 'rodríguez': 619, 'lópez': 620, '1964': 621, 'retir': 622, 'spanish': 623, 'rightback': 624, 'aleksandr_luzin': 625, 'igorevich': 626, 'luzin': 627, 'игоревич': 628, 'лузин': 629, '1995': 630, 'current': 631, 'mfk': 632, 'topvar': 633, 'topoľčani': 634, 'slovak': 635, 'liga': 636, 'made': 637, 'debut': 638, 'leagu': 639, 'krasnodar2': 640, 'chernomoret': 641, 'novorossiysk': 642, 'american_thigh': 643, 'thigh': 644, 'altern': 645, 'rock': 646, 'band': 647, 'veruca': 648, 'salt': 649, 'minti': 650, 'fresh': 651, '1994': 652, 'rereleas': 653, 'dgc': 654, 'refer': 655, 'acdc': 656, 'shook': 657, 'night': 658, 'artwork': 659, 'glossi': 660, 'paper': 661, 'alan_brown_lrbaustralian_politicianrrb': 662, 'alan': 663, 'john': 664, 'januari': 665, '1946': 666, 'australian': 667, 'liber': 668, 'victorian': 669, 'legisl': 670, 'leader': 671, 'opposit': 672, '1989': 673, '1991': 674, 'befor': 675, 'enter': 676, 'polit': 677, '1979': 678, 'abolish': 679, 'westernport': 680, 'western': 681, 'gippsland': 682, 'easili': 683, 'doug': 684, 'jen': 685, 'expel': 686, 'parti': 687, '1977': 688, 'backbench': 689, 'dure': 690, 'hamer': 691, 'thompson': 692, 'lost': 693, '1982': 694, 'promot': 695, 'shadow': 696, 'ministri': 697, 'minist': 698, 'youth': 699, 'sport': 700, 'educ': 701, 'aborigin': 702, 'affair': 703, 'leadership': 704, 'jeff': 705, 'kennett': 706, 'howev': 707, 'narrowli': 708, '1988': 709, 'mani': 710, 'unhappi': 711, 'oppos': 712, 'room': 713, 'manag': 714, 'instal': 715, 'fail': 716, 'full': 717, 'advantag': 718, 'variou': 719, 'crise': 720, 'labor': 721, 'negoti': 722, 'coalit': 723, 'relat': 724, 'tradit': 725, 'poor': 726, 'come': 727, 'five': 728, 'win': 729, 'thought': 730, 'threecorn': 731, 'contest': 732, 'tacit': 733, 'partyroom': 734, 'coup': 735, 'spill': 736, 'motion': 737, 'carri': 738, 'opt': 739, 'recontest': 740, 'allow': 741, 'unoppos': 742, 'concess': 743, 'kept': 744, 'frontbench': 745, 'return': 746, 'power': 747, 'public': 748, 'late': 749, '1996': 750, 'appoint': 751, 'agent': 752, 'byelect': 753, 'safe': 754, 'west': 755, 'renam': 756, '1985': 757, 'independ': 758, 'candid': 759, 'susan': 760, 'davi': 761, 'go': 762, 'decis': 763, 'promin': 764, 'unsuccess': 765, 'ran': 766, 'bass': 767, 'coast': 768, 'shire': 769, 'council': 770, 'hovel': 771, '2012': 772, 'al_lucas_lrbamerican_footballrrb': 773, 'albert': 774, 'luca': 775, '1978': 776, 'april': 777, '2005': 778, 'nfl': 779, 'arena': 780, 'afl': 781, 'gamerel': 782, 'spinal': 783, 'cord': 784, 'injuri': 785, 'lo': 786, 'angel': 787, 'aveng': 788, 'alfred_thompson_brich': 789, 'alfr': 790, 'bricher': 791, '1837': 792, '30': 793, '1908': 794, 'white': 795, 'mountain': 796, 'art': 797, 'hudson': 798, 'river': 799}\n"
     ]
    }
   ],
   "source": [
    "class InvertedIndex:\n",
    "    def __init__(self, vocab, doc_term_freqs):\n",
    "        self.vocab = vocab\n",
    "        self.doc_len = [0] * len(doc_term_freqs)\n",
    "        self.doc_term_freqs = [[] for i in range(len(vocab))]\n",
    "        self.doc_ids = [[] for i in range(len(vocab))]\n",
    "        self.doc_freqs = [0] * len(vocab)\n",
    "        self.total_num_docs = 0\n",
    "        self.max_doc_len = 0\n",
    "        for docid, term_freqs in enumerate(doc_term_freqs):\n",
    "            doc_len = sum(term_freqs.values())\n",
    "            self.max_doc_len = max(doc_len, self.max_doc_len)\n",
    "            self.doc_len[docid] = doc_len\n",
    "            self.total_num_docs += 1\n",
    "            for term, freq in term_freqs.items():\n",
    "                term_id = vocab[term]\n",
    "                self.doc_ids[term_id].append(docid)\n",
    "                self.doc_term_freqs[term_id].append(freq)\n",
    "                self.doc_freqs[term_id] += 1\n",
    "\n",
    "    def num_terms(self):\n",
    "        return len(self.doc_ids)\n",
    "\n",
    "    def num_docs(self):\n",
    "        return self.total_num_docs\n",
    "\n",
    "    def docids(self, term):\n",
    "        term_id = self.vocab[term]\n",
    "        return self.doc_ids[term_id]\n",
    "\n",
    "    def freqs(self, term):\n",
    "        term_id = self.vocab[term]\n",
    "        return self.doc_term_freqs[term_id]\n",
    "\n",
    "    def f_t(self, term):\n",
    "        term_id = self.vocab[term]\n",
    "        return self.doc_freqs[term_id]\n",
    "\n",
    "invindex = InvertedIndex(vocab, doc_term_freqs)\n",
    "\n",
    "# print inverted index stats\n",
    "print(\"documents = {}\".format(invindex.num_docs()))\n",
    "print(\"number of terms = {}\".format(invindex.num_terms()))\n",
    "print(\"longest document length = {}\".format(invindex.max_doc_len))\n",
    "\n",
    "print(\"\\n\")\n",
    "print(vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3.4 Uses score function to rank the sentences.\n",
    "\n",
    "\\begin{equation*}\n",
    "Score(Q,d) = \\frac{1}{\\sqrt{|d|}} \\times \\sum_{i=1}^q \\log(1 + f_{d,t}) * \\log( \\frac{N}{f_t} ) \n",
    "\\end{equation*}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stemmed query is = ['alexand', 'princeton', 'state']\n",
      "RANK  1 DOCID        4 SCORE 1.096 CONTENT Alexander_McNair 6 David McNair , Jr. , Alexander 's father -LRB- b. 1736 -\n",
      "RANK  2 DOCID        7 SCORE 0.929 CONTENT Alexander_McNair 9 Alexander was defeated .\n",
      "\n",
      "RANK  3 DOCID        1 SCORE 0.617 CONTENT Alexander_McNair 1 He was the first Governor of Missouri from its entry as \n",
      "RANK  4 DOCID       64 SCORE 0.588 CONTENT Alabama_elections,_2018 0 A general election will be held in the U.S. state\n",
      "RANK  5 DOCID        9 SCORE 0.563 CONTENT Alexander_McNair 13 In 1804 , McNair traveled to what is now Missouri , the\n",
      "RANK  6 DOCID       14 SCORE 0.555 CONTENT Alexander_McNair 20 Later in September , he enlisted as a private in the Fi\n",
      "RANK  7 DOCID        5 SCORE 0.536 CONTENT Alexander_McNair 7 Alexander went to school as a child , and attended one t\n",
      "RANK  8 DOCID       38 SCORE 0.521 CONTENT Alta_Outcome_Document 26 Indigenous peoples ' increasing involvement with U\n",
      "RANK  9 DOCID       65 SCORE 0.521 CONTENT Alabama_elections,_2018 1 All of Alabama 's executive officers will be up f\n",
      "RANK 10 DOCID        0 SCORE 0.519 CONTENT Alexander_McNair 0 Alexander McNair -LRB- May 5 , 1775 -- March 18 , 1826 -\n"
     ]
    }
   ],
   "source": [
    "from math import log, sqrt\n",
    "\n",
    "# given a query and an index returns a list of the k highest scoring documents as tuples containing <docid,score>\n",
    "def query_tfidf(query, index, k=10):\n",
    "    \n",
    "    # scores stores doc ids and their scores\n",
    "    scores = Counter()\n",
    "\n",
    "    for term in query:   \n",
    "        # N: total number of documents\n",
    "        N = index.num_docs()    \n",
    "        # ft: document frequency of term \n",
    "        ft = index.f_t(term) \n",
    "        # docs: all doc ids that contain the term \n",
    "        docs = index.docids(term)\n",
    "        # dft: all document freqs of the term\n",
    "        dft = index.freqs(term)                 \n",
    "        \n",
    "        # num: index used for iterate the docs\n",
    "        for num, docid in enumerate(docs):                   \n",
    "            #fdt: frequency of term t in document d                                                   \n",
    "            fdt = dft[num]\n",
    "            # length: length of the doc\n",
    "            length = sqrt(abs(index.doc_len[docid]))        \n",
    "            \n",
    "            # tfidf: construct the score formula \n",
    "            tfidf = log(1 + fdt)*log(N/ft) \n",
    "            # score: the final score\n",
    "            scores[docid] += tfidf/length \n",
    "\n",
    "    return scores.most_common(k)\n",
    "\n",
    "\n",
    "# testing: query is a claim from the train dataset\n",
    "\n",
    "# query = \"Alexander Lukashenko is a head of state\"\n",
    "query = \"Alexander princeton is a of state\"\n",
    "# stemmed_query = nltk.stem.PorterStemmer().stem(query).split()\n",
    "\n",
    "# strip punctuations: remove ',' '.',etc.\n",
    "stemmed_query = []\n",
    "query = re.sub(r'[^\\w\\s]', '', query)      \n",
    "tokens = nltk.tokenize.word_tokenize(query)  \n",
    "                \n",
    "for token in tokens:\n",
    "    token = stemmer.stem(token)  \n",
    "    token = token.lower()  \n",
    "    if token not in stop_words:\n",
    "        stemmed_query.append(token)  \n",
    "print(\"stemmed query is = {}\".format(stemmed_query))\n",
    "\n",
    "      \n",
    "results = query_tfidf(stemmed_query, invindex)\n",
    "for rank, res in enumerate(results):\n",
    "    print(\"RANK {:2d} DOCID {:8d} SCORE {:.3f} CONTENT {:}\".format(rank+1,res[0],res[1],document[res[0]][:75]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
