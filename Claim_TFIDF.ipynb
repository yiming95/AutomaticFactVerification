{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-20T05:27:22.293683Z",
     "start_time": "2019-05-20T05:27:20.733381Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import gc\n",
    "import json\n",
    "import pickle\n",
    "import datetime\n",
    "import nltk\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from itertools import groupby\n",
    "from scipy.spatial.distance import cosine\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from scipy.sparse import coo_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load  dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-20T05:28:48.525606Z",
     "start_time": "2019-05-20T05:27:24.943523Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.7/site-packages/IPython/core/interactiveshell.py:2785: DtypeWarning: Columns (1) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(25248397, 3)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>page_identifier</th>\n",
       "      <th>sentence_number</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Alexander_McNair</td>\n",
       "      <td>0</td>\n",
       "      <td>Alexander_McNair 0 Alexander McNair Alexander ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Alexander_McNair</td>\n",
       "      <td>1</td>\n",
       "      <td>Alexander_McNair 1 Alexander McNair He was the...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Alexander_McNair</td>\n",
       "      <td>4</td>\n",
       "      <td>Alexander_McNair 4 Alexander McNair McNair was...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Alexander_McNair</td>\n",
       "      <td>5</td>\n",
       "      <td>Alexander_McNair 5 Alexander McNair His grandf...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Alexander_McNair</td>\n",
       "      <td>6</td>\n",
       "      <td>Alexander_McNair 6 Alexander McNair David McNa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Alexander_McNair</td>\n",
       "      <td>7</td>\n",
       "      <td>Alexander_McNair 7 Alexander McNair Alexander ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Alexander_McNair</td>\n",
       "      <td>8</td>\n",
       "      <td>Alexander_McNair 8 Alexander McNair He reached...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Alexander_McNair</td>\n",
       "      <td>9</td>\n",
       "      <td>Alexander_McNair 9 Alexander McNair Alexander ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Alexander_McNair</td>\n",
       "      <td>10</td>\n",
       "      <td>Alexander_McNair 10 Alexander McNair He became...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Alexander_McNair</td>\n",
       "      <td>13</td>\n",
       "      <td>Alexander_McNair 13 Alexander McNair In 1804 ,...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    page_identifier sentence_number  \\\n",
       "0  Alexander_McNair               0   \n",
       "1  Alexander_McNair               1   \n",
       "2  Alexander_McNair               4   \n",
       "3  Alexander_McNair               5   \n",
       "4  Alexander_McNair               6   \n",
       "5  Alexander_McNair               7   \n",
       "6  Alexander_McNair               8   \n",
       "7  Alexander_McNair               9   \n",
       "8  Alexander_McNair              10   \n",
       "9  Alexander_McNair              13   \n",
       "\n",
       "                                                text  \n",
       "0  Alexander_McNair 0 Alexander McNair Alexander ...  \n",
       "1  Alexander_McNair 1 Alexander McNair He was the...  \n",
       "2  Alexander_McNair 4 Alexander McNair McNair was...  \n",
       "3  Alexander_McNair 5 Alexander McNair His grandf...  \n",
       "4  Alexander_McNair 6 Alexander McNair David McNa...  \n",
       "5  Alexander_McNair 7 Alexander McNair Alexander ...  \n",
       "6  Alexander_McNair 8 Alexander McNair He reached...  \n",
       "7  Alexander_McNair 9 Alexander McNair Alexander ...  \n",
       "8  Alexander_McNair 10 Alexander McNair He became...  \n",
       "9  Alexander_McNair 13 Alexander McNair In 1804 ,...  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load_processed_corpus_df = pd.read_pickle(\"./processed_corpus.pkl\")\n",
    "load_processed_corpus_df = pd.read_csv(\"./new_wiki.csv\")\n",
    "print(load_processed_corpus_df.shape)\n",
    "load_processed_corpus_df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load  traning, dev, test dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-20T05:29:35.042585Z",
     "start_time": "2019-05-20T05:29:34.733337Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of the dev data is: 5001\n",
      "Length of the dev result data is: 5001\n",
      "Length of the test data is: 14997\n"
     ]
    }
   ],
   "source": [
    "with open('devset.json', 'r') as f1:  # load dev dataset\n",
    "        dev_data = json.load(f1) \n",
    "print(\"Length of the dev data is: \" + str(len(dev_data)))\n",
    "\n",
    "with open('devset_result.json', 'r') as f2:  # store result \n",
    "        res_data = json.load(f2) \n",
    "print(\"Length of the dev result data is: \" + str(len(res_data)))\n",
    "\n",
    "with open('test-unlabelled.json', 'r') as f3:  # store result \n",
    "     test_data = json.load(f3) \n",
    "print(\"Length of the test data is: \" + str(len(test_data)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-20T05:29:38.087938Z",
     "start_time": "2019-05-20T05:29:37.956662Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/zhangyiming/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "nltk.download('stopwords')\n",
    "\n",
    "lemmatizer = nltk.stem.WordNetLemmatizer()\n",
    "def lemmatize(word):\n",
    "    lemma = lemmatizer.lemmatize(word,'v')\n",
    "    if lemma == word:\n",
    "        lemma = lemmatizer.lemmatize(word,'n')\n",
    "    return lemma\n",
    "\n",
    "def pre_process(comment) -> str:\n",
    "    # lower cased\n",
    "    comment = comment.lower()\n",
    "    # tokenize\n",
    "    words =  nltk.tokenize.word_tokenize(comment)\n",
    "    # lemmatize \n",
    "    words = [lemmatize(w) for w in words]\n",
    "    \n",
    "    stop_words = nltk.corpus.stopwords.words('english')\n",
    "    words = [w for w in words if not w in stop_words]\n",
    "    processed_comment = \" \".join(words)\n",
    "    return processed_comment\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build a dictionary to store index in dataframe ( only run once)\n",
    "## Deal with LRB in dictionary. (only run once)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-20T05:50:45.763344Z",
     "start_time": "2019-05-20T05:29:40.947218Z"
    }
   },
   "outputs": [],
   "source": [
    "page_dictionary = {}\n",
    "\n",
    "for index, row in load_processed_corpus_df.iterrows(): \n",
    "    if isinstance(row['page_identifier'], float):\n",
    "        continue\n",
    "    page_dictionary.setdefault(row['page_identifier'],[]).append(index)\n",
    "\n",
    "def solve_LRB(dictionary):\n",
    "    for key, value in dictionary.items():\n",
    "        if '_-LRB' in key:\n",
    "            if key.split('_-LRB')[0] in dictionary:\n",
    "                for v in value:\n",
    "                    dictionary[key.split('_-LRB')[0]].append(v)\n",
    "\n",
    "    return dictionary\n",
    "\n",
    "lrb_dictionary = solve_LRB(page_dictionary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Delete keys in dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-20T10:51:59.237751Z",
     "start_time": "2019-05-20T10:51:59.234435Z"
    }
   },
   "outputs": [],
   "source": [
    "delete_words_list = ['The','Part', 'Most','Water', 'How', 'Love','Speech','American','President','German','Irish',\n",
    "                     'Indian','Spanish','Japan','Califorina','Americans','Chinese','British','Monday','Tuesday',\n",
    "                    'Wednesday','Thursday', 'Friday','Saturday', 'Sunday','January','February','March','April',\n",
    "                     'May', 'June', 'July', 'August', 'September', 'October', 'November','December']\n",
    "\n",
    "for word in delete_words_list:\n",
    "    try:\n",
    "        del lrb_dictionary[word]\n",
    "    except KeyError:\n",
    "        pass\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-20T10:51:49.597829Z",
     "start_time": "2019-05-20T10:51:49.480187Z"
    }
   },
   "outputs": [],
   "source": [
    "# page keys are the page identifier, keys of the page_dictionary\n",
    "page_keys = list(lrb_dictionary.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use dictionary to retrieve the sentence text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-20T10:52:01.715754Z",
     "start_time": "2019-05-20T10:52:01.712516Z"
    }
   },
   "outputs": [],
   "source": [
    "# input: a page identifier string, keys are all the keys of the page_dictionary\n",
    "# output: return a list of strings which contains all the relevanted sentence text,also with page_identifier and sentence number so\n",
    "# that it can be written as evidence part.\n",
    "\n",
    "def retrieve_sentenceText(claim_word,page_keys,page_dictionary,df):\n",
    "    retrieved_sentence = []\n",
    "    if claim_word in page_keys:\n",
    "        retrieved_index = page_dictionary[claim_word]      # all indexes in the dataframe\n",
    "        for index in retrieved_index:\n",
    "             # retrieve all the raw doc txt\n",
    "            retrieved_sentence.append(df.loc[index, 'text'])\n",
    "    return retrieved_sentence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rule on Claim: find the Upper cased words.\n",
    "\n",
    "Rule1: First Continuous Upper words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-20T10:52:03.732106Z",
     "start_time": "2019-05-20T10:52:03.728025Z"
    }
   },
   "outputs": [],
   "source": [
    "def find_upper_word(claim, page_keys):\n",
    "    res_list = []\n",
    "    res_index = []      \n",
    "    words = claim.split()\n",
    "    \n",
    "    start = \"\"\n",
    "    temp = \"\"\n",
    "    i = 0\n",
    "    while i < len(words):         \n",
    "        # first step: find uppercase word in the claim\n",
    "        if words[i][0].isupper():\n",
    "            temp = words[i]        \n",
    "            start = temp  # start(as a cache)                        \n",
    "            for j in range(i,len(words)-1):\n",
    "                temp = temp + '_' + words[j+1]\n",
    "                #print(temp)\n",
    "                if temp in page_keys:\n",
    "                    start = temp  # matchs the word as long as possible\n",
    "                    i = j + 1\n",
    "                \n",
    "                if j - i > 2:\n",
    "                    break\n",
    "                \n",
    "            res_list.append(start)  \n",
    "        i += 1                    \n",
    "    return res_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-20T10:52:07.505004Z",
     "start_time": "2019-05-20T10:52:05.492842Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['International', 'University_of_Mississippi']\n",
      "['Home_for_the_Holidays', 'American']\n",
      "['Water', 'History_of_Earth']\n",
      "['How_to_Train_Your_Dragon_2']\n"
     ]
    }
   ],
   "source": [
    "test_claim1 = \"International students come to the University of Mississippi from 90 cities\"\n",
    "print(find_upper_word(test_claim1,page_keys))\n",
    "test_claim2 = \"Home for the Holidays stars an American actress\"\n",
    "print(find_upper_word(test_claim2,page_keys))\n",
    "test_claim3 = \"Water is part of the History of Earth\"\n",
    "print(find_upper_word(test_claim3,page_keys))\n",
    "test_claim4 = \"How to Train Your Dragon 2 used real dragons\"\n",
    "print(find_upper_word(test_claim4,page_keys))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-20T06:39:29.779459Z",
     "start_time": "2019-05-20T06:39:29.776679Z"
    }
   },
   "outputs": [],
   "source": [
    "# TESTING LRB\n",
    "# print(find_upper_word(\"Savages was exclusively a German film.\"))\n",
    "# print(retrieve_sentenceText(find_upper_word(\"Savages was exclusively a German film.\")[0], page_keys,lrb_dictionary,load_processed_corpus_df))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Method1 :\n",
    "\n",
    "### TFIDF Vectorizer and SVD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-20T10:52:11.081427Z",
     "start_time": "2019-05-20T10:52:10.993200Z"
    }
   },
   "outputs": [],
   "source": [
    "def retrieval_evidence_func(query,page_keys,page_dictionary,load_processed_corpus_df):\n",
    "        \n",
    "    res = []\n",
    "    # Determine if the first word in query is \"There\" and \"A\"\n",
    "    if query.split()[0] == \"There\":\n",
    "        query = query.replace(\"There\",\"there\") \n",
    "        \n",
    "    if query.split()[0] == \"A\":\n",
    "        query = query.replace(\"A\",\"a\") \n",
    "    \n",
    "    # remove all 's \n",
    "    for word in query.split():\n",
    "        if (word[-2:len(word)]) == \"'s\":\n",
    "            query = query.replace(word,word[:-2])\n",
    "            \n",
    "     # remove all '\n",
    "    for word in query.split():\n",
    "        if (word[-1:len(word)]) == \"'\":\n",
    "            query = query.replace(word,word[:-1])\n",
    "            \n",
    "    # remove all .\n",
    "    for word in query.split():\n",
    "        if (word[-1:len(word)]) == \".\":\n",
    "            query = query.replace(word,word[:-1])\n",
    "            \n",
    "    query_corpus = []\n",
    "    if len(find_upper_word(query,page_keys)) >= 1:\n",
    "        for query_word in find_upper_word(query,page_keys):\n",
    "            query_corpus.extend(retrieve_sentenceText(query_word, page_keys,page_dictionary,load_processed_corpus_df))\n",
    "        \n",
    "        if len(query_corpus) == 1:\n",
    "            res.append([query_corpus[0].split()[0],int(query_corpus[0].split()[1])])\n",
    "        else:\n",
    "            # build a tfidf model on the query corpus\n",
    "            tfidf_vectorizer = TfidfVectorizer()\n",
    "            tfidf_matrix = tfidf_vectorizer.fit_transform(query_corpus)\n",
    "            tfidf_matrix = tfidf_matrix.T\n",
    "\n",
    "            # apply svd\n",
    "            K= 2 # number of desirable features \n",
    "            U, s, VT = np.linalg.svd(tfidf_matrix.toarray())\n",
    "            #tfidf_reduced = np.dot(U[:,:K], np.dot(np.diag(s[:K]), VT[:K, :]))\n",
    "            terms_rep = np.dot(U[:,:K], np.diag(s[:K]))\n",
    "            docs_rep = np.dot(np.diag(s[:K]), VT[:K, :]).T\n",
    "\n",
    "            # calculate query rep\n",
    "            query_rep = []\n",
    "            for q in pre_process(query).split():\n",
    "                if q in tfidf_vectorizer.vocabulary_:\n",
    "                    query_rep.append(tfidf_vectorizer.vocabulary_[q])\n",
    "                else:\n",
    "                    continue\n",
    "            query_rep = np.mean(terms_rep[query_rep],axis=0)\n",
    "\n",
    "            # calculate cosine similarity between the query and retrieved sentences\n",
    "            query_doc_cos_dist = []\n",
    "            for doc_rep in docs_rep:\n",
    "                query_doc_cos_dist.append(cosine(query_rep, doc_rep))\n",
    "\n",
    "            query_doc_sort_index = np.argsort(np.array(query_doc_cos_dist))\n",
    "            # retrievel top 3\n",
    "            count = 0\n",
    "            for rank, sort_index in enumerate(query_doc_sort_index):\n",
    "                res.append([query_corpus[sort_index].split()[0],int(query_corpus[sort_index].split()[1])])\n",
    "                if count == 3:\n",
    "                    break\n",
    "                else:\n",
    "                    count += 1     \n",
    "        return res\n",
    "    \n",
    "    else:\n",
    "        return []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Method2: \n",
    "### Count Vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-20T10:52:13.129526Z",
     "start_time": "2019-05-20T10:52:13.120401Z"
    }
   },
   "outputs": [],
   "source": [
    "def retrieval_evidence_func2(query,page_keys,page_dictionary,load_processed_corpus_df):\n",
    "    res = []\n",
    "    # Determine if the first word in query is \"There\" and \"A\"\n",
    "    if query.split()[0] == \"There\":\n",
    "        query = query.replace(\"There\",\"there\") \n",
    "        \n",
    "    if query.split()[0] == \"A\":\n",
    "        query = query.replace(\"A\",\"a\") \n",
    "    \n",
    "    # remove all 's \n",
    "    for word in query.split():\n",
    "        if (word[-2:len(word)]) == \"'s\":\n",
    "            query = query.replace(word,word[:-2])\n",
    "            \n",
    "     # remove all '\n",
    "    for word in query.split():\n",
    "        if (word[-1:len(word)]) == \"'\":\n",
    "            query = query.replace(word,word[:-1])\n",
    "            \n",
    "    # remove all .\n",
    "    for word in query.split():\n",
    "        if (word[-1:len(word)]) == \".\":\n",
    "            query = query.replace(word,word[:-1])\n",
    "    \n",
    "    for word in query.split():\n",
    "        if (word[-1:len(word)]) == \",\":\n",
    "            query = query.replace(word,word[:-1])\n",
    "            \n",
    "    query_corpus = []\n",
    "    if len(find_upper_word(query,page_keys)) >= 1:\n",
    "        for query_word in find_upper_word(query,page_keys):\n",
    "            query_corpus.extend(retrieve_sentenceText(query_word, page_keys,page_dictionary,load_processed_corpus_df))\n",
    "        \n",
    "        if len(query_corpus) == 1:\n",
    "            res.append([query_corpus[0].split()[0],int(query_corpus[0].split()[1])])\n",
    "        else:\n",
    "            # preprocess claim\n",
    "            processed_query_claim = pre_process(query)\n",
    "            # preprocess evidences\n",
    "            processed_query_corpus = []\n",
    "            for corpus in query_corpus:\n",
    "                processed_query_corpus.append(pre_process(corpus))\n",
    "\n",
    "            count_vectorizer = CountVectorizer()\n",
    "\n",
    "            processed_query_corpus.insert(0, processed_query_claim)\n",
    "\n",
    "            count = count_vectorizer.fit_transform(processed_query_corpus)\n",
    "\n",
    "            query_rep = count[0].todense()\n",
    "            docs_rep = count[1:].todense()\n",
    "\n",
    "            # calculate cosine similarity between the query and retrieved sentences\n",
    "            query_doc_cos_dist = []\n",
    "\n",
    "            # cosine distance, and hence no need to revese argsort result\n",
    "            for doc_rep in docs_rep:\n",
    "                query_doc_cos_dist.append(cosine(query_rep, doc_rep))\n",
    "\n",
    "            query_doc_sort_index = np.argsort(np.array(query_doc_cos_dist))\n",
    "\n",
    "            count = 0\n",
    "            for rank, sort_index in enumerate(query_doc_sort_index):\n",
    "                res.append([query_corpus[sort_index].split()[0],int(query_corpus[sort_index].split()[1])])\n",
    "                if count == 2:\n",
    "                    break\n",
    "                else:\n",
    "                    count += 1   \n",
    "        return res\n",
    "        \n",
    "    else:\n",
    "        return []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-20T10:52:24.260073Z",
     "start_time": "2019-05-20T10:52:17.418622Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['Brad_Wilk', 4], ['Brad_Wilk', 0], ['Rage', 0]]\n",
      "[['Faroe_Islands', 9], ['Faroe_Islands', 1], ['Faroe_Islands', 11]]\n",
      "[['With_Love', 0], ['Down_-LRB-film-RRB-', 2], ['Down_-LRB-film-RRB-', 9]]\n",
      "[['Telemundo', 0], ['Telemundo', 9], ['Telemundo', 8]]\n",
      "[['Russian', 18], ['Russian', 23], ['Russian', 16]]\n",
      "[['Glass', 20], ['Glass', 14], ['Glass', 10]]\n",
      "[['Damon_Albarn', 4], ['Damon_Albarn', 13], ['Damon_Albarn', 12]]\n"
     ]
    }
   ],
   "source": [
    "# testing\n",
    "test_claim1 = \"Brad Wilk helped co-found Rage in 1962.\"\n",
    "test_claim2 = \"The Faroe Islands are no longer part of the Kingdom of Mercia.\"\n",
    "test_claim3 = \"Down With Love is a 2003 comedy film.\"\n",
    "test_claim4 = \"Telemundo is a English-language television network.\"\n",
    "# To test LRB\n",
    "test_claim5 = 'Hourglass is performed by a Russian singer-songwriter.'\n",
    "# To test first word \"There\"\n",
    "test_claim6 = \"There are no musical or creative works in existence that have been created by Phillip Glass.\"\n",
    "# To test 's ' in the word\n",
    "test_claim7 = \"Damon Albarn's debut album was released in 2011.\"\n",
    "\n",
    "test_claim8 = \"Temple of the Dog celebrated the 37th anniversary of their self-titled album.\"\n",
    "test_claim9 = \"Savages was exclusively a German film.\"\n",
    "\n",
    "print(retrieval_evidence_func2(test_claim1,page_keys,lrb_dictionary,load_processed_corpus_df))\n",
    "print(retrieval_evidence_func2(test_claim2,page_keys,lrb_dictionary,load_processed_corpus_df))\n",
    "print(retrieval_evidence_func2(test_claim3,page_keys,lrb_dictionary,load_processed_corpus_df))\n",
    "print(retrieval_evidence_func2(test_claim4,page_keys,lrb_dictionary,load_processed_corpus_df))\n",
    "print(retrieval_evidence_func2(test_claim5,page_keys,lrb_dictionary,load_processed_corpus_df))\n",
    "print(retrieval_evidence_func2(test_claim6,page_keys,lrb_dictionary,load_processed_corpus_df))\n",
    "print(retrieval_evidence_func2(test_claim7,page_keys,lrb_dictionary,load_processed_corpus_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To test: can not find \n",
    "test1 = \"Boyhood is about Mason Evans, Jr's childhood.\"\n",
    "test2 = \"In 1986, Tatum O'Neal got married.\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Write result to  devset file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-20T10:55:47.544383Z",
     "start_time": "2019-05-20T10:52:33.616579Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n",
      "61\n",
      "62\n",
      "63\n",
      "64\n",
      "65\n",
      "66\n",
      "67\n",
      "68\n",
      "69\n",
      "70\n",
      "71\n",
      "72\n",
      "73\n",
      "74\n",
      "75\n",
      "76\n",
      "77\n",
      "78\n",
      "79\n",
      "80\n",
      "81\n",
      "82\n",
      "83\n",
      "84\n",
      "85\n",
      "86\n",
      "87\n",
      "88\n",
      "89\n",
      "90\n",
      "91\n",
      "92\n",
      "93\n",
      "94\n",
      "95\n",
      "96\n",
      "97\n",
      "98\n",
      "99\n",
      "100\n",
      "101\n",
      "102\n",
      "103\n",
      "104\n",
      "105\n",
      "106\n",
      "107\n",
      "108\n",
      "109\n",
      "110\n",
      "111\n",
      "112\n",
      "113\n",
      "114\n",
      "115\n",
      "116\n",
      "117\n",
      "118\n",
      "119\n",
      "120\n",
      "121\n",
      "122\n",
      "123\n",
      "124\n",
      "125\n",
      "126\n",
      "127\n",
      "128\n",
      "129\n",
      "130\n",
      "131\n",
      "132\n",
      "133\n",
      "134\n",
      "135\n",
      "136\n",
      "137\n",
      "138\n",
      "139\n",
      "140\n",
      "141\n",
      "142\n",
      "143\n",
      "144\n",
      "145\n",
      "146\n",
      "147\n",
      "148\n",
      "149\n",
      "150\n",
      "151\n",
      "152\n",
      "153\n",
      "154\n",
      "155\n",
      "156\n",
      "157\n",
      "158\n",
      "159\n",
      "160\n",
      "161\n",
      "162\n",
      "163\n",
      "164\n",
      "165\n",
      "166\n",
      "167\n",
      "168\n",
      "169\n",
      "170\n",
      "171\n",
      "172\n",
      "173\n",
      "174\n",
      "175\n",
      "176\n",
      "177\n",
      "178\n",
      "179\n",
      "180\n",
      "181\n",
      "182\n",
      "183\n",
      "184\n",
      "185\n",
      "186\n",
      "187\n",
      "188\n",
      "189\n",
      "190\n",
      "191\n",
      "192\n",
      "193\n",
      "194\n",
      "195\n",
      "196\n",
      "197\n",
      "198\n",
      "199\n",
      "200\n",
      "201\n",
      "202\n",
      "203\n",
      "204\n",
      "205\n",
      "206\n",
      "207\n",
      "208\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-237-568b0e149691>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mres_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"evidence\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mquery\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mres_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"claim\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0mres_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"evidence\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mretrieval_evidence_func2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquery\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mpage_keys\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlrb_dictionary\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mload_processed_corpus_df\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m     \u001b[0mi\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-235-164811bbbd04>\u001b[0m in \u001b[0;36mretrieval_evidence_func2\u001b[0;34m(query, page_keys, page_dictionary, load_processed_corpus_df)\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m     \u001b[0mquery_corpus\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfind_upper_word\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquery\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mpage_keys\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mquery_word\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfind_upper_word\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquery\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mpage_keys\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m             \u001b[0mquery_corpus\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mretrieve_sentenceText\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquery_word\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpage_keys\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mpage_dictionary\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mload_processed_corpus_df\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-232-f12ebba7a0b8>\u001b[0m in \u001b[0;36mfind_upper_word\u001b[0;34m(claim, page_keys)\u001b[0m\n\u001b[1;32m     15\u001b[0m                 \u001b[0mtemp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtemp\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'_'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mwords\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m                 \u001b[0;31m#print(temp)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m                 \u001b[0;32mif\u001b[0m \u001b[0mtemp\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpage_keys\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m                     \u001b[0mstart\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtemp\u001b[0m  \u001b[0;31m# matchs the word as long as possible\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m                     \u001b[0mi\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mj\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "i = 0\n",
    "for key in list(res_data):\n",
    "    res_data[key][\"evidence\"] = []\n",
    "    query= (res_data[key][\"claim\"])\n",
    "    res_data[key][\"evidence\"] = retrieval_evidence_func2(query,page_keys,lrb_dictionary,load_processed_corpus_df)\n",
    "    i += 1\n",
    "    print(i)\n",
    "    # print(retrieval_evidence_func(query,page_keys,page_dictionary,load_processed_corpus_df))\n",
    "    \n",
    "with open('result_dev_520_4_tf5.json', 'w') as f:\n",
    "    json.dump(res_data, f, indent = 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Write result to test file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-19T14:04:46.081357Z",
     "start_time": "2019-05-19T13:25:06.999455Z"
    }
   },
   "outputs": [],
   "source": [
    "i = 0\n",
    "for key in list(test_data):\n",
    "    test_data[key][\"evidence\"] = []\n",
    "    query= (test_data[key][\"claim\"])\n",
    "    test_data[key][\"evidence\"] = retrieval_evidence_func2(query,page_keys,lrb_dictionary,load_processed_corpus_df)\n",
    "    i += 1\n",
    "    print(i)\n",
    "    # print(retrieval_evidence_func(query,page_keys,page_dictionary,load_processed_corpus_df))\n",
    "    \n",
    "with open('result_test_519_tf3.json', 'w') as f:\n",
    "    json.dump(test_data, f, indent = 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DataFrame for BERT Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('train.json', 'r') as f4:  # store result \n",
    "        train_data = json.load(f4) \n",
    "\n",
    "claim_list = []\n",
    "evidence_list = []\n",
    "result_list = []\n",
    "\n",
    "def bert_data(train_data,page_keys,lrb_dictionary,load_processed_corpus_df):\n",
    "    for key in list(train_data):\n",
    "        claim = (train_data[key][\"claim\"])\n",
    "        claim_list.append(claim)\n",
    "        \n",
    "        res = []\n",
    "        # Determine if the first word in query is \"There\" and \"A\"\n",
    "        if claim.split()[0] == \"There\":\n",
    "            claim = claim.replace(\"There\",\"there\") \n",
    "\n",
    "        if claim.split()[0] == \"A\":\n",
    "            claim = claim.replace(\"A\",\"a\") \n",
    "\n",
    "        # remove all 's \n",
    "        for word in claim.split():\n",
    "            if (word[-2:len(word)]) == \"'s\":\n",
    "                claim = claim.replace(word,word[:-2])\n",
    "\n",
    "         # remove all '\n",
    "        for word in claim.split():\n",
    "            if (word[-1:len(word)]) == \"'\":\n",
    "                claim = claim.replace(word,word[:-1])\n",
    "\n",
    "        # remove all .\n",
    "        for word in claim.split():\n",
    "            if (word[-1:len(word)]) == \".\":\n",
    "                claim = claim.replace(word,word[:-1])\n",
    "\n",
    "        if len(find_upper_word(claim,page_keys)) >= 1:\n",
    "            for claim_word in find_upper_word(claim,page_keys):\n",
    "                claim_corpus = retrieve_sentenceText(claim_word, page_keys,lrb_dictionary,load_processed_corpus_df)        \n",
    "                    \n",
    "                evidence_list.append(claim_corpus)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
