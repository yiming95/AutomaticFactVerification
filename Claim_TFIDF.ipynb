{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Information Retrieval\n",
    "Author: Yiming Zhang \n",
    "Stuent ID: 889262\n",
    "\n",
    "It implements the document retrieval and sentence retrieval.\n",
    "I builds a dictionary to store the page identifier and sentence index.\n",
    "Then use the rule based information extraction function to retrievel the document.\n",
    "And many methods such as tfidf, BOW are used to retrievel the sentence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-26T04:31:55.860911Z",
     "start_time": "2019-05-26T04:31:54.768054Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import gc\n",
    "import json\n",
    "import pickle\n",
    "import datetime\n",
    "import nltk\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from itertools import groupby\n",
    "from scipy.spatial.distance import cosine\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from scipy.sparse import coo_matrix\n",
    "import multiprocessing\n",
    "from multiprocessing import Manager,Pool\n",
    "import copy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load  dataframe\n",
    "The dataframe is the result from \"Process_New_wiki.ipynb\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-26T04:33:00.270700Z",
     "start_time": "2019-05-26T04:31:57.175612Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(25248397, 3)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>page_identifier</th>\n",
       "      <th>sentence_number</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>42nd_Street_Ferry</td>\n",
       "      <td>0</td>\n",
       "      <td>42nd_Street_Ferry 0 42nd Street Ferry 42nd Str...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>42nd_Street_Ferry</td>\n",
       "      <td>3</td>\n",
       "      <td>42nd_Street_Ferry 3 42nd Street Ferry 42nd Str...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>42nd_Street_Ferry</td>\n",
       "      <td>5</td>\n",
       "      <td>42nd_Street_Ferry 5 42nd Street Ferry 42nd Str...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Aeroflot-Plus</td>\n",
       "      <td>0</td>\n",
       "      <td>Aeroflot-Plus 0 Aeroflot-Plus Aeroflot-Plus wa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>A._U._Wyman</td>\n",
       "      <td>0</td>\n",
       "      <td>A._U._Wyman 0 A. U. Wyman Albert U. Wyman 1833...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>A._U._Wyman</td>\n",
       "      <td>3</td>\n",
       "      <td>A._U._Wyman 3 A. U. Wyman Wyman was raised in ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>A._U._Wyman</td>\n",
       "      <td>4</td>\n",
       "      <td>A._U._Wyman 4 A. U. Wyman After completing a c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>A._U._Wyman</td>\n",
       "      <td>5</td>\n",
       "      <td>A._U._Wyman 5 A. U. Wyman He later went into b...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>A._U._Wyman</td>\n",
       "      <td>8</td>\n",
       "      <td>A._U._Wyman 8 A. U. Wyman In 1863 , Wyman beca...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>A._U._Wyman</td>\n",
       "      <td>9</td>\n",
       "      <td>A._U._Wyman 9 A. U. Wyman He was then appointe...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     page_identifier sentence_number  \\\n",
       "0  42nd_Street_Ferry               0   \n",
       "1  42nd_Street_Ferry               3   \n",
       "2  42nd_Street_Ferry               5   \n",
       "3      Aeroflot-Plus               0   \n",
       "4        A._U._Wyman               0   \n",
       "5        A._U._Wyman               3   \n",
       "6        A._U._Wyman               4   \n",
       "7        A._U._Wyman               5   \n",
       "8        A._U._Wyman               8   \n",
       "9        A._U._Wyman               9   \n",
       "\n",
       "                                                text  \n",
       "0  42nd_Street_Ferry 0 42nd Street Ferry 42nd Str...  \n",
       "1  42nd_Street_Ferry 3 42nd Street Ferry 42nd Str...  \n",
       "2  42nd_Street_Ferry 5 42nd Street Ferry 42nd Str...  \n",
       "3  Aeroflot-Plus 0 Aeroflot-Plus Aeroflot-Plus wa...  \n",
       "4  A._U._Wyman 0 A. U. Wyman Albert U. Wyman 1833...  \n",
       "5  A._U._Wyman 3 A. U. Wyman Wyman was raised in ...  \n",
       "6  A._U._Wyman 4 A. U. Wyman After completing a c...  \n",
       "7  A._U._Wyman 5 A. U. Wyman He later went into b...  \n",
       "8  A._U._Wyman 8 A. U. Wyman In 1863 , Wyman beca...  \n",
       "9  A._U._Wyman 9 A. U. Wyman He was then appointe...  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load_processed_corpus_df = pd.read_pickle(\"./processed_corpus.pkl\")\n",
    "load_processed_corpus_df = pd.read_csv(\"./new_wiki.csv\")\n",
    "print(load_processed_corpus_df.shape)\n",
    "load_processed_corpus_df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load  traning, dev, test dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-26T04:33:24.614121Z",
     "start_time": "2019-05-26T04:33:24.530831Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of the dev data is: 5001\n",
      "Length of the dev result data is: 5001\n",
      "Length of the test data is: 14997\n"
     ]
    }
   ],
   "source": [
    "with open('devset.json', 'r') as f1:  # load dev dataset\n",
    "        dev_data = json.load(f1) \n",
    "print(\"Length of the dev data is: \" + str(len(dev_data)))\n",
    "\n",
    "with open('devset_result.json', 'r') as f2:  # store result \n",
    "        res_data = json.load(f2) \n",
    "print(\"Length of the dev result data is: \" + str(len(res_data)))\n",
    "\n",
    "with open('test-unlabelled.json', 'r') as f3:  # store result \n",
    "     test_data = json.load(f3) \n",
    "print(\"Length of the test data is: \" + str(len(test_data)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess\n",
    "Remove stop words, lemma, tokenzition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-26T04:33:26.533119Z",
     "start_time": "2019-05-26T04:33:26.450716Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/zhangyiming/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "nltk.download('stopwords')\n",
    "\n",
    "lemmatizer = nltk.stem.WordNetLemmatizer()\n",
    "def lemmatize(word):\n",
    "    lemma = lemmatizer.lemmatize(word,'v')\n",
    "    if lemma == word:\n",
    "        lemma = lemmatizer.lemmatize(word,'n')\n",
    "    return lemma\n",
    "\n",
    "def pre_process(comment) -> str:\n",
    "    # lower cased\n",
    "    comment = comment.lower()\n",
    "    # tokenize\n",
    "    words =  nltk.tokenize.word_tokenize(comment)\n",
    "    # lemmatize \n",
    "    words = [lemmatize(w) for w in words]\n",
    "    \n",
    "    stop_words = nltk.corpus.stopwords.words('english')\n",
    "    words = [w for w in words if not w in stop_words]\n",
    "    processed_comment = \" \".join(words)\n",
    "    return processed_comment\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build a dictionary to store index in dataframe ( only run once)\n",
    "## Deal with LRB in dictionary. (only run once)\n",
    "The code of building the dictionary is in the comment below.\n",
    "For convience, I just save the dicitoanry and load it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-26T04:33:42.418326Z",
     "start_time": "2019-05-26T04:33:28.477310Z"
    }
   },
   "outputs": [],
   "source": [
    "with open('lrb_dictionary.json', 'r') as f4:  # load lrb dictioanry json\n",
    "        lrb_dictionary = json.load(f4) \n",
    "\n",
    "# method one \n",
    "# page_dictionary = {}\n",
    "\n",
    "# for index, row in load_processed_corpus_df.iterrows(): \n",
    "#     if isinstance(row['page_identifier'], float):\n",
    "#         continue\n",
    "#     page_dictionary.setdefault(row['page_identifier'],[]).append(index)\n",
    "\n",
    "# def solve_LRB(dictionary):\n",
    "#     for key, value in dictionary.items():\n",
    "#         if '_-LRB' in key:\n",
    "#             if key.split('_-LRB')[0] in dictionary:\n",
    "#                 for v in value:\n",
    "#                     dictionary[key.split('_-LRB')[0]].append(v)\n",
    "\n",
    "#     return dictionary\n",
    "\n",
    "# lrb_dictionary = solve_LRB(page_dictionary)\n",
    "\n",
    "# method two\n",
    "# page_dictionary = {}\n",
    "\n",
    "# for index, row in load_processed_corpus_df.iterrows(): \n",
    "#     if isinstance(row['page_identifier'], float):\n",
    "#         continue\n",
    "#     page_dictionary.setdefault(row['page_identifier'],[]).append(index)\n",
    "\n",
    "# def solve_LRB(dictionary):\n",
    "#     lrb_dictionary = copy.deepcopy(dictionary)\n",
    "#     for key, value in dictionary.items():\n",
    "#         if '_-LRB' in key:\n",
    "#             if key.split('_-LRB')[0] in lrb_dictionary:\n",
    "#                 lrb_dictionary[key.split('_-LRB')[0]].extend(value)\n",
    "#             else:\n",
    "#                 lrb_dictionary.update({\n",
    "#                     key.split('_-LRB')[0]: value\n",
    "#                 })\n",
    "#     return lrb_dictionary\n",
    "\n",
    "# lrb_dictionary = solve_LRB(page_dictionary)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-23T03:04:59.615778Z",
     "start_time": "2019-05-23T03:04:30.444046Z"
    }
   },
   "outputs": [],
   "source": [
    "# with open('lrb_dictionary_new.json', 'w') as f:\n",
    "#     json.dump(lrb_dictionary, f, indent = 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Delete keys in dictionary.\n",
    "Some words in the page identifeir are irrelvant to the evidnece, so we delete some after observation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-26T04:34:06.196652Z",
     "start_time": "2019-05-26T04:34:06.192375Z"
    }
   },
   "outputs": [],
   "source": [
    "delete_words_list = ['The','Part', 'Most','Water', 'How', 'Love','Speech','American','President','German','Irish',\n",
    "                     'Indian','Spanish','Japan','Califorina','Americans','Chinese','British','Monday','Tuesday',\n",
    "                    'Wednesday','Thursday', 'Friday','Saturday', 'Sunday','January','February','March','April',\n",
    "                     'May', 'June', 'July', 'August', 'September', 'October', 'November','December','Russian']\n",
    "\n",
    "for word in delete_words_list:\n",
    "    try:\n",
    "        del lrb_dictionary[word]\n",
    "    except KeyError:\n",
    "        pass\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-26T04:34:28.967472Z",
     "start_time": "2019-05-26T04:34:28.832043Z"
    }
   },
   "outputs": [],
   "source": [
    "# page keys are the page identifier, keys of the page_dictionary\n",
    "page_keys = list(lrb_dictionary.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retrieve the sentence text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-26T04:35:17.928455Z",
     "start_time": "2019-05-26T04:35:17.925028Z"
    },
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# input: a page identifier string, keys are all the keys of the page_dictionary\n",
    "# output: return a list of strings which contains all the relevanted sentence text,also with page_identifier and sentence number so\n",
    "# that it can be written as evidence part.\n",
    "\n",
    "def retrieve_sentenceText(claim_word,page_keys,page_dictionary,df):\n",
    "    retrieved_sentence = []\n",
    "    if claim_word in page_keys:\n",
    "        retrieved_index = page_dictionary[claim_word]      # all indexes in the dataframe\n",
    "        for index in retrieved_index:\n",
    "             # retrieve all the raw doc txt\n",
    "            retrieved_sentence.append(df.loc[index, 'text'])\n",
    "    return retrieved_sentence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rule on Claim: find the Upper cased words.\n",
    "\n",
    "Rule1: First Continuous Upper words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-26T04:35:45.643058Z",
     "start_time": "2019-05-26T04:35:45.638990Z"
    }
   },
   "outputs": [],
   "source": [
    "def find_upper_word(claim, page_keys):\n",
    "    res_list = []\n",
    "    res_index = []      \n",
    "    words = claim.split()\n",
    "    \n",
    "    start = \"\"\n",
    "    temp = \"\"\n",
    "    i = 0\n",
    "    while i < len(words):         \n",
    "        # first step: find uppercase word in the claim\n",
    "        if words[i][0].isupper():\n",
    "            temp = words[i]        \n",
    "            start = temp  # start(as a cache)                        \n",
    "            for j in range(i,len(words)-1):\n",
    "                temp = temp + '_' + words[j+1]\n",
    "#                 print(temp)\n",
    "                if temp in page_keys:\n",
    "                    start = temp  # matchs the word as long as possible\n",
    "                    i = j + 1\n",
    "                \n",
    "                if j - i > 2:\n",
    "                    break\n",
    "                \n",
    "            res_list.append(start)  \n",
    "        i += 1                    \n",
    "    return res_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-26T05:24:28.511358Z",
     "start_time": "2019-05-26T05:24:26.700808Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Home_for_the_Holidays', 'American']\n",
      "['Home_for_the_Holidays', 'The', 'Holly_Hunter']\n"
     ]
    }
   ],
   "source": [
    "# test_claim1 = \"International students come to the University of Mississippi from 90 cities\"\n",
    "# print(find_upper_word(test_claim1,page_keys))\n",
    "# test_claim2 = \"Home for the Holidays stars an American actress\"\n",
    "# print(find_upper_word(test_claim2,page_keys))\n",
    "# test_claim3 = \"Water is part of the History of Earth\"\n",
    "# print(find_upper_word(test_claim3,page_keys))\n",
    "# test_claim4 = \"How to Train Your Dragon 2 used real dragons\"\n",
    "# print(find_upper_word(test_claim4,page_keys))\n",
    "# test_claim5 = \"The Armenian Genocide was the extermination of Armenians who were mostly Turkish citizens.\"\n",
    "# print(find_upper_word(test_claim5,page_keys))\n",
    "# test_claim6 = \"Boyhood is about Mason Evans, Jr's childhood.\"\n",
    "# print(find_upper_word(test_claim6,page_keys))\n",
    "# test_claim7 = \"L.A. Reid has served as the president of a record label.\"\n",
    "# print(find_upper_word(test_claim7,page_keys))\n",
    "# test_claim8 = \"there are no musical or creative works in existence that have been created by Philip Glass\"\n",
    "# print(find_upper_word(test_claim8,page_keys))\n",
    "\n",
    "test_claim9 = \"Home for the Holidays stars an American actress.\"\n",
    "print(find_upper_word(test_claim9,page_keys))\n",
    "test_claim10 = \"Home for the Holidays 1995 film The film stars Holly Hunter\"\n",
    "print(find_upper_word(test_claim10,page_keys))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-20T06:39:29.779459Z",
     "start_time": "2019-05-20T06:39:29.776679Z"
    }
   },
   "outputs": [],
   "source": [
    "# TESTING LRB\n",
    "# print(find_upper_word(\"Savages was exclusively a German film.\"))\n",
    "# print(retrieve_sentenceText(find_upper_word(\"Savages was exclusively a German film.\")[0], page_keys,lrb_dictionary,load_processed_corpus_df))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Method1 : TFIDF and SVD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-23T03:16:47.209650Z",
     "start_time": "2019-05-23T03:16:47.199774Z"
    }
   },
   "outputs": [],
   "source": [
    "def retrieval_evidence_func(query,page_keys,page_dictionary,load_processed_corpus_df):\n",
    "        \n",
    "    res = []\n",
    "    # Determine if the first word in query is \"There\" and \"A\"\n",
    "    if query.split()[0] == \"There\":\n",
    "        query = query.replace(\"There\",\"there\") \n",
    "        \n",
    "    if query.split()[0] == \"A\":\n",
    "        query = query.replace(\"A\",\"a\") \n",
    "    \n",
    "    # remove all 's \n",
    "    for word in query.split():\n",
    "        if (word[-2:len(word)]) == \"'s\":\n",
    "            query = query.replace(word,word[:-2])\n",
    "            \n",
    "    query_corpus = []\n",
    "    if len(find_upper_word(query,page_keys)) >= 1:\n",
    "        for query_word in find_upper_word(query,page_keys):\n",
    "            if query_word[-1:len(query_word)] == \".\":\n",
    "                query_word = query_word[:-1]\n",
    "            if query_word[-1:len(query_word)] == \",\":\n",
    "                query_word = query_word[:-1]\n",
    "            query_corpus.extend(retrieve_sentenceText(query_word, page_keys,page_dictionary,load_processed_corpus_df))\n",
    "        \n",
    "        if len(query_corpus) == 1:\n",
    "            res.append([query_corpus[0].split()[0],int(query_corpus[0].split()[1])])\n",
    "        else:\n",
    "            # build a tfidf model on the query corpus\n",
    "            tfidf_vectorizer = TfidfVectorizer()\n",
    "            tfidf_matrix = tfidf_vectorizer.fit_transform(query_corpus)\n",
    "            tfidf_matrix = tfidf_matrix.T\n",
    "\n",
    "            # apply svd\n",
    "            K= 2 # number of desirable features \n",
    "            U, s, VT = np.linalg.svd(tfidf_matrix.toarray())\n",
    "            #tfidf_reduced = np.dot(U[:,:K], np.dot(np.diag(s[:K]), VT[:K, :]))\n",
    "            terms_rep = np.dot(U[:,:K], np.diag(s[:K]))\n",
    "            docs_rep = np.dot(np.diag(s[:K]), VT[:K, :]).T\n",
    "\n",
    "            # calculate query rep\n",
    "            query_rep = []\n",
    "            for q in pre_process(query).split():\n",
    "                if q in tfidf_vectorizer.vocabulary_:\n",
    "                    query_rep.append(tfidf_vectorizer.vocabulary_[q])\n",
    "                else:\n",
    "                    continue\n",
    "            query_rep = np.mean(terms_rep[query_rep],axis=0)\n",
    "\n",
    "            # calculate cosine similarity between the query and retrieved sentences\n",
    "            query_doc_cos_dist = []\n",
    "            for doc_rep in docs_rep:\n",
    "                query_doc_cos_dist.append(cosine(query_rep, doc_rep))\n",
    "\n",
    "            query_doc_sort_index = np.argsort(np.array(query_doc_cos_dist))\n",
    "            # retrievel top 3\n",
    "            count = 0\n",
    "            for rank, sort_index in enumerate(query_doc_sort_index):\n",
    "                res.append([query_corpus[sort_index].split()[0],int(query_corpus[sort_index].split()[1])])\n",
    "                if count == 3:\n",
    "                    break\n",
    "                else:\n",
    "                    count += 1     \n",
    "        return res\n",
    "    \n",
    "    else:\n",
    "        return []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Method2: BOW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-23T03:17:14.132376Z",
     "start_time": "2019-05-23T03:17:14.124756Z"
    }
   },
   "outputs": [],
   "source": [
    "def retrieval_evidence_func2(query,page_keys,page_dictionary,load_processed_corpus_df):\n",
    "    res = []\n",
    "    # Determine if the first word in query is \"There\" and \"A\"\n",
    "    if query.split()[0] == \"There\":\n",
    "        query = query.replace(\"There\",\"there\") \n",
    "        \n",
    "    if query.split()[0] == \"A\":\n",
    "        query = query.replace(\"A\",\"a\") \n",
    "    \n",
    "    # remove all 's \n",
    "    for word in query.split():\n",
    "        if (word[-2:len(word)]) == \"'s\":\n",
    "            query = query.replace(word,word[:-2])\n",
    "            \n",
    "     # remove all '\n",
    "    for word in query.split():\n",
    "        if (word[-1:len(word)]) == \"'\":\n",
    "            query = query.replace(word,word[:-1])\n",
    "            \n",
    "    query_corpus = []\n",
    "    if len(find_upper_word(query,page_keys)) >= 1:\n",
    "        for query_word in find_upper_word(query,page_keys):\n",
    "            if query_word[-1:len(query_word)] == \".\":\n",
    "                query_word = query_word[:-1]\n",
    "            if query_word[-1:len(query_word)] == \",\":\n",
    "                query_word = query_word[:-1]\n",
    "            query_corpus.extend(retrieve_sentenceText(query_word, page_keys,page_dictionary,load_processed_corpus_df))\n",
    "        \n",
    "        if len(query_corpus) == 1:\n",
    "            res.append([query_corpus[0].split()[0],int(query_corpus[0].split()[1])])\n",
    "        else:\n",
    "            # preprocess claim\n",
    "            processed_query_claim = pre_process(query)\n",
    "            # preprocess evidences\n",
    "            processed_query_corpus = []\n",
    "            for corpus in query_corpus:\n",
    "                processed_query_corpus.append(pre_process(corpus))\n",
    "\n",
    "            count_vectorizer = CountVectorizer()\n",
    "\n",
    "            processed_query_corpus.insert(0, processed_query_claim)\n",
    "\n",
    "            count = count_vectorizer.fit_transform(processed_query_corpus)\n",
    "\n",
    "            query_rep = count[0].todense()\n",
    "            docs_rep = count[1:].todense()\n",
    "\n",
    "            # calculate cosine similarity between the query and retrieved sentences\n",
    "            query_doc_cos_dist = []\n",
    "\n",
    "            # cosine distance, and hence no need to revese argsort result\n",
    "            for doc_rep in docs_rep:\n",
    "                query_doc_cos_dist.append(cosine(query_rep, doc_rep))\n",
    "\n",
    "            query_doc_sort_index = np.argsort(np.array(query_doc_cos_dist))\n",
    "\n",
    "            count = 0\n",
    "            for rank, sort_index in enumerate(query_doc_sort_index):\n",
    "                res.append([query_corpus[sort_index].split()[0],int(query_corpus[sort_index].split()[1])])\n",
    "                if count == 2:\n",
    "                    break\n",
    "                else:\n",
    "                    count += 1   \n",
    "        return res\n",
    "        \n",
    "    else:\n",
    "        return []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Method3 TF IDF with threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-23T06:30:46.266961Z",
     "start_time": "2019-05-23T06:30:46.258792Z"
    }
   },
   "outputs": [],
   "source": [
    "def retrieval_evidence_func3(query,page_keys,page_dictionary,load_processed_corpus_df):\n",
    "    res = []\n",
    "    # Determine if the first word in query is \"There\" and \"A\"\n",
    "    if query.split()[0] == \"There\":\n",
    "        query = query.replace(\"There\",\"there\") \n",
    "        \n",
    "    if query.split()[0] == \"A\":\n",
    "        query = query.replace(\"A\",\"a\") \n",
    "    \n",
    "    # remove all 's \n",
    "    for word in query.split():\n",
    "        if (word[-2:len(word)]) == \"'s\":\n",
    "            query = query.replace(word,word[:-2])\n",
    "            \n",
    "     # remove all '\n",
    "    for word in query.split():\n",
    "        if (word[-1:len(word)]) == \"'\":\n",
    "            query = query.replace(word,word[:-1])\n",
    "            \n",
    "    count = 0\n",
    "    query_corpus = []\n",
    "\n",
    "    if len(find_upper_word(query,page_keys)) >= 1:\n",
    "        for query_word in find_upper_word(query,page_keys):\n",
    "            if query_word[-1:len(query_word)] == \".\":\n",
    "                query_word = query_word[:-1]\n",
    "            if query_word[-1:len(query_word)] == \",\":\n",
    "                query_word = query_word[:-1]\n",
    "                \n",
    "            query_corpus.extend(retrieve_sentenceText(query_word, page_keys,page_dictionary,load_processed_corpus_df))\n",
    "        \n",
    "        if len(query_corpus) == 0:\n",
    "            return query_corpus\n",
    "        \n",
    "        if len(query_corpus) == 1:\n",
    "            res.append([query_corpus[0].split()[0],int(query_corpus[0].split()[1])])\n",
    "        else:\n",
    "            # preprocess claim\n",
    "            processed_query_claim = pre_process(query)\n",
    "            # preprocess evidences\n",
    "            processed_query_corpus = []\n",
    "            for corpus in query_corpus:\n",
    "                processed_query_corpus.append(pre_process(corpus))\n",
    "            \n",
    "            tfidf_vectorizer = TfidfVectorizer()\n",
    "            processed_query_corpus.insert(0, processed_query_claim)\n",
    "\n",
    "            tfidf = tfidf_vectorizer.fit_transform(processed_query_corpus)\n",
    "\n",
    "            query_rep = tfidf[0].todense()\n",
    "            docs_rep = tfidf[1:].todense()\n",
    "\n",
    "            # calculate cosine similarity between the query and retrieved sentences\n",
    "            query_doc_cos_dist = []\n",
    "\n",
    "            # cosine distance, and hence no need to revese argsort result\n",
    "            for doc_rep in docs_rep:\n",
    "                query_doc_cos_dist.append(cosine(query_rep, doc_rep))\n",
    "\n",
    "            query_doc_sort_index = np.argsort(np.array(query_doc_cos_dist))\n",
    "            \n",
    "            count = 0\n",
    "            threshold = 0.0\n",
    "#             best_query_corpus = []\n",
    "            for rank, sort_index in enumerate(query_doc_sort_index):\n",
    "                threshold = query_doc_cos_dist[sort_index] \n",
    "#                 best_query_corpus = [query_corpus[sort_index].split()[0],int(query_corpus[sort_index].split()[1])]\n",
    "                if threshold > 0.78:\n",
    "                    break\n",
    "                else:\n",
    "                    res.append([query_corpus[sort_index].split()[0],int(query_corpus[sort_index].split()[1])])                    \n",
    "                    if count == 2:\n",
    "                        break\n",
    "                    else:\n",
    "                        count += 1 \n",
    "#                 res.append([query_corpus[sort_index].split()[0],int(query_corpus[sort_index].split()[1]),query_doc_cos_dist[sort_index]])\n",
    "            \n",
    "#             if len(res) == 0:\n",
    "#                 res.append(best_query_corpus)\n",
    "                \n",
    "        return res\n",
    "        \n",
    "    else:\n",
    "        return []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Methond4 BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import keras\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout\n",
    "from IPython.display import SVG\n",
    "from keras.utils.vis_utils import model_to_dot\n",
    "from keras.optimizers import Adam\n",
    "import tensorflow as tf\n",
    "from keras.backend.tensorflow_backend import set_session\n",
    "nltk.download('punkt')\n",
    "\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "sess = tf.Session(config=config)\n",
    "\n",
    "from bert_serving.client import BertClient\n",
    "bc = BertClient(check_length=False)\n",
    "\n",
    "# for cased restart server with \n",
    "# bert-serving-start -model_dir /share/ShareFolder/cased_L-24_H-1024_A-16/ -cased_tokenization -max_batch_size=256 -gpu_memory_fraction=0.5 -num_worker=1\n",
    "# for uncased restart server with \n",
    "# bert-serving-start -model_dir /share/ShareFolder/cased_L-24_H-1024_A-16/ -max_batch_size=256 -gpu_memory_fraction=0.9 -num_worker=1\n",
    "\n",
    "def get_keras_model():\n",
    "    model = Sequential()\n",
    "    model.add(Dense(units=200, activation='relu', input_dim=3072))\n",
    "    model.add(Dropout(0.3))\n",
    "    model.add(Dense(units=50, activation='relu', input_dim=3072))\n",
    "    model.add(Dropout(0.3))\n",
    "    model.add(Dense(units=1, activation='sigmoid'))\n",
    "    # optimizer = Adam(lr=0.01)\n",
    "    model.compile(loss=keras.losses.binary_crossentropy,\n",
    "                  optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "    model.load_weights(\"best_weights_sentence.hdf5\")\n",
    "    return model\n",
    "\n",
    "def get_features(claim_list, evidence_list, pair_list):\n",
    "    claim_features = bc.encode(claim_list)\n",
    "    evidence_features = bc.encode(evidence_list)\n",
    "    pair_features = bc.encode(pair_list)\n",
    "    return np.concatenate([claim_features, evidence_features, pair_features], axis=1)\n",
    "\n",
    "keras_model = get_keras_model()\n",
    "\n",
    "\n",
    "def retrieval_evidence_func4(query,page_keys,page_dictionary,load_processed_corpus_df):\n",
    "    res = []\n",
    "    # Determine if the first word in query is \"There\" and \"A\"\n",
    "    if query.split()[0] == \"There\":\n",
    "        query = query.replace(\"There\",\"there\") \n",
    "        \n",
    "    if query.split()[0] == \"A\":\n",
    "        query = query.replace(\"A\",\"a\") \n",
    "    \n",
    "    # remove all 's \n",
    "    for word in query.split():\n",
    "        if (word[-2:len(word)]) == \"'s\":\n",
    "            query = query.replace(word,word[:-2])\n",
    "            \n",
    "     # remove all '\n",
    "    for word in query.split():\n",
    "        if (word[-1:len(word)]) == \"'\":\n",
    "            query = query.replace(word,word[:-1])\n",
    "            \n",
    "    # remove all .\n",
    "    for word in query.split():\n",
    "        if (word[-1:len(word)]) == \".\":\n",
    "            query = query.replace(word,word[:-1])\n",
    "    \n",
    "    for word in query.split():\n",
    "        if (word[-1:len(word)]) == \",\":\n",
    "            query = query.replace(word,word[:-1])\n",
    "    \n",
    "    count = 0\n",
    "    query_corpus = []\n",
    "    for query_word in find_upper_word(query,page_keys):\n",
    "        query_corpus.extend(retrieve_sentenceText(query_word, page_keys,page_dictionary,load_processed_corpus_df))\n",
    "\n",
    "    processed_query_claim = pre_process(query)\n",
    "    # preprocess evidences\n",
    "    claim_list = []\n",
    "    evidence_list = []\n",
    "    pair_list = []\n",
    "    \n",
    "    if len(query_corpus) == 0:\n",
    "        return res\n",
    "    \n",
    "    for evidence in query_corpus:\n",
    "        claim_list.append(processed_query_claim)\n",
    "        evidence_list.append(evidence)\n",
    "        pair_list.append(processed_query_claim + \" ||| \" + evidence)\n",
    "\n",
    "    feature_matrix = get_features(claim_list, evidence_list, pair_list)\n",
    "    labels = keras_model.predict(feature_matrix, batch_size=32, verbose=0)\n",
    "    probabilities = []\n",
    "    for label in labels:\n",
    "        probabilities.append(label[0])\n",
    "\n",
    "    # 排序, desending order\n",
    "    query_doc_sort_index = np.argsort(probabilities)\n",
    "    query_doc_sort_index[::-1]   # reverse\n",
    "\n",
    "    count = 0\n",
    "    for sort_index in query_doc_sort_index:\n",
    "        if probabilities[sort_index] > 0.5:\n",
    "            res.append([query_corpus[sort_index].split()[0],int(query_corpus[sort_index].split()[1])])\n",
    "            count += 1\n",
    "            if count >= 5:\n",
    "                break\n",
    "\n",
    "    return res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Method5  TFIDF with new rules on brackets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-23T07:06:27.467540Z",
     "start_time": "2019-05-23T07:06:27.322099Z"
    }
   },
   "outputs": [],
   "source": [
    "def retrieval_evidence_func5(query,page_keys,page_dictionary,load_processed_corpus_df):\n",
    "    res = []\n",
    "    # Determine if the first word in query is \"There\" and \"A\"\n",
    "    if query.split()[0] == \"There\":\n",
    "        query = query.replace(\"There\",\"there\") \n",
    "        \n",
    "    if query.split()[0] == \"A\":\n",
    "        query = query.replace(\"A\",\"a\") \n",
    "    \n",
    "    # remove all 's \n",
    "    for word in query.split():\n",
    "        if (word[-2:len(word)]) == \"'s\":\n",
    "            query = query.replace(word,word[:-2])\n",
    "            \n",
    "     # remove all '\n",
    "    for word in query.split():\n",
    "        if (word[-1:len(word)]) == \"'\":\n",
    "            query = query.replace(word,word[:-1])\n",
    "            \n",
    "    query_corpus = []\n",
    "    if len(find_upper_word(query,page_keys)) >= 1:\n",
    "        for query_word in find_upper_word(query,page_keys):\n",
    "            if query_word[-1:len(query_word)] == \".\":\n",
    "                query_word = query_word[:-1]\n",
    "            if query_word[-1:len(query_word)] == \",\":\n",
    "                query_word = query_word[:-1]\n",
    "            \n",
    "            if query_word in page_keys:\n",
    "                retrieved_index = lrb_dictionary[query_word]\n",
    "\n",
    "                for index in retrieved_index:\n",
    "                    original_head = load_processed_corpus_df.loc[index, 'page_identifier'] \n",
    "                    if '-LRB' in original_head:\n",
    "                        head_list = original_head[original_head.find('-LRB-')+len('-LRB-'):original_head.rfind('-RRB-')].split('_')\n",
    "                        \n",
    "                        for head in head_list:\n",
    "                            if head in query:\n",
    "                                query_corpus.append((load_processed_corpus_df.loc[index, 'text']))\n",
    "                                break\n",
    "                            else:\n",
    "                                continue\n",
    "                    else:\n",
    "                        query_corpus.append((load_processed_corpus_df.loc[index, 'text']))\n",
    "        \n",
    "            \n",
    "        if len(query_corpus) == 1:\n",
    "            res.append([query_corpus[0].split()[0],int(query_corpus[0].split()[1])])\n",
    "        else:\n",
    "            # preprocess claim\n",
    "            processed_query_claim = pre_process(query)\n",
    "            # preprocess evidences\n",
    "            processed_query_corpus = []\n",
    "            for corpus in query_corpus:\n",
    "                processed_query_corpus.append(pre_process(corpus))\n",
    "            \n",
    "            tfidf_vectorizer = TfidfVectorizer()\n",
    "            processed_query_corpus.insert(0, processed_query_claim)\n",
    "\n",
    "            tfidf = tfidf_vectorizer.fit_transform(processed_query_corpus)\n",
    "\n",
    "            query_rep = tfidf[0].todense()\n",
    "            docs_rep = tfidf[1:].todense()\n",
    "\n",
    "            # calculate cosine similarity between the query and retrieved sentences\n",
    "            query_doc_cos_dist = []\n",
    "           # cosine distance, and hence no need to revese argsort result\n",
    "            for doc_rep in docs_rep:\n",
    "                query_doc_cos_dist.append(cosine(query_rep, doc_rep))\n",
    "            \n",
    "            query_doc_sort_index = np.argsort(np.array(query_doc_cos_dist))\n",
    "            \n",
    "            count = 0\n",
    "            threshold = 0.0\n",
    "            for rank, sort_index in enumerate(query_doc_sort_index):\n",
    "                threshold = query_doc_cos_dist[sort_index] \n",
    "                if threshold > 0.78:\n",
    "                    break\n",
    "                else:\n",
    "                    res.append([query_corpus[sort_index].split()[0],int(query_corpus[sort_index].split()[1])])                    \n",
    "                    if count == 2:\n",
    "                        break\n",
    "                    else:\n",
    "                        count += 1 \n",
    "\n",
    "        return res\n",
    "        \n",
    "    else:\n",
    "        return []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-25T12:52:59.713174Z",
     "start_time": "2019-05-25T12:52:59.681203Z"
    }
   },
   "outputs": [],
   "source": [
    "from math import*\n",
    " \n",
    "def euclidean_distance(x,y):\n",
    "     return sqrt(sum(pow(a-b,2) for a, b in zip(x, y)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Method 6: Pure TF IDF "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-26T04:36:13.789755Z",
     "start_time": "2019-05-26T04:36:13.780977Z"
    }
   },
   "outputs": [],
   "source": [
    "def retrieval_evidence_func6(query,page_keys,page_dictionary,load_processed_corpus_df):\n",
    "    res = []\n",
    "    # Determine if the first word in query is \"There\" and \"A\"\n",
    "    if query.split()[0] == \"There\":\n",
    "        query = query.replace(\"There\",\"there\") \n",
    "        \n",
    "    if query.split()[0] == \"A\":\n",
    "        query = query.replace(\"A\",\"a\") \n",
    "    \n",
    "    # remove all 's \n",
    "    for word in query.split():\n",
    "        if (word[-2:len(word)]) == \"'s\":\n",
    "            query = query.replace(word,word[:-2])\n",
    "            \n",
    "     # remove all '\n",
    "    for word in query.split():\n",
    "        if (word[-1:len(word)]) == \"'\":\n",
    "            query = query.replace(word,word[:-1])\n",
    "        \n",
    "    query_corpus = []\n",
    "    if len(find_upper_word(query,page_keys)) >= 1:\n",
    "        for query_word in find_upper_word(query,page_keys):\n",
    "            if query_word[-1:len(query_word)] == \".\":\n",
    "                query_word = query_word[:-1]\n",
    "            if query_word[-1:len(query_word)] == \",\":\n",
    "                query_word = query_word[:-1]\n",
    "            \n",
    "            query_corpus.extend(retrieve_sentenceText(query_word, page_keys,page_dictionary,load_processed_corpus_df))\n",
    "        \n",
    "        if len(query_corpus) == 1:\n",
    "            res.append([query_corpus[0].split()[0],int(query_corpus[0].split()[1])])\n",
    "        else:\n",
    "            # preprocess claim\n",
    "            processed_query_claim = pre_process(query)\n",
    "            # preprocess evidences\n",
    "            processed_query_corpus = []\n",
    "            for corpus in query_corpus:\n",
    "                processed_query_corpus.append(pre_process(corpus))\n",
    "            \n",
    "            tfidf_vectorizer = TfidfVectorizer(ngram_range=(1,2))\n",
    "            processed_query_corpus.insert(0, processed_query_claim)\n",
    "\n",
    "            tfidf = tfidf_vectorizer.fit_transform(processed_query_corpus)\n",
    "\n",
    "            query_rep = tfidf[0].todense()\n",
    "            docs_rep = tfidf[1:].todense()\n",
    "                        # calculate cosine similarity between the query and retrieved sentences\n",
    "            query_doc_cos_dist = []\n",
    "\n",
    "            # cosine distance, and hence no need to revese argsort result\n",
    "            for doc_rep in docs_rep:\n",
    "                query_doc_cos_dist.append(cosine(query_rep, doc_rep))\n",
    "#                 query_doc_cos_dist.append(euclidean_distance(query_rep, doc_rep))\n",
    "                \n",
    "            query_doc_sort_index = np.argsort(np.array(query_doc_cos_dist))\n",
    "\n",
    "            count = 0\n",
    "            for rank, sort_index in enumerate(query_doc_sort_index):\n",
    "                res.append([query_corpus[sort_index].split()[0],int(query_corpus[sort_index].split()[1])])\n",
    "                if count == 4:\n",
    "                    break\n",
    "                else:\n",
    "                    count += 1   \n",
    "        return res\n",
    "        \n",
    "    else:\n",
    "        return []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-26T03:46:26.817659Z",
     "start_time": "2019-05-26T03:44:45.697795Z"
    }
   },
   "outputs": [],
   "source": [
    "# testing\n",
    "test_claim1 = \"Brad Wilk helped co-found Rage in 1962.\"\n",
    "test_claim2 = \"The Faroe Islands are no longer part of the Kingdom of Mercia.\"\n",
    "test_claim3 = \"Down With Love is a 2003 comedy film.\"\n",
    "test_claim4 = \"Telemundo is a English-language television network.\"\n",
    "# To test LRB\n",
    "test_claim5 = 'Hourglass is performed by a Russian singer-songwriter.'\n",
    "# To test first word \"There\"\n",
    "test_claim6 = \"There are no musical or creative works in existence that have been created by Phillip Glass.\"\n",
    "# To test 's ' in the word\n",
    "test_claim7 = \"Damon Albarn's debut album was released in 2011.\"\n",
    "\n",
    "test_claim8 = \"Temple of the Dog celebrated the 37th anniversary of their self-titled album.\"\n",
    "test_claim9 = \"Savages was exclusively a German film.\"\n",
    "test_claim10 = \"L.A. Reid has served as the president of a record label.\"\n",
    "test_claim11 = \"Bermuda Triangle is in the western part of the Himalayas.\"\n",
    "\n",
    "print(retrieval_evidence_func6(test_claim1,page_keys,lrb_dictionary,load_processed_corpus_df))\n",
    "print(retrieval_evidence_func6(test_claim2,page_keys,lrb_dictionary,load_processed_corpus_df))\n",
    "print(retrieval_evidence_func6(test_claim3,page_keys,lrb_dictionary,load_processed_corpus_df))\n",
    "print(retrieval_evidence_func6(test_claim4,page_keys,lrb_dictionary,load_processed_corpus_df))\n",
    "print(retrieval_evidence_func6(test_claim5,page_keys,lrb_dictionary,load_processed_corpus_df))\n",
    "print(retrieval_evidence_func6(test_claim6,page_keys,lrb_dictionary,load_processed_corpus_df))\n",
    "print(retrieval_evidence_func6(test_claim7,page_keys,lrb_dictionary,load_processed_corpus_df))\n",
    "print(retrieval_evidence_func6(test_claim8,page_keys,lrb_dictionary,load_processed_corpus_df))\n",
    "print(retrieval_evidence_func6(test_claim9,page_keys,lrb_dictionary,load_processed_corpus_df))\n",
    "print(retrieval_evidence_func6(test_claim10,page_keys,lrb_dictionary,load_processed_corpus_df))\n",
    "print(retrieval_evidence_func6(test_claim11,page_keys,lrb_dictionary,load_processed_corpus_df))\n",
    "\n",
    "print(\"\\n\")\n",
    "\n",
    "print(retrieval_evidence_func7(test_claim1,page_keys,lrb_dictionary,load_processed_corpus_df))\n",
    "print(retrieval_evidence_func7(test_claim2,page_keys,lrb_dictionary,load_processed_corpus_df))\n",
    "print(retrieval_evidence_func7(test_claim3,page_keys,lrb_dictionary,load_processed_corpus_df))\n",
    "print(retrieval_evidence_func7(test_claim4,page_keys,lrb_dictionary,load_processed_corpus_df))\n",
    "print(retrieval_evidence_func7(test_claim5,page_keys,lrb_dictionary,load_processed_corpus_df))\n",
    "print(retrieval_evidence_func7(test_claim6,page_keys,lrb_dictionary,load_processed_corpus_df))\n",
    "print(retrieval_evidence_func7(test_claim7,page_keys,lrb_dictionary,load_processed_corpus_df))\n",
    "print(retrieval_evidence_func7(test_claim8,page_keys,lrb_dictionary,load_processed_corpus_df))\n",
    "print(retrieval_evidence_func7(test_claim9,page_keys,lrb_dictionary,load_processed_corpus_df))\n",
    "print(retrieval_evidence_func7(test_claim10,page_keys,lrb_dictionary,load_processed_corpus_df))\n",
    "print(retrieval_evidence_func7(test_claim11,page_keys,lrb_dictionary,load_processed_corpus_df))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Write result to  devset file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-26T03:43:28.725457Z",
     "start_time": "2019-05-26T03:39:51.596147Z"
    }
   },
   "outputs": [],
   "source": [
    "global_test_data=Manager().dict(res_data)   \n",
    "\n",
    "i = multiprocessing.Value(\"i\",0)\n",
    "def deal_dataset(key):\n",
    "    detail_dict = global_test_data[key]\n",
    "    detail_dict[\"evidence\"] = []\n",
    "    query = detail_dict[\"claim\"]\n",
    "#     if detail_dict[\"label\"] == \"NOT ENOUGH INFO\":\n",
    "#         detail_dct[\"evidence\"] = []\n",
    "#     else:\n",
    "#     detail_dict[\"evidence\"] = retrieval_evidence_func3(query,page_keys,lrb_dictionary,load_processed_corpus_df)\n",
    "\n",
    "    detail_dict[\"evidence\"] = retrieval_evidence_func6(query,page_keys,lrb_dictionary,load_processed_corpus_df)\n",
    "    global_test_data[key] = detail_dict\n",
    "    i.value += 1\n",
    "    print(i.value)\n",
    "\n",
    "pool=Pool(processes = 10)\n",
    "for key in list(res_data):\n",
    "    pool.apply_async(deal_dataset, args=(key,))\n",
    "\n",
    "pool.close()\n",
    "pool.join()  \n",
    "\n",
    "with open('result_dev_526_1_tfidf.json', 'w') as f:\n",
    "    json.dump(dict(global_test_data), f, indent = 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Write result to test file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-24T12:43:59.121018Z",
     "start_time": "2019-05-24T11:15:42.411474Z"
    }
   },
   "outputs": [],
   "source": [
    "global_test_data=Manager().dict(test_data)    \n",
    "\n",
    "i = multiprocessing.Value(\"i\",0)\n",
    "def deal_dataset(key):\n",
    "    detail_dict = global_test_data[key]\n",
    "    detail_dict[\"evidence\"] = []\n",
    "    query = detail_dict[\"claim\"]\n",
    "    detail_dict[\"evidence\"] = retrieval_evidence_func6(query,page_keys,lrb_dictionary,load_processed_corpus_df)\n",
    "    global_test_data[key] = detail_dict\n",
    "    i.value += 1\n",
    "    print(i.value)\n",
    "\n",
    "pool=Pool(processes = 10)\n",
    "for key in list(test_data):\n",
    "    pool.apply_async(deal_dataset, args=(key,))\n",
    "\n",
    "pool.close()\n",
    "pool.join()  \n",
    "\n",
    "with open('result_test_524_tfidf5.json', 'w') as f:\n",
    "    json.dump(dict(global_test_data), f, indent = 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Write training data result to file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-22T10:00:08.741196Z",
     "start_time": "2019-05-22T10:00:07.829079Z"
    }
   },
   "outputs": [],
   "source": [
    "with open('train.json', 'r') as f9:  # load dev dataset\n",
    "        train_data = json.load(f9) \n",
    "print(\"Length of the train data is: \" + str(len(train_data)))\n",
    "\n",
    "with open('train_result.json', 'r') as f8:  # store result \n",
    "        train_res_data = json.load(f8) \n",
    "print(\"Length of the train result data is: \" + str(len(train_res_data)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-22T11:33:44.182412Z",
     "start_time": "2019-05-22T10:12:47.795612Z"
    }
   },
   "outputs": [],
   "source": [
    "global_test_data=Manager().dict(train_res_data)   \n",
    "\n",
    "i = multiprocessing.Value(\"i\",0)\n",
    "def deal_dataset(key):\n",
    "    detail_dict = global_test_data[key]\n",
    "    detail_dict[\"evidence\"] = []\n",
    "    query = detail_dict[\"claim\"]\n",
    "#     if detail_dict[\"label\"] == \"NOT ENOUGH INFO\":\n",
    "#         detail_dct[\"evidence\"] = []\n",
    "#     else:\n",
    "#     detail_dict[\"evidence\"] = retrieval_evidence_func3(query,page_keys,lrb_dictionary,load_processed_corpus_df)\n",
    "\n",
    "    detail_dict[\"evidence\"] = retrieval_evidence_func5(query,page_keys,lrb_dictionary,load_processed_corpus_df)\n",
    "    global_test_data[key] = detail_dict\n",
    "    i.value += 1\n",
    "    print(i.value)\n",
    "\n",
    "pool=Pool(processes = 10)\n",
    "# first 50000 in train data\n",
    "for key in list(train_res_data)[:50000]:\n",
    "    pool.apply_async(deal_dataset, args=(key,))\n",
    "\n",
    "pool.close()\n",
    "pool.join()  \n",
    "\n",
    "with open('result_train_522_tfidf3.json', 'w') as f:\n",
    "    json.dump(dict(global_test_data), f, indent = 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "251px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
