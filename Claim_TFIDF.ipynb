{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load processed dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-17T13:30:44.234560Z",
     "start_time": "2019-05-17T13:27:16.922003Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(25248397, 3)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>page_identifier</th>\n",
       "      <th>sentence_number</th>\n",
       "      <th>sentence_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Bedadi,_Ethiopia</td>\n",
       "      <td>0</td>\n",
       "      <td>bedadi be a village in south western ethiopia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Bedadi,_Ethiopia</td>\n",
       "      <td>1</td>\n",
       "      <td>locate in seka chekorsa a woreda in the jimma ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Bedadi,_Ethiopia</td>\n",
       "      <td>4</td>\n",
       "      <td>the central statistical agency have not publis...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Bedadi,_Ethiopia</td>\n",
       "      <td>7</td>\n",
       "      <td>category populate place in the oromia region</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Big_Apple_Circus</td>\n",
       "      <td>0</td>\n",
       "      <td>the big apple circus be a circus base in new y...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Big_Apple_Circus</td>\n",
       "      <td>1</td>\n",
       "      <td>open in 1977 later become a nonprofit organiza...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Big_Apple_Circus</td>\n",
       "      <td>2</td>\n",
       "      <td>the circus have be know for it community outre...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Big_Apple_Circus</td>\n",
       "      <td>3</td>\n",
       "      <td>big apple circus file for chapter 11 bankruptc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Big_Apple_Circus</td>\n",
       "      <td>4</td>\n",
       "      <td>the circus will be renew in october 2017 for i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Bayete</td>\n",
       "      <td>0</td>\n",
       "      <td>bayete may refer to</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    page_identifier sentence_number  \\\n",
       "0  Bedadi,_Ethiopia               0   \n",
       "1  Bedadi,_Ethiopia               1   \n",
       "2  Bedadi,_Ethiopia               4   \n",
       "3  Bedadi,_Ethiopia               7   \n",
       "4  Big_Apple_Circus               0   \n",
       "5  Big_Apple_Circus               1   \n",
       "6  Big_Apple_Circus               2   \n",
       "7  Big_Apple_Circus               3   \n",
       "8  Big_Apple_Circus               4   \n",
       "9            Bayete               0   \n",
       "\n",
       "                                       sentence_text  \n",
       "0      bedadi be a village in south western ethiopia  \n",
       "1  locate in seka chekorsa a woreda in the jimma ...  \n",
       "2  the central statistical agency have not publis...  \n",
       "3       category populate place in the oromia region  \n",
       "4  the big apple circus be a circus base in new y...  \n",
       "5  open in 1977 later become a nonprofit organiza...  \n",
       "6  the circus have be know for it community outre...  \n",
       "7  big apple circus file for chapter 11 bankruptc...  \n",
       "8  the circus will be renew in october 2017 for i...  \n",
       "9                                bayete may refer to  "
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "load_processed_corpus_df = pd.read_pickle(\"./processed_corpus.pkl\")\n",
    "print(load_processed_corpus_df.shape)\n",
    "load_processed_corpus_df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build a dictionary to store index in dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-17T14:01:00.004799Z",
     "start_time": "2019-05-17T13:41:13.820170Z"
    }
   },
   "outputs": [],
   "source": [
    "page_dictionary = {}\n",
    "\n",
    "for index, row in load_processed_corpus_df.iterrows(): \n",
    "    page_dictionary.setdefault(row['page_identifier'],[]).append(index)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-17T14:25:17.459194Z",
     "start_time": "2019-05-17T14:25:17.176599Z"
    }
   },
   "outputs": [],
   "source": [
    "# page keys are the page identifier, keys of the page_dictionary\n",
    "page_keys = list(page_dictionary.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-17T14:25:19.785383Z",
     "start_time": "2019-05-17T14:25:19.781646Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of keys = 5396106\n",
      "['Bedadi,_Ethiopia', 'Big_Apple_Circus', 'Bayete', 'Barkhalbina', 'Bihovo', 'Barry_Ditewig', 'Bawbee', 'Battle_of_Kenapacomaqua', 'Bertram,_California', 'Benjamin_D._Pritchard']\n",
      "[4, 5, 6, 7, 8]\n",
      "[15590516, 15590517, 15590518, 15590519, 15590520, 15590521, 15590522, 15590523, 15590524, 15590525, 15590526, 15590527, 15590528, 15590529, 15590530, 15590531, 15590532, 15590533]\n"
     ]
    }
   ],
   "source": [
    "# testing\n",
    "print(\"Length of keys = {}\".format(len(page_keys)))\n",
    "print(page_keys[:10])\n",
    "print(page_dictionary['Big_Apple_Circus'])\n",
    "print(page_dictionary['Alexander_McNair'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use dictionary to retrieve the sentence text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-17T14:51:18.300770Z",
     "start_time": "2019-05-17T14:51:18.296784Z"
    }
   },
   "outputs": [],
   "source": [
    "# input: a page identifier string, keys are all the keys of the page_dictionary\n",
    "# output: return a list of strings which contains all the relevanted sentence text\n",
    "\n",
    "def retrieve_sentenceText(claim_word,page_keys,page_dictionary,df):\n",
    "    retrieved_sentence = []\n",
    "    if claim_word in page_keys:\n",
    "        retrieved_index = page_dictionary[claim_word]      # all indexes in the dataframe\n",
    "        for index in retrieved_index:\n",
    "            retrieved_sentence.append(df.loc[index, 'sentence_text'])  # retrieve all the raw doc txt\n",
    "    return retrieved_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-17T14:51:20.918839Z",
     "start_time": "2019-05-17T14:51:20.741518Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['the big apple circus be a circus base in new york city', 'open in 1977 later become a nonprofit organization it become a tourist attraction', 'the circus have be know for it community outreach program include clown care a well a it humane treatment of animal', 'big apple circus file for chapter 11 bankruptcy protection in november 2016 and exit bankruptcy in february 2017 after it asset be buy by compass partner', 'the circus will be renew in october 2017 for it 40th anniversary season']\n"
     ]
    }
   ],
   "source": [
    "print(retrieve_sentenceText('Big_Apple_Circus', page_keys,page_dictionary,load_processed_corpus_df))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load devset dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-17T14:52:00.713991Z",
     "start_time": "2019-05-17T14:52:00.354712Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of the dev data is: 5001\n",
      "Length of the dev result data is: 5001\n",
      "Length of the test data is: 14997\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "\n",
    "with open('devset.json', 'r') as f1:  # load dev dataset\n",
    "        dev_data = json.load(f1) \n",
    "print(\"Length of the dev data is: \" + str(len(dev_data)))\n",
    "\n",
    "with open('devset_result.json', 'r') as f2:  # store result \n",
    "        res_data = json.load(f2) \n",
    "print(\"Length of the dev result data is: \" + str(len(res_data)))\n",
    "\n",
    "with open('test-unlabelled.json', 'r') as f3:  # store result \n",
    "     test_data = json.load(f3) \n",
    "print(\"Length of the test data is: \" + str(len(test_data)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build TF IDF Vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-17T15:09:25.758985Z",
     "start_time": "2019-05-17T15:01:18.604845Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:08:04.874127\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import pickle\n",
    "import datetime\n",
    "import gc\n",
    "\n",
    "gc.collect()\n",
    "start1 = datetime.datetime.now()\n",
    "\n",
    "tfidf_vectorizer = TfidfVectorizer(max_features = 5000)\n",
    "tfidf_list = tfidf_vectorizer.fit(load_processed_corpus_df['sentence_text'].tolist())\n",
    "\n",
    "pickle.dump(tfidf_list, open(\"tfidf5000list.pickle\",\"wb\"))\n",
    "\n",
    "end1 = datetime.datetime.now()\n",
    "print(end1-start1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rule on Claim: find the Upper cased words.\n",
    "\n",
    "Rule1: First Continuous Upper words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-17T16:20:28.260988Z",
     "start_time": "2019-05-17T16:20:28.256165Z"
    }
   },
   "outputs": [],
   "source": [
    "from itertools import groupby\n",
    "\n",
    "def ranges(lst):\n",
    "    pos = (j - i for i, j in enumerate(lst))\n",
    "    t = 0\n",
    "    for i, els in groupby(pos):\n",
    "        l = len(list(els))\n",
    "        el = lst[t]\n",
    "        t += l\n",
    "        yield list(range(el, el+l))\n",
    "\n",
    "def find_upper_word(claim):\n",
    "    res = \"\"\n",
    "    res_index = []      \n",
    "    words = claim.split()\n",
    "    for index, word in enumerate(words):\n",
    "        if word[0].isupper():\n",
    "            res_index.append(index)\n",
    "        \n",
    "    for group in list(ranges(res_index)):\n",
    "        # find the first group of upper case continuous words\n",
    "        if len(group) >= 2:\n",
    "            for i in group:\n",
    "                res = res + '_' + words[i]\n",
    "            return res[1:]\n",
    "    # return null if no two or more continuous upper words\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-17T16:20:47.209363Z",
     "start_time": "2019-05-17T16:20:47.204692Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Brad_Wilk\n",
      "The_Faroe_Islands\n"
     ]
    }
   ],
   "source": [
    "test_claim = \"Brad Wilk helped co-found Rage in 1962.\"\n",
    "print(find_upper_word(test_claim))\n",
    "test_claim2 = \"The Faroe Islands are no longer part of the Kingdom of Mercia.\"\n",
    "print(find_upper_word(test_claim2))\n",
    "test_claim3 = \"Down With Love is a 2003 comedy film.\"\n",
    "print(find_upper_word(test_claim2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
