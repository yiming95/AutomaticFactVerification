{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict Label with extracted evidence texts\n",
    "This notebook builds the MLP model for RTM step according to the FNC competition paper.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data preprocess"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load data as pandas DF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-04T08:59:16.326577Z",
     "start_time": "2019-05-04T08:59:13.628198Z"
    }
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "file_path = \"./JSONFiles/\"\n",
    "\n",
    "with open(file=file_path+\"train_with_text.json\", mode='r') as f:\n",
    "    train = json.load(f)\n",
    "\n",
    "train_list = []\n",
    "for key in train.keys():\n",
    "    record = train.get(key)\n",
    "    claim = record.get(\"claim\")\n",
    "    evi_texts = record.get(\"evidence_texts\")\n",
    "    text = \"\"\n",
    "    for evi in evi_texts:\n",
    "        text += evi\n",
    "    SUP = NOINFO = REF = 0\n",
    "    if record.get(\"label\") == \"SUPPORTS\":\n",
    "        SUP = 1\n",
    "    elif record.get(\"label\") == \"REFUTES\":\n",
    "        REF = 1\n",
    "    else:\n",
    "        NOINFO = 1\n",
    "    train_record = {\n",
    "        \"claim\": claim,\n",
    "        \"evi_text\": text,\n",
    "        \"SUP\": SUP,\n",
    "        \"NOINFO\": NOINFO,\n",
    "        \"REF\": REF\n",
    "    }\n",
    "    train_list.append(train_record)\n",
    "\n",
    "train_df = pd.DataFrame(train_list)\n",
    "\n",
    "train_df[0: 10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tokenization and Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-04T09:00:20.196873Z",
     "start_time": "2019-05-04T08:59:16.328711Z"
    }
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "\n",
    "tokenizer = nltk.tokenize.RegexpTokenizer(r'\\w+')\n",
    "lemmatizer = nltk.stem.WordNetLemmatizer()\n",
    "\n",
    "def lemmatize(word):\n",
    "    lemma = lemmatizer.lemmatize(word,'v')\n",
    "    if lemma == word:\n",
    "        lemma = lemmatizer.lemmatize(word,'n')\n",
    "    return lemma\n",
    "\n",
    "def pre_process(comment) -> str:\n",
    "    # lower cased\n",
    "    comment = comment.lower()\n",
    "    # tokenize\n",
    "    words = tokenizer.tokenize(comment)\n",
    "    # lemmatize \n",
    "    words = [lemmatize(w) for w in words]\n",
    "    # remove stop words\n",
    "#     stop_words = nltk.corpus.stopwords.words('english')\n",
    "#     words = [w for w in words if not w in stop_words]\n",
    "    # return result\n",
    "    processed_comment = \" \".join(words)\n",
    "    return processed_comment\n",
    "\n",
    "corpus = pd.concat([train_df['claim'], train_df['evi_text']])\n",
    "processed_corpus = corpus.apply(lambda text: pre_process(text))\n",
    "train_df['claim'] = processed_corpus.iloc[0: len(train)]\n",
    "train_df['evi_text'] = processed_corpus.iloc[len(train):,]\n",
    "\n",
    "train_df[0: 10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TF Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-04T09:00:28.929081Z",
     "start_time": "2019-05-04T09:00:20.228998Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from scipy.sparse import coo_matrix, hstack\n",
    "\n",
    "max_features = 5000 \n",
    "\n",
    "claim_tf_vectorizer = CountVectorizer(max_features=max_features)\n",
    "claim_tf = claim_tf_vectorizer.fit_transform(train_df['claim'])\n",
    "evi_text_tf_vectorizer = CountVectorizer(max_features=max_features)\n",
    "evi_text_tf = evi_text_tf_vectorizer.fit_transform(train_df['evi_text'])\n",
    "tf_features = hstack([claim_tf, evi_text_tf])\n",
    "\n",
    "tf_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TF_IDF Cosine similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-04T09:28:57.711545Z",
     "start_time": "2019-05-04T09:28:00.043205Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "max_features = 5000\n",
    "\n",
    "claim_tfidf_vectorizer = TfidfVectorizer(max_features=max_features, norm='l2')\n",
    "claim_tfidf = claim_tf_vectorizer.fit_transform(train_df['claim'])\n",
    "evi_tfidf_vectorizer = TfidfVectorizer(max_features=max_features, norm='l2')\n",
    "evi_tfidf = evi_text_tf_vectorizer.fit_transform(train_df['evi_text'])\n",
    "\n",
    "cosines = np.zeros((claim_tfidf.shape[0], 1))\n",
    "for i in range(len(cosines)):\n",
    "    claim_vector = claim_tfidf[i]\n",
    "    evi_vector = evi_tfidf[i]\n",
    "    cosine_matrix = cosine_similarity([claim_vector.toarray()[0], evi_vector.toarray()[0]])\n",
    "    cosines[i][0] = cosine_matrix[0][1]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-04T09:25:00.223600Z",
     "start_time": "2019-05-04T09:25:00.220788Z"
    }
   },
   "source": [
    "#### Concat features together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-04T10:03:35.489763Z",
     "start_time": "2019-05-04T10:03:30.099779Z"
    }
   },
   "outputs": [],
   "source": [
    "x_train = hstack([tf_features, cosines]).toarray()\n",
    "y_train = train_df[train_df.columns[0:3]].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build and Train model\n",
    "Build an MLP with tensor (10001, 1) as input, 1 hidden layer with 100 neurons, and softmax layer for output. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-04T09:55:14.505691Z",
     "start_time": "2019-05-04T09:55:14.403761Z"
    }
   },
   "outputs": [],
   "source": [
    "# import keras\n",
    "# from keras.layers import Dense, Input, LSTM, Embedding, Dropout, Activation\n",
    "# from keras.layers import Bidirectional, GlobalMaxPool1D\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from IPython.display import SVG\n",
    "from keras.utils.vis_utils import model_to_dot\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(units=64, activation='relu', input_dim=x_train.shape[1]))\n",
    "model.add(Dense(units=3, activation='softmax'))\n",
    "model.compile(loss=keras.losses.categorical_crossentropy,\n",
    "              optimizer=keras.optimizers.SGD(lr=0.01, momentum=0.9, nesterov=True))\n",
    "\n",
    "model.summary()\n",
    "SVG(model_to_dot(model).create(prog='dot', format='svg'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-04T10:07:33.287906Z",
     "start_time": "2019-05-04T10:07:30.461202Z"
    }
   },
   "outputs": [],
   "source": [
    "model.fit(x=x_train, y=y_train, batch_size=128, epochs=1, validation_split=0.1)\n",
    "# x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-04-29T10:39:45.256150Z",
     "start_time": "2019-04-29T10:39:45.252494Z"
    }
   },
   "source": [
    "## Apply model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Apply model on dev data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Apply model on test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
