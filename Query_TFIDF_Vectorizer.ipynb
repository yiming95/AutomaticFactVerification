{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-17T06:42:09.501500Z",
     "start_time": "2019-05-17T06:40:40.673418Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of the train data is: 145449\n",
      "Length of the dev data is: 5001\n",
      "Length of the dev result data is: 5001\n",
      "Length of the test data is: 14997\n",
      "Length of the corpus is: 25248397\n",
      "Length of the corpus is: 25248397\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "with open('train.json', 'r') as f:  # load training dataset\n",
    "        train_data = json.load(f)   \n",
    "print(\"Length of the train data is: \" + str(len(train_data)))\n",
    "\n",
    "with open('devset.json', 'r') as f1:  # load dev dataset\n",
    "        dev_data = json.load(f1) \n",
    "print(\"Length of the dev data is: \" + str(len(dev_data)))\n",
    "\n",
    "with open('devset_result.json', 'r') as f2:  # store result \n",
    "        res_data = json.load(f2) \n",
    "print(\"Length of the dev result data is: \" + str(len(res_data)))\n",
    "\n",
    "with open('test-unlabelled.json', 'r') as f3:  # store result \n",
    "     test_data = json.load(f3) \n",
    "print(\"Length of the test data is: \" + str(len(test_data)))\n",
    "        \n",
    "# appeand all the wiki txt sentences to one document\n",
    "def loadfile(folder): \n",
    "    corpus = []\n",
    "    list_of_files = os.listdir(folder)\n",
    "    \n",
    "    for file in list_of_files:\n",
    "        try:\n",
    "            filename = os.path.join(folder, file)\n",
    "            with open(filename, 'r') as doc:\n",
    "                for line in doc:\n",
    "                    corpus.append(line) \n",
    "        except Exception as e:\n",
    "            print(\"No files found here!\")\n",
    "            raise e\n",
    "    return corpus\n",
    "\n",
    "def loadTextFile(folder): \n",
    "    corpus = []\n",
    "    list_of_files = os.listdir(folder)\n",
    "    \n",
    "    for file in list_of_files:\n",
    "        try:\n",
    "            filename = os.path.join(folder, file)\n",
    "            with open(filename, 'r') as doc:\n",
    "                for line in doc:\n",
    "                    corpus.append(' '.join(line.split()[2:])) \n",
    "        except Exception as e:\n",
    "            print(\"No files found here!\")\n",
    "            raise e\n",
    "    return corpus\n",
    "\n",
    "corpus = loadfile(\"wiki-pages-text\")\n",
    "text_corpus = loadTextFile(\"wiki-pages-text\")\n",
    "print(\"Length of the corpus is: \" + str(len(corpus)))\n",
    "print(\"Length of the corpus is: \" + str(len(text_corpus)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pandas split data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-17T06:20:15.175952Z",
     "start_time": "2019-05-17T06:20:08.306579Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100, 3)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>page_identifier</th>\n",
       "      <th>sentence_number</th>\n",
       "      <th>sentence_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Alexander_McNair</td>\n",
       "      <td>0</td>\n",
       "      <td>Alexander McNair -LRB- May 5   1775 -- March 1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Alexander_McNair</td>\n",
       "      <td>1</td>\n",
       "      <td>He was the first Governor of Missouri from its...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Alexander_McNair</td>\n",
       "      <td>4</td>\n",
       "      <td>McNair was born in Lancaster in the Province o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Alexander_McNair</td>\n",
       "      <td>5</td>\n",
       "      <td>His grandfather   David McNair   Sr.   immigra...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Alexander_McNair</td>\n",
       "      <td>6</td>\n",
       "      <td>David McNair   Jr.   Alexander 's father -LRB-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Alexander_McNair</td>\n",
       "      <td>7</td>\n",
       "      <td>Alexander went to school as a child   and atte...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Alexander_McNair</td>\n",
       "      <td>8</td>\n",
       "      <td>He reached an agreement with his mother and br...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Alexander_McNair</td>\n",
       "      <td>9</td>\n",
       "      <td>Alexander was defeated .\\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Alexander_McNair</td>\n",
       "      <td>10</td>\n",
       "      <td>He became a member of the Pennsylvania militia...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Alexander_McNair</td>\n",
       "      <td>13</td>\n",
       "      <td>In 1804   McNair traveled to what is now Misso...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    page_identifier sentence_number  \\\n",
       "0  Alexander_McNair               0   \n",
       "1  Alexander_McNair               1   \n",
       "2  Alexander_McNair               4   \n",
       "3  Alexander_McNair               5   \n",
       "4  Alexander_McNair               6   \n",
       "5  Alexander_McNair               7   \n",
       "6  Alexander_McNair               8   \n",
       "7  Alexander_McNair               9   \n",
       "8  Alexander_McNair              10   \n",
       "9  Alexander_McNair              13   \n",
       "\n",
       "                                       sentence_text  \n",
       "0  Alexander McNair -LRB- May 5   1775 -- March 1...  \n",
       "1  He was the first Governor of Missouri from its...  \n",
       "2  McNair was born in Lancaster in the Province o...  \n",
       "3  His grandfather   David McNair   Sr.   immigra...  \n",
       "4  David McNair   Jr.   Alexander 's father -LRB-...  \n",
       "5  Alexander went to school as a child   and atte...  \n",
       "6  He reached an agreement with his mother and br...  \n",
       "7                         Alexander was defeated .\\n  \n",
       "8  He became a member of the Pennsylvania militia...  \n",
       "9  In 1804   McNair traveled to what is now Misso...  "
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus_df = pd.DataFrame(corpus[:100])\n",
    "corpus_df.columns = ['text']\n",
    "\n",
    "corpus_df['page_identifier'] = corpus_df.text.apply(lambda x: x.split(' ')[0])  \n",
    "corpus_df['sentence_number'] = corpus_df.text.apply(lambda x: x.split(' ')[1]) \n",
    "corpus_df['sentence_text'] = corpus_df.text.apply(lambda x: x.split(' ')[2:])  \n",
    "corpus_df['sentence_text'] = [','.join(map(str, l)) for l in corpus_df['sentence_text']]\n",
    "corpus_df[\"sentence_text\"] = corpus_df['sentence_text'].str.replace(',',' ')\n",
    "corpus_df = corpus_df.drop('text', 1)\n",
    "\n",
    "print(corpus_df.shape)\n",
    "corpus_df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-17T06:21:26.812974Z",
     "start_time": "2019-05-17T06:21:26.584137Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/zhangyiming/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "tokenizer = nltk.tokenize.RegexpTokenizer(r'\\w+')\n",
    "lemmatizer = nltk.stem.WordNetLemmatizer()\n",
    "\n",
    "nltk.download('wordnet')\n",
    "def lemmatize(word):\n",
    "    lemma = lemmatizer.lemmatize(word,'v')\n",
    "    if lemma == word:\n",
    "        lemma = lemmatizer.lemmatize(word,'n')\n",
    "    return lemma\n",
    "\n",
    "def pre_process(comment) -> str:\n",
    "    # lower cased\n",
    "    comment = comment.lower()\n",
    "    # tokenize\n",
    "    words = tokenizer.tokenize(comment)\n",
    "    # lemmatize \n",
    "    words = [lemmatize(w) for w in words]\n",
    "    processed_comment = \" \".join(words)\n",
    "    return processed_comment\n",
    "\n",
    "def process_dataset(dataset: pd.DataFrame) -> pd.DataFrame:\n",
    "    processed_corpus = dataset['sentence_text'].apply(lambda text: pre_process(text))\n",
    "    return processed_corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess the corpus dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-17T06:21:30.802347Z",
     "start_time": "2019-05-17T06:21:30.735978Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:00:00.052291\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>alexander mcnair lrb may 5 1775 march 18 1826 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>he be the first governor of missouri from it e...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>mcnair be bear in lancaster in the province of...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>his grandfather david mcnair sr immigrate to p...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>david mcnair jr alexander s father lrb b 1736 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>alexander go to school a a child and attend on...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>he reach an agreement with his mother and brot...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>alexander be defeat</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>he become a member of the pennsylvania militia...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>in 1804 mcnair travel to what be now missouri ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       sentence_text\n",
       "0  alexander mcnair lrb may 5 1775 march 18 1826 ...\n",
       "1  he be the first governor of missouri from it e...\n",
       "2  mcnair be bear in lancaster in the province of...\n",
       "3  his grandfather david mcnair sr immigrate to p...\n",
       "4  david mcnair jr alexander s father lrb b 1736 ...\n",
       "5  alexander go to school a a child and attend on...\n",
       "6  he reach an agreement with his mother and brot...\n",
       "7                                alexander be defeat\n",
       "8  he become a member of the pennsylvania militia...\n",
       "9  in 1804 mcnair travel to what be now missouri ..."
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import datetime\n",
    "start = datetime.datetime.now()\n",
    "\n",
    "processed_corpus_df = pd.DataFrame(process_dataset(corpus_df))\n",
    "processed_corpus_df.to_pickle(\"./processed_corpus.pkl\")\n",
    "\n",
    "end = datetime.datetime.now()\n",
    "print(end-start)\n",
    "processed_corpus_df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load processed corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-17T06:21:36.198435Z",
     "start_time": "2019-05-17T06:21:36.181830Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100, 1)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>alexander mcnair lrb may 5 1775 march 18 1826 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>he be the first governor of missouri from it e...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>mcnair be bear in lancaster in the province of...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>his grandfather david mcnair sr immigrate to p...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>david mcnair jr alexander s father lrb b 1736 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>alexander go to school a a child and attend on...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>he reach an agreement with his mother and brot...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>alexander be defeat</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>he become a member of the pennsylvania militia...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>in 1804 mcnair travel to what be now missouri ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       sentence_text\n",
       "0  alexander mcnair lrb may 5 1775 march 18 1826 ...\n",
       "1  he be the first governor of missouri from it e...\n",
       "2  mcnair be bear in lancaster in the province of...\n",
       "3  his grandfather david mcnair sr immigrate to p...\n",
       "4  david mcnair jr alexander s father lrb b 1736 ...\n",
       "5  alexander go to school a a child and attend on...\n",
       "6  he reach an agreement with his mother and brot...\n",
       "7                                alexander be defeat\n",
       "8  he become a member of the pennsylvania militia...\n",
       "9  in 1804 mcnair travel to what be now missouri ..."
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_processed_corpus_df = pd.read_pickle(\"./processed_corpus.pkl\")\n",
    "print(load_processed_corpus_df.shape)\n",
    "load_processed_corpus_df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use Sklearn to build TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-17T06:45:52.386008Z",
     "start_time": "2019-05-17T06:45:04.226458Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'datetime' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-491d5c8268de>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mstart1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdatetime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdatetime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mtfidf_vectorizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTfidfVectorizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax_features\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m20000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'datetime' is not defined"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import pickle\n",
    "\n",
    "start1 = datetime.datetime.now()\n",
    "\n",
    "tfidf_vectorizer = TfidfVectorizer(max_features = 20000)\n",
    "#tfidf = tfidf_vectorizer.fit(load_processed_corpus_df['sentence_text'])\n",
    "tfidf = tfidf_vectorizer.fit(text_corpus)\n",
    "\n",
    "pickle.dump(tfidf, open(\"tfidf.pickle\",\"wb\"))\n",
    "\n",
    "end1 = datetime.datetime.now()\n",
    "print(end1-start1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build a dictionary for page identifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-17T05:12:41.315031Z",
     "start_time": "2019-05-17T05:11:04.785451Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of keys = 5396106\n",
      "['Alexander McNair', 'Alatskivi', 'An American Girl Story - Maryellen 1955-COLON- Extraordinary Christmas', 'Alta Outcome Document', 'Al ‘Urban', 'Albano buoy system', 'Amuka', 'Aleksandr Abdulkhalikov', 'Amalia Ciardi Duprè', 'Akbil']\n"
     ]
    }
   ],
   "source": [
    "page_dictionary = {}\n",
    "\n",
    "for id, doc in enumerate(corpus):\n",
    "    page_dictionary.setdefault((doc.split()[0].replace(\"_\", \" \")), []).append(id)\n",
    "\n",
    "# keys are the page identifier\n",
    "page_keys = list(page_dictionary.keys())\n",
    "print(\"Length of keys = {}\".format(len(page_keys)))\n",
    "print(page_keys[:10])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build a word dictionary for each word in page identifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-17T05:20:05.544555Z",
     "start_time": "2019-05-17T05:19:43.404972Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1841142\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def create_keywords(keys, dic):\n",
    "    keywords = {}\n",
    "    for key in keys:\n",
    "        words = key.split()\n",
    "        for word in words:\n",
    "            if word not in keywords:\n",
    "                keywords[word] = [dic[key]]\n",
    "            else:\n",
    "                keywords[word].append(dic[key])\n",
    "    return keywords\n",
    "\n",
    "# keywords should then remove stop words\n",
    "keywords = create_keywords(page_keys, page_dictionary)\n",
    "print(len(keywords))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-17T05:21:34.658126Z",
     "start_time": "2019-05-17T05:21:34.608204Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4156\n",
      "8\n",
      "7661\n"
     ]
    }
   ],
   "source": [
    "flatten = lambda l: [item for sublist in l for item in sublist]\n",
    "print(len(flatten(keywords['Aleksandr'])))\n",
    "print(len(flatten(keywords['Kaepernick'])))\n",
    "print(len(flatten(keywords['Colin'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-17T05:21:39.206306Z",
     "start_time": "2019-05-17T05:21:39.196112Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8\n",
      "['Colin Rand Kaepernick -LRB- -LSB- ` kæpərnɪk -RSB- ; born November 3 , 1987 -RRB- is an American football quarterback who is currently a free agent .', 'Kaepernick played collegiate football at the University of Nevada where he was named the Western Athletic Offensive Player of the Year twice and was the Most Valuable Player of the 2008 Humanitarian Bowl .', 'Kaepernick was selected by the San Francisco 49ers in the second round of the 2011 NFL Draft .', \"Kaepernick began his professional career as a backup to Alex Smith , but became the 49ers ' starter in the middle of the 2012 season after Smith suffered a concussion .\", \"He remained the team 's starting quarterback for the rest of the season and went on to lead the 49ers to their first Super Bowl appearance since 1994 , losing to the Baltimore Ravens .\"]\n"
     ]
    }
   ],
   "source": [
    "# use retrieved sentence index to retrieve the raw sentence\n",
    "# only return the sentence text part\n",
    "\n",
    "retrieved_corpus = []\n",
    "for sentence in flatten(keywords['Kaepernick']):\n",
    "    #print(sentence)\n",
    "    #print(corpus[sentence])\n",
    "    retrieved_corpus.append(' '.join(corpus[sentence].split()[2:]))\n",
    "\n",
    "print(len(retrieved_corpus))\n",
    "print(retrieved_corpus[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-17T05:21:50.412176Z",
     "start_time": "2019-05-17T05:21:44.517105Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Word</th>\n",
       "      <th>Index</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Alexander</td>\n",
       "      <td>[[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>McNair</td>\n",
       "      <td>[[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Alatskivi</td>\n",
       "      <td>[[18, 19, 20, 21], [1291710, 1291711, 1291712,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>An</td>\n",
       "      <td>[[22, 23, 24, 25], [2305, 2306], [3514, 3515, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>American</td>\n",
       "      <td>[[22, 23, 24, 25], [80, 81, 82, 83], [143], [1...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Word                                              Index\n",
       "0  Alexander  [[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13...\n",
       "1     McNair  [[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13...\n",
       "2  Alatskivi  [[18, 19, 20, 21], [1291710, 1291711, 1291712,...\n",
       "3         An  [[22, 23, 24, 25], [2305, 2306], [3514, 3515, ...\n",
       "4   American  [[22, 23, 24, 25], [80, 81, 82, 83], [143], [1..."
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus_keywords = pd.DataFrame(list(keywords.items()))\n",
    "corpus_keywords.columns = ['Word','Index']\n",
    "corpus_keywords.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-17T05:24:22.430306Z",
     "start_time": "2019-05-17T05:24:22.049982Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Word</th>\n",
       "      <th>Index</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>63373</th>\n",
       "      <td>Colin</td>\n",
       "      <td>[[317928, 317929, 317930, 317931, 317932, 3179...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Word                                              Index\n",
       "63373  Colin  [[317928, 317929, 317930, 317931, 317932, 3179..."
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus_keywords.loc[corpus_keywords['Word'] == 'Colin']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Find corpus index for all the claim tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-16T07:40:35.435277Z",
     "start_time": "2019-05-16T07:40:35.427810Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Aaron\n",
      "Burr\n",
      "killed\n",
      "Alexander\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17]\n",
      "Hamilton\n",
      "in\n",
      "Seaside\n",
      "Heights\n",
      "New\n",
      "Jersey\n",
      "CHiPs\n",
      "is\n",
      "an\n",
      "American\n",
      "[80, 81, 82, 83]\n",
      "romance\n",
      "film\n",
      "\n",
      "\n",
      "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17]\n",
      "[80, 81, 82, 83]\n"
     ]
    }
   ],
   "source": [
    "retrieved_sentences = []  # results raw sentences contains the page identifiers that is in the claim\n",
    "\n",
    "test_claims = []\n",
    "test_claim1 = \"Aaron Burr killed Alexander Alexander Hamilton in Seaside Heights, New Jersey.\"\n",
    "test_claim2 = \"CHiPs is an American romance film.\"\n",
    "test_claims.append(test_claim1)\n",
    "test_claims.append(test_claim2)\n",
    "\n",
    "for test_claim in test_claims:\n",
    "    for token in tokenizer.tokenize(test_claim):\n",
    "        print(token)\n",
    "        if token in keywords:\n",
    "            print(keywords[token])\n",
    "            retrieved_sentences.append(keywords[token])\n",
    "        \n",
    "print(\"\\n\")      \n",
    "flatten = lambda l: [item for sublist in l for item in sublist]\n",
    "# retrieved sentence\n",
    "print(retrieved_sentences[0])\n",
    "print(retrieved_sentences[1])\n",
    "#print(flatten(retrieved_sentences))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Find all the corpus text for the claim tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-16T10:11:33.379370Z",
     "start_time": "2019-05-16T10:11:33.374425Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18\n",
      "['Alexander McNair -LRB- May 5 , 1775 -- March 18 , 1826 -RRB- was an American frontiersman and politician .', 'He was the first Governor of Missouri from its entry as a state in 1820 , until 1824 .', 'McNair was born in Lancaster in the Province of Pennsylvania and grew up in Mifflin County .', 'His grandfather , David McNair , Sr. , immigrated to Pennsylvania from Donaghmore , County Donegal , Ireland around 1733 and had Scottish ancestors from Loch Lomond .', \"David McNair , Jr. , Alexander 's father -LRB- b. 1736 -RRB- , fought with General George Washington in the Trenton and Princeton campaigns in the winter of 1776 -- 77 , and died in February 1777 as a result of wounds received in battle and exposure when Alexander was less than two years old .\"]\n"
     ]
    }
   ],
   "source": [
    "# use retrieved sentence index to retrieve the raw sentence\n",
    "# only return the sentence text part\n",
    "\n",
    "retrieved_corpus = []\n",
    "for sentence in retrieved_sentences[0]:\n",
    "    #print(sentence)\n",
    "    #print(corpus[sentence])\n",
    "    retrieved_corpus.append(' '.join(corpus[sentence].split()[2:]))\n",
    "\n",
    "print(len(retrieved_corpus))\n",
    "print(retrieved_corpus[:5])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate cosine between claim and retrieved text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_test_claim = pd.DataFrame(list(test))\n",
    "corpus_test_claim.columns = ['Word','Index']\n",
    "corpus_test_claim.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-17T06:25:55.509865Z",
     "start_time": "2019-05-17T06:25:55.499199Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Aaron Burr killed Alexander Hamilton in Seaside Heights, New Jersey.\n",
      "\n",
      "\n",
      "['Colin Rand Kaepernick -LRB- -LSB- ` kæpərnɪk -RSB- ; born November 3 , 1987 -RRB- is an American football quarterback who is currently a free agent .', 'Kaepernick played collegiate football at the University of Nevada where he was named the Western Athletic Offensive Player of the Year twice and was the Most Valuable Player of the 2008 Humanitarian Bowl .', 'Kaepernick was selected by the San Francisco 49ers in the second round of the 2011 NFL Draft .', \"Kaepernick began his professional career as a backup to Alex Smith , but became the 49ers ' starter in the middle of the 2012 season after Smith suffered a concussion .\", \"He remained the team 's starting quarterback for the rest of the season and went on to lead the 49ers to their first Super Bowl appearance since 1994 , losing to the Baltimore Ravens .\", 'During the 2013 season , his first full season as a starter , Kaepernick helped the 49ers reach the NFC Championship , losing to the Seattle Seahawks .', 'In the following seasons , Kaepernick lost and won back his starting job , with the 49ers missing the playoffs for three years consecutively .', 'In 2016 , Kaepernick gained national attention when he began protesting by not standing while the United States national anthem was being performed before the start of games , motivated by what he viewed as the oppression of non-white races in the U.S. His actions prompted a wide variety of responses , including additional athletes in the NFL and other U.S. sports leagues protesting the anthem in various ways .']\n",
      "(1, 500)\n",
      "(8, 500)\n",
      "(1, 500)\n",
      "0.0\n",
      "0.0\n",
      "0.08034141593410438\n"
     ]
    }
   ],
   "source": [
    "from scipy.spatial.distance import cosine\n",
    "\n",
    "print(test_claim1)\n",
    "print(\"\\n\")\n",
    "print(retrieved_corpus)\n",
    "\n",
    "test_claim1_list = []\n",
    "test_claim1_list.append(test_claim1)\n",
    "tfidf_claim1 = tfidf.transform(test_claim1_list)\n",
    "print(tfidf_claim1.shape)\n",
    "\n",
    "tfidf_retrieved = tfidf.transform(retrieved_corpus)\n",
    "print(tfidf_retrieved.shape)\n",
    "print(tfidf_retrieved[0].shape)\n",
    "\n",
    "print(1 - cosine(tfidf_claim1.toarray(),tfidf_retrieved[0].toarray()))\n",
    "print(1 - cosine(tfidf_claim1.toarray(),tfidf_retrieved[1].toarray()))\n",
    "print(1 - cosine(tfidf_claim1.toarray(),tfidf_retrieved[2].toarray()))\n",
    "#cosine_value = 1 - cosine(tfidf_claim1,tfidf_retrieved)\n",
    "#print(cosine_value)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Top 5 Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.spatial.distance import cosine\n",
    "\n",
    "query_rep = tfidf_claim1.toarray()\n",
    "docs_rep = tfidf_retrieved_corpus.toarray()\n",
    "\n",
    "query_doc_cos_dist = [cosine(query_rep, doc_rep) for doc_rep in docs_rep]\n",
    "query_doc_sort_index = np.argsort(np.array(query_doc_cos_dist))\n",
    "\n",
    "print_count = 0\n",
    "for rank, sort_index in enumerate(query_doc_sort_index):\n",
    "    print ('Rank : ', rank, ' Consine : ', 1 - query_doc_cos_dist[sort_index],' Review : ', datax['Review Text'][sort_index])\n",
    "    if print_count == 4 :\n",
    "        break\n",
    "    else:\n",
    "        print_count += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-17T04:56:39.154219Z",
     "start_time": "2019-05-17T04:56:29.731111Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package brown to\n",
      "[nltk_data]     /Users/zhangyiming/nltk_data...\n",
      "[nltk_data]   Package brown is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import brown\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "nltk.download(\"brown\")\n",
    "\n",
    "brown_docs = []\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def lemmatize(word):\n",
    "    lemma = lemmatizer.lemmatize(word,'v')\n",
    "    if lemma == word:\n",
    "        lemma = lemmatizer.lemmatize(word,'n')\n",
    "    return lemma\n",
    "\n",
    "# iterate brown corpus and save words as documents\n",
    "for paragraph in brown.paras():\n",
    "    brown_word = set()\n",
    "    for sentence in paragraph:\n",
    "        for word in sentence:\n",
    "            # remove words that are not alphabetic\n",
    "            if word.isalpha():\n",
    "                # lower-cased\n",
    "                word = word.lower()\n",
    "                # lemmatized\n",
    "                # word = lemmatizer.lemmatize(word)\n",
    "                word = lemmatize(word)\n",
    "                brown_word.add(word)\n",
    "                    \n",
    "    brown_docs.append(brown_word)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-17T04:57:35.769460Z",
     "start_time": "2019-05-17T04:57:35.766289Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15667\n",
      "{'county', 'take', 'an', 'any', 'grand', 'say', 'recent', 'evidence', 'jury', 'irregularity', 'of', 'produce', 'that', 'place', 'fulton', 'no', 'friday', 'the', 'election', 'investigation', 'primary'}\n"
     ]
    }
   ],
   "source": [
    "print(len(brown_docs))\n",
    "print(brown_docs[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenize the claim."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-05-16T07:03:15.132938Z",
     "start_time": "2019-05-16T07:03:15.116479Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Colin Kaepernick became a starting quarterback during the 49ers 63rd season in the National Football League.\n",
      "['Colin', 'Kaepernick', 'became', 'a', 'starting', 'quarterback', 'during', 'the', '49ers', '63rd', 'season', 'in', 'the', 'National', 'Football', 'League']\n",
      "\n",
      "\n",
      "Tilda Swinton is a vegan.\n",
      "['Tilda', 'Swinton', 'is', 'a', 'vegan']\n",
      "\n",
      "\n",
      "Fox 2000 Pictures released the film Soul Food.\n",
      "['Fox', '2000', 'Pictures', 'released', 'the', 'film', 'Soul', 'Food']\n",
      "\n",
      "\n",
      "Anne Rice was born in New Jersey.\n",
      "['Anne', 'Rice', 'was', 'born', 'in', 'New', 'Jersey']\n",
      "\n",
      "\n",
      "Telemundo is a English-language television network.\n",
      "['Telemundo', 'is', 'a', 'English', 'language', 'television', 'network']\n",
      "\n",
      "\n",
      "Damon Albarn's debut album was released in 2011.\n",
      "['Damon', 'Albarn', 's', 'debut', 'album', 'was', 'released', 'in', '2011']\n",
      "\n",
      "\n",
      "There is a capital called Mogadishu.\n",
      "['There', 'is', 'a', 'capital', 'called', 'Mogadishu']\n",
      "\n",
      "\n",
      "Savages was exclusively a German film.\n",
      "['Savages', 'was', 'exclusively', 'a', 'German', 'film']\n",
      "\n",
      "\n",
      "Happiness in Slavery is a gospel song by Nine Inch Nails.\n",
      "['Happiness', 'in', 'Slavery', 'is', 'a', 'gospel', 'song', 'by', 'Nine', 'Inch', 'Nails']\n",
      "\n",
      "\n",
      "Andrew Kevin Walker is only Chinese.\n",
      "['Andrew', 'Kevin', 'Walker', 'is', 'only', 'Chinese']\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    " for key in list(res_data)[:10]:\n",
    "    res_data[key][\"evidence\"] = []\n",
    "    print(res_data[key][\"claim\"])\n",
    "    claim_query = res_data[key]['claim']\n",
    "    claim_tokens = tokenizer.tokenize(claim_query)\n",
    "    print(claim_tokens)\n",
    "    print(\"\\n\")\n",
    "\n",
    "    for claim_token in claim_tokens:\n",
    "        if claim_token in keywords:\n",
    "            print(keywords[claim_token])\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from bert_serving.client import BertClient\n",
    "from termcolor import colored\n",
    "\n",
    "topk = 5\n",
    "\n",
    "questions = retrieved_corpus\n",
    "with BertClient(port=4000, port_out=4001) as bc:\n",
    "    doc_vecs = bc.encode(questions)\n",
    "\n",
    "    while True:\n",
    "        query = input(colored('your question: ', 'green'))\n",
    "        query_vec = bc.encode([query])[0]\n",
    "        # compute normalized dot product as score\n",
    "        score = np.sum(query_vec * doc_vecs, axis=1) / np.linalg.norm(doc_vecs, axis=1)\n",
    "        topk_idx = np.argsort(score)[::-1][:topk]\n",
    "        print('top %d questions similar to \"%s\"' % (topk, colored(query, 'green')))\n",
    "        for idx in topk_idx:\n",
    "            print('> %s\\t%s' % (colored('%.1f' % score[idx], 'cyan'), colored(questions[idx], 'yellow')))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
